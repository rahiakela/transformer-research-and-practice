{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 4298,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 3093
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "01_prompting_foundation_model",
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-research-and-practice/blob/main/developing-kaggle-notebooks/10-GenAI/01_prompting_foundation_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'llama-2/pytorch/7b-chat-hf/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F3093%2F4298%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240130%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240130T083846Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6b73f960d8428fdaaa16d27cc349b415784b917861e35ddc90c1dca9082a1ac804f1460d983685d90a60f1aa4489f018fd24ac711367bd9a9d72c6e7c40b496b54d77b2742976e8f9ec49b31f8d8e9aae82de7fc2106086adca7fb9f1f3af2d957f9230aafd442bc8a8b1e4370568cbc241199980dc90826dd7c8f0a31839f86029d71dfc795e1d0ad64c36579020fe6a45e7c84b5ae8150d138590153fc91c0d749b443ff85a61c335da1f9b497894c38b3e1487b2b1dc855bd596a414bc473e143a455200a3ec272ce214a9acdf55ec54b81d72d398774d5739f3d19ae8a9575141431eb2076cc83b98b0e5554c00049a93eda5561e6762a14247471f480da'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "HmPiOIVwhXp5",
        "outputId": "2aec187a-4c4f-4370-f6f8-753b3f2517e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading llama-2/pytorch/7b-chat-hf/1, 20836388871 bytes compressed\n",
            "[==================================================] 20836388871 bytes downloaded\n",
            "Downloaded and uncompressed: llama-2/pytorch/7b-chat-hf/1\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction  \n",
        "\n",
        "\n",
        "We will use LlaMa v2 model to test if it can be used to perform simple math operations.\n",
        "\n",
        "The model used is:\n",
        "\n",
        "* **Model**: Llama 2  \n",
        "* **Variation**: 7b-chat-hf  \n",
        "* **Version**: V1  \n",
        "* **Framework**: PyTorch  \n"
      ],
      "metadata": {
        "id": "g00DP3qEhXqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and utils"
      ],
      "metadata": {
        "id": "yvy6PfvmhXqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xformers\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "q5YZlPNbx2we"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "import warnings\n",
        "from time import time\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-01-29T10:28:02.311636Z",
          "iopub.execute_input": "2024-01-29T10:28:02.31215Z",
          "iopub.status.idle": "2024-01-29T10:30:14.386542Z",
          "shell.execute_reply.started": "2024-01-29T10:28:02.312119Z",
          "shell.execute_reply": "2024-01-29T10:30:14.385168Z"
        },
        "trusted": true,
        "id": "nf_2HhoGhXqE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_tokenize_create_pipeline():\n",
        "    \"\"\"\n",
        "    Load the model\n",
        "    Create a\n",
        "    Args\n",
        "    Returns:\n",
        "        tokenizer\n",
        "        pipeline\n",
        "    \"\"\"\n",
        "    # adapted from https://huggingface.co/blog/llama2#using-transformers\n",
        "    time_1 = time()\n",
        "    model = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "    time_2 = time()\n",
        "    print(f\"Load model and init tokenizer: {round(time_2-time_1, 3)}\")\n",
        "    pipeline = transformers.pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",)\n",
        "    time_3 = time()\n",
        "    print(f\"Prepare pipeline: {round(time_3-time_2, 3)}\")\n",
        "    return tokenizer, pipeline"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T10:43:53.07408Z",
          "iopub.execute_input": "2024-01-29T10:43:53.075307Z",
          "iopub.status.idle": "2024-01-29T10:43:53.083412Z",
          "shell.execute_reply.started": "2024-01-29T10:43:53.075278Z",
          "shell.execute_reply": "2024-01-29T10:43:53.082569Z"
        },
        "trusted": true,
        "id": "r0fXDWD6hXqG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(tokenizer, pipeline, prompt_to_test):\n",
        "    \"\"\"\n",
        "    Perform a query\n",
        "    print the result\n",
        "    Args:\n",
        "        tokenizer: the tokenizer\n",
        "        pipeline: the pipeline\n",
        "        prompt_to_test: the prompt\n",
        "    Returns\n",
        "        None\n",
        "    \"\"\"\n",
        "    # adapted from https://huggingface.co/blog/llama2#using-transformers\n",
        "    time_1 = time()\n",
        "    sequences = pipeline(\n",
        "        prompt_to_test,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_length=200,)\n",
        "    time_2 = time()\n",
        "    print(f\"Test inference: {round(time_2-time_1, 3)}\")\n",
        "    for seq in sequences:\n",
        "        print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "_kg_hide-output": false,
        "_kg_hide-input": false,
        "execution": {
          "iopub.status.busy": "2024-01-29T10:47:35.551261Z",
          "iopub.execute_input": "2024-01-29T10:47:35.551622Z",
          "iopub.status.idle": "2024-01-29T10:47:35.557679Z",
          "shell.execute_reply.started": "2024-01-29T10:47:35.551596Z",
          "shell.execute_reply": "2024-01-29T10:47:35.55683Z"
        },
        "trusted": true,
        "id": "nhEkqSoihXqH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize the model, tokenizer and create a pipeline"
      ],
      "metadata": {
        "id": "2J4pI9vqhXqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer, pipeline = load_model_tokenize_create_pipeline()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T10:47:41.789583Z",
          "iopub.execute_input": "2024-01-29T10:47:41.790221Z",
          "iopub.status.idle": "2024-01-29T10:47:54.093111Z",
          "shell.execute_reply.started": "2024-01-29T10:47:41.790197Z",
          "shell.execute_reply": "2024-01-29T10:47:54.092061Z"
        },
        "trusted": true,
        "id": "TErcWVnHhXqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After we initialized the model and the tokenizer, we created a pipeline. Creating of the pipeline takes the longest time.\n",
        "\n",
        "Now let's test the model with few mathematical prompts."
      ],
      "metadata": {
        "id": "_PhMFYxAhXqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Can LlaMa v2 do simple math?"
      ],
      "metadata": {
        "id": "6n-lahABhXqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt #1: Perform a simple sum"
      ],
      "metadata": {
        "id": "JJJEKx4UhXqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_to_test = 'Prompt: Adrian has three apples. His sister Anne has ten apples more than him. How many apples has Anne?'\n",
        "test_model(tokenizer, pipeline, prompt_to_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "APVKi2JWhXqL",
        "outputId": "af25322f-39ea-42a4-c8f4-b52c01e029ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test inference: 11.485\n",
            "Result: Prompt: Adrian has three apples. His sister Anne has ten apples more than him. How many apples has Anne?\n",
            "\n",
            "Solution: Let's use algebra to solve this problem. Let's say Adrian has x apples. Since Anne has ten apples more than him, Anne has x + 10 apples.\n",
            "\n",
            "Now, we can use the information given in the problem to find the value of x. We know that Adrian has three apples, so x = 3.\n",
            "\n",
            "So, Anne has 10 + 3 = 13 apples.\n",
            "\n",
            "Therefore, Anne has 13 apples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt #2: Ask for the area of a circle, giving the radius"
      ],
      "metadata": {
        "id": "lQ-nlEtIhXqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_to_test = 'Prompt: A circle has the radius 5. What is the area of the circle?'\n",
        "test_model(tokenizer, pipeline, prompt_to_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "IL1RfJ8zhXqM",
        "outputId": "6e032ecd-bd33-48b3-981d-fb60cf9fd880",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test inference: 7.263\n",
            "Result: Prompt: A circle has the radius 5. What is the area of the circle?\n",
            "\n",
            "Answer: The formula for the area of a circle is:\n",
            "\n",
            "A = πr^2\n",
            "\n",
            "where A is the area of the circle and r is the radius of the circle.\n",
            "\n",
            "So, in this case, the radius of the circle is 5, so the area of the circle is:\n",
            "\n",
            "A = π(5)^2\n",
            "= 3.14(25)\n",
            "= 78.5\n",
            "\n",
            "Therefore, the area of the circle is 78.5 square units.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt #3: Calculate an equation with two unknowns"
      ],
      "metadata": {
        "id": "gw_h0TLChXqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_to_test = 'Prompt: Anne and Adrian have a total of 10 apples. Anne has 2 apples more than Adrian.\\\n",
        "How many apples has each of the children Anne and Adrian?'\n",
        "test_model(tokenizer, pipeline, prompt_to_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "5l0fEBC8hXqN",
        "outputId": "cfbddaf4-25fc-45a9-81d7-14a923e5a036",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test inference: 9.631\n",
            "Result: Prompt: Anne and Adrian have a total of 10 apples. Anne has 2 apples more than Adrian.How many apples has each of the children Anne and Adrian? Solution: We are given that Anne has 2 apples more than Adrian, so Adrian has x apples. We are also told that Anne has a total of 10 apples, so Anne has 10 - 2 = 8 apples. So, Adrian has x = 8 apples and Anne has 8 apples. Explanation: Let's use the information given in the problem to find out how many apples each of the children, Anne and Adrian, has. We know that Anne has 2 more apples than Adrian, so Adrian has x apples. We are also told that Anne has a total of 10 apples, so Anne has 10 - 2 = 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt #4: Calculate the no solution of linear equation"
      ],
      "metadata": {
        "id": "a68xcWwv0xYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_to_test = 'Prompt: A thief run away 1 km per hour and after that a police follow him 1 km per hour.\\\n",
        "When police will catch the thief?'\n",
        "test_model(tokenizer, pipeline, prompt_to_test)"
      ],
      "metadata": {
        "id": "2uqurctE09gO",
        "outputId": "435dcf1a-61b2-4151-cb42-dbc2a6545569",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test inference: 5.633\n",
            "Result: Prompt: A thief run away 1 km per hour and after that a police follow him 1 km per hour.When police will catch the thief?\n",
            "Answer:\n",
            "The police will catch the thief in 2 hours.\n",
            "Explanation:\n",
            "The thief is running away at a rate of 1 km per hour, which means that the police will cover the same distance in 1 hour.\n",
            "So, after 1 hour, the police will be 1 km behind the thief.\n",
            "Therefore, it will take the police 2 hours to catch the thief.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_to_test = 'Prompt: A thief run away with speed of 1 km per hour and after 1 hour, a police follow him with speed uof 1 km per hour.\\\n",
        "When police will catch the thief?'\n",
        "test_model(tokenizer, pipeline, prompt_to_test)"
      ],
      "metadata": {
        "id": "BeaeyCa_2Am3",
        "outputId": "deea7aef-a7b6-4305-a7d5-699870ea7614",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test inference: 9.9\n",
            "Result: Prompt: A thief run away with speed of 1 km per hour and after 1 hour, a police follow him with speed uof 1 km per hour.When police will catch the thief?\n",
            "A thief is running away with a speed of 1 km per hour, and after 1 hour, a police officer starts chasing him with the same speed. When will the police officer catch the thief?\n",
            "Let's first calculate the distance traveled by the thief in 1 hour:\n",
            "Distance = Speed x Time\n",
            "Distance = 1 km/h x 1 h = 1 km\n",
            "Now, let's calculate the distance traveled by the police officer in the same 1 hour:\n",
            "Distance = Speed x Time\n",
            "Distance = 1 km/h x 1 h = 1 km\n",
            "Since both the thief and the police officer have traveled the same distance, the thief is now behind\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions\n",
        "\n",
        "\n",
        "After we initialized the model and the tokenizer, which took on GPU T4 x2 at the first run under 200 sec. (this time might be variable), then each prompt only took less than 10 sec.\n",
        "\n",
        "The simple math questions are answered sometime correct, sometime incorrectly.\n",
        "\n",
        "It might be related to the temperature factor (that is not set here explicitly)."
      ],
      "metadata": {
        "id": "pHHUUodqhXqN"
      }
    }
  ]
}
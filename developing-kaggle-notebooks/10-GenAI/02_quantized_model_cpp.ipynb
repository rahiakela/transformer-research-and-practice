{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 4298,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 3093
        }
      ],
      "dockerImageVersionId": 30559,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "02-quantized-model.cpp",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-research-and-practice/blob/main/developing-kaggle-notebooks/10-GenAI/02_quantized_model_cpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'llama-2/pytorch/7b-chat-hf/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F3093%2F4298%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240130%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240130T075110Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3254183c64a2d49515910c6c10a9ee9d46701e52c85e30aa3a340e33c2ae5d454eed566d16c26fa37b12007d8759b7a3bf58ac3e5445eb12a8244d9b697c5a777961beb5fc2d925c4b6b174cf4959871e86e2b2003ac7e2cfc672b06852a6509fd317e2906db8d1ffe1452bacdfdd152ffea0eac7c761c55f996e85521d26e22fbbdd0072ac3a4381d27dac7c3c946e02af9a261c27db79453adacbd0e0289ebe969bef2c888468cdb7cb87854d91ed4d8acd20e4453c93af90d93a6f82ad44465fcb13a9291c475b25471286da8b246afcc30a5d4c695b1d3f54d640fbe1a7d1a32623a3d0406a2f92f76d180ae4f61a64de81561e98a244ae55f7ba470b772'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "zhEReLhzWd7Y",
        "outputId": "684d489e-2f75-4771-998b-9c1ad36a41ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading llama-2/pytorch/7b-chat-hf/1, 20836388871 bytes compressed\n",
            "[==================================================] 20836388871 bytes downloaded\n",
            "Downloaded and uncompressed: llama-2/pytorch/7b-chat-hf/1\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "In other notebooks we demonstrated how we can use **Llama 2** model for various tasks, from testing it on math problems, to creating a sequential task chain (with output of previous task used as parameter in the input of the next task) and to create Retrieval Augmented Generation system, with Llama 2 as LLM, ChromaDB as vector database and Langchain as task chaining framework.  \n",
        "\n",
        "In this notebook we will experiment with llama.cpp. This library help us to run Llama and other models on lower performance hardware (consumer hardware). It converts/quantizes Llama model to GGUF format."
      ],
      "metadata": {
        "id": "xnhbLvaMWd7e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation\n",
        "\n",
        "\n",
        "We start by installing llama.cpp."
      ],
      "metadata": {
        "id": "yLB3N2ZzWd7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n",
        "!git clone https://github.com/ggerganov/llama.cpp.git"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-01-30T07:48:53.21453Z",
          "iopub.execute_input": "2024-01-30T07:48:53.214832Z",
          "iopub.status.idle": "2024-01-30T07:49:28.790025Z",
          "shell.execute_reply.started": "2024-01-30T07:48:53.214791Z",
          "shell.execute_reply": "2024-01-30T07:49:28.788913Z"
        },
        "trusted": true,
        "id": "2-qoWR5lWd7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "MR9e4saOgCtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we convert our model to **llama.cpp** format."
      ],
      "metadata": {
        "id": "u8rkXX2UWd7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python llama.cpp/convert.py /kaggle/input/llama-2/pytorch/7b-chat-hf/1 \\\n",
        "  --outfile llama-7b.gguf \\\n",
        "  --outtype q8_0"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-01-30T07:49:28.792634Z",
          "iopub.execute_input": "2024-01-30T07:49:28.793417Z",
          "iopub.status.idle": "2024-01-30T07:49:30.368069Z",
          "shell.execute_reply.started": "2024-01-30T07:49:28.793372Z",
          "shell.execute_reply": "2024-01-30T07:49:30.367038Z"
        },
        "trusted": true,
        "id": "zK_KgS2WWd7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```log\n",
        "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F16    | [4096, 4096]\n",
        "model.norm.weight                                -> output_norm.weight                       | F16    | [4096]\n",
        "Writing llama-7b.gguf, format 7\n",
        "Traceback (most recent call last):\n",
        "  File \"/content/llama.cpp/convert.py\", line 1474, in <module>\n",
        "    main()\n",
        "  File \"/content/llama.cpp/convert.py\", line 1468, in main\n",
        "    OutputFile.write_all(outfile, ftype, params, model, vocab, special_vocab,\n",
        "  File \"/content/llama.cpp/convert.py\", line 1113, in write_all\n",
        "    check_vocab_size(params, vocab, pad_vocab=pad_vocab)\n",
        "  File \"/content/llama.cpp/convert.py\", line 959, in check_vocab_size\n",
        "    raise Exception(msg)\n",
        "Exception: Vocab size mismatch (model has 32000, but /kaggle/input/llama-2/pytorch/7b-chat-hf/1/tokenizer.model has 32001).\n",
        "```"
      ],
      "metadata": {
        "id": "KxP40gyYg861"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import packages"
      ],
      "metadata": {
        "id": "sKCZVlBVWd7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-30T07:50:08.050546Z",
          "iopub.execute_input": "2024-01-30T07:50:08.051424Z",
          "iopub.status.idle": "2024-01-30T07:50:08.075081Z",
          "shell.execute_reply.started": "2024-01-30T07:50:08.051387Z",
          "shell.execute_reply": "2024-01-30T07:50:08.073722Z"
        },
        "trusted": true,
        "id": "gBLD6yMVWd7n"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the model\n",
        "\n",
        "\n",
        "Let's quickly test the model. We initialize first the model."
      ],
      "metadata": {
        "id": "_2eHBbCRWd7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = Llama(model_path=\"/kaggle/working/llama-7b.gguf\")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-01-30T07:49:30.699227Z",
          "iopub.status.idle": "2024-01-30T07:49:30.69956Z",
          "shell.execute_reply.started": "2024-01-30T07:49:30.699402Z",
          "shell.execute_reply": "2024-01-30T07:49:30.699417Z"
        },
        "trusted": true,
        "id": "hUkq4s_9Wd7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a question."
      ],
      "metadata": {
        "id": "u_OG6iRAWd7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(\"Q: Name three capital cities in Europe? A: \", max_tokens=38, stop=[\"Q:\", \"\\n\"], echo=True)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-01-30T07:49:56.561951Z",
          "iopub.execute_input": "2024-01-30T07:49:56.56304Z",
          "iopub.status.idle": "2024-01-30T07:49:56.586675Z",
          "shell.execute_reply.started": "2024-01-30T07:49:56.562999Z",
          "shell.execute_reply": "2024-01-30T07:49:56.585473Z"
        },
        "trusted": true,
        "id": "_rK70wKlWd7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now let's see the output."
      ],
      "metadata": {
        "id": "Fe4Fg83lWd7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-28T19:58:12.80223Z",
          "iopub.execute_input": "2023-09-28T19:58:12.803307Z",
          "iopub.status.idle": "2023-09-28T19:58:12.815377Z",
          "shell.execute_reply.started": "2023-09-28T19:58:12.803268Z",
          "shell.execute_reply": "2023-09-28T19:58:12.814151Z"
        },
        "_kg_hide-input": true,
        "trusted": true,
        "id": "Sse2fwSmWd7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's run a math question."
      ],
      "metadata": {
        "id": "-X1ABThWWd7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(\"If a circle has the radius 3, what is its area?\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-28T20:04:02.264226Z",
          "iopub.execute_input": "2023-09-28T20:04:02.264933Z",
          "iopub.status.idle": "2023-09-28T20:04:33.775037Z",
          "shell.execute_reply.started": "2023-09-28T20:04:02.264898Z",
          "shell.execute_reply": "2023-09-28T20:04:33.773985Z"
        },
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "S6a-brf6Wd7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the answer."
      ],
      "metadata": {
        "id": "fylVRTGnWd7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(output['choices'][0]['text'])"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "skih0uOLWd7r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 4298,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 3093
        }
      ],
      "dockerImageVersionId": 30559,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "04-simple-sequential-langchain",
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-research-and-practice/blob/main/developing-kaggle-notebooks/10-GenAI/04_simple_sequential_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'llama-2/pytorch/7b-chat-hf/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F3093%2F4298%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240130%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240130T125344Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Db55d6c3e3f06c3f84751ee02016215a0da55a152645454ad756e7e55fe6e839bad7ee7e510fbf04c14a47397be525189f05fa5bf86e47591442370a3dd9d6da916ad620c50db3e963aa5427a963ea523d1ff0c6e34737fb3f3788c491d26be98c1f19852add1d8c03b9d3576c19230103eceed783955745a094e146aaa43ad13d3b87735ecab6ef4a92812b40e73fff20a6194321df14a48881e1d02c42543ad25239595567bfb60b00a8863d48e356bfe1ec5f2c009369c612a7687c6dadbc586a00dc99a66f23ccf99fe5a428ca3c85bd4c9bf91471986149474b0c06d2b2728ab1e5f7138b0d7d6402e7e2f23ee0f39bf0d8d96250465af59096cc49e9aa6'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "XhwyA0P4buaY"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "## Objective  \n",
        "\n",
        "Use Llama 2 and Langchain to create a multi-step task chain.\n",
        "\n",
        "## Models details  \n",
        "\n",
        "* **Model #1**: Llama 2  \n",
        "* **Variation**: 7b-chat-hf    \n",
        "* **Version**: V1  \n",
        "* **Framework**: PyTorch  \n",
        "\n",
        "\n",
        "LlaMA 2 model is pretrained and fine-tuned with 2 Trillion tokens and 7 to 70 Billion parameters which makes it one of the powerful open source models. It is a highly improvement over LlaMA 1 model."
      ],
      "metadata": {
        "id": "1iEmMgYqbuab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# InstalIing, imports, utils"
      ],
      "metadata": {
        "id": "Qt_VK0cobuac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install packages."
      ],
      "metadata": {
        "id": "qARw7JFRbuac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.33.0 accelerate==0.22.0 einops==0.6.1 langchain==0.0.300 xformers==0.0.21 \\\n",
        "bitsandbytes==0.41.1 sentence_transformers==2.2.2 chromadb==0.4.12"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2023-10-28T10:08:30.830196Z",
          "iopub.execute_input": "2023-10-28T10:08:30.831278Z",
          "iopub.status.idle": "2023-10-28T10:12:02.108145Z",
          "shell.execute_reply.started": "2023-10-28T10:08:30.831242Z",
          "shell.execute_reply": "2023-10-28T10:12:02.107001Z"
        },
        "trusted": true,
        "id": "uF8hqXmUbuac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import packages."
      ],
      "metadata": {
        "id": "kHKci4zxbuac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda, bfloat16\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from time import time\n",
        "\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain\n",
        "from langchain import PromptTemplate\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2023-10-28T10:12:02.110792Z",
          "iopub.execute_input": "2023-10-28T10:12:02.111248Z",
          "iopub.status.idle": "2023-10-28T10:12:10.059033Z",
          "shell.execute_reply.started": "2023-10-28T10:12:02.111206Z",
          "shell.execute_reply": "2023-10-28T10:12:10.058205Z"
        },
        "trusted": true,
        "id": "VZMdC29dbuad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize model, tokenizer, query pipeline  \n",
        "\n",
        "Define the model, the device, and the bitsandbytes configuration."
      ],
      "metadata": {
        "id": "7RzChBV3buad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_id = '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'\n",
        "\n",
        "model_2_id = '/kaggle/input/llama-2/pytorch/13b-chat-hf/1'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2023-10-28T10:15:46.260369Z",
          "iopub.execute_input": "2023-10-28T10:15:46.26138Z",
          "iopub.status.idle": "2023-10-28T10:15:46.269829Z",
          "shell.execute_reply.started": "2023-10-28T10:15:46.261331Z",
          "shell.execute_reply": "2023-10-28T10:15:46.268733Z"
        },
        "trusted": true,
        "id": "VulRgkMKbuad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the model and the tokenizer.  \n",
        "\n",
        "We perform this operation for both models (7b & 13b)."
      ],
      "metadata": {
        "id": "fyo-y3iAbuad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_1 = time()\n",
        "model_1_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_1_id,\n",
        ")\n",
        "model_1 = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_1_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_1_config,\n",
        "    quantization_config=None,\n",
        "    device_map='auto',\n",
        ")\n",
        "tokenizer_1 = AutoTokenizer.from_pretrained(model_1_id)\n",
        "time_2 = time()\n",
        "print(f\"Prepare model #1, tokenizer: {round(time_2-time_1, 3)} sec.\")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2023-10-28T10:12:10.158187Z",
          "iopub.execute_input": "2023-10-28T10:12:10.158521Z",
          "iopub.status.idle": "2023-10-28T10:14:13.678174Z",
          "shell.execute_reply.started": "2023-10-28T10:12:10.158492Z",
          "shell.execute_reply": "2023-10-28T10:14:13.676957Z"
        },
        "trusted": true,
        "id": "qPtMbwRXbuae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a pipeline."
      ],
      "metadata": {
        "id": "1ZuPUoOEbuae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_1 = time()\n",
        "query_pipeline_1 = transformers.pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model_1,\n",
        "        tokenizer=tokenizer_1,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",)\n",
        "time_2 = time()\n",
        "print(f\"Prepare pipeline #1: {round(time_2-time_1, 3)} sec.\")\n",
        "\n",
        "llm_1 = HuggingFacePipeline(pipeline=query_pipeline_1)\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2023-10-28T10:14:13.679427Z",
          "iopub.execute_input": "2023-10-28T10:14:13.67975Z",
          "iopub.status.idle": "2023-10-28T10:14:15.778422Z",
          "shell.execute_reply.started": "2023-10-28T10:14:13.679721Z",
          "shell.execute_reply": "2023-10-28T10:14:15.777451Z"
        },
        "trusted": true,
        "id": "zH_sJ4q-buae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We test it by running a simple query."
      ],
      "metadata": {
        "id": "_jPZrY_obuae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking model #1\n",
        "llm_1(prompt=\"What is the most popular food in France for tourists? Just return the name of the food.\")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2023-10-28T10:14:15.779806Z",
          "iopub.execute_input": "2023-10-28T10:14:15.780237Z",
          "iopub.status.idle": "2023-10-28T10:14:21.410591Z",
          "shell.execute_reply.started": "2023-10-28T10:14:15.780187Z",
          "shell.execute_reply": "2023-10-28T10:14:21.409513Z"
        },
        "trusted": true,
        "id": "5itoZnUhbuae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define and execute the sequential chain\n",
        "\n",
        "\n",
        "We define a chain with two tasks in sequence.  \n",
        "The input for the second step is the output of the first step.  "
      ],
      "metadata": {
        "id": "p15JeeLIbuae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sequential_chain(country, llm):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        country: country selected\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    time_1 = time()\n",
        "    template = \"What is the most popular food in {country} for tourists? Just return the name of the food.\"\n",
        "\n",
        "    #  first task in chain\n",
        "    first_prompt = PromptTemplate(\n",
        "\n",
        "    input_variables=[\"country\"],\n",
        "\n",
        "    template=template)\n",
        "\n",
        "    chain_one = LLMChain(llm = llm, prompt = first_prompt)\n",
        "\n",
        "    # second step in chain\n",
        "    second_prompt = PromptTemplate(\n",
        "\n",
        "    input_variables=[\"food\"],\n",
        "\n",
        "    template=\"What are the top three ingredients in {food}. Just return the answer as three bullet points.\",)\n",
        "\n",
        "    chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
        "\n",
        "    # combine the two steps and run the chain sequence\n",
        "    overall_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)\n",
        "    overall_chain.run(country)\n",
        "    time_2 = time()\n",
        "    print(f\"Run sequential chain: {round(time_2-time_1, 3)} sec.\")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2023-10-28T10:14:21.412032Z",
          "iopub.execute_input": "2023-10-28T10:14:21.412437Z",
          "iopub.status.idle": "2023-10-28T10:14:21.420628Z",
          "shell.execute_reply.started": "2023-10-28T10:14:21.412407Z",
          "shell.execute_reply": "2023-10-28T10:14:21.419579Z"
        },
        "trusted": true,
        "id": "DsLtLkeJbuae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the sequence with Llama v2 **7b** chat HF model."
      ],
      "metadata": {
        "id": "iBhFDn_pbuae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_answer = sequential_chain(\"France\", llm_1)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2023-10-28T10:14:21.422179Z",
          "iopub.execute_input": "2023-10-28T10:14:21.422538Z",
          "iopub.status.idle": "2023-10-28T10:14:25.016087Z",
          "shell.execute_reply.started": "2023-10-28T10:14:21.422508Z",
          "shell.execute_reply": "2023-10-28T10:14:25.014978Z"
        },
        "trusted": true,
        "id": "FJA6whUUbuae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the sequence with Llama v2 **7b** chat HF model."
      ],
      "metadata": {
        "id": "iEbHMAo7buaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_answer = sequential_chain(\"Italy\", llm_1)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2023-10-28T10:14:25.017559Z",
          "iopub.execute_input": "2023-10-28T10:14:25.017974Z",
          "iopub.status.idle": "2023-10-28T10:14:28.929011Z",
          "shell.execute_reply.started": "2023-10-28T10:14:25.017935Z",
          "shell.execute_reply": "2023-10-28T10:14:28.927914Z"
        },
        "trusted": true,
        "id": "g5X1dufAbuaf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
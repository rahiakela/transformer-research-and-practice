{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-multi-task-learning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNK+Gj5YH0u05PYjLnPZeco",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transfer-learning-for-natural-language-processing/blob/main/3-shallow-transfer-learning-for-nlp/3_multi_task_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47P5vUyRmABC"
      },
      "source": [
        "## Multi-Task Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqn_lp-LmA5-"
      },
      "source": [
        "In this notebook, we will cover some prominent shallow transfer learning approaches and concepts. This allows us to explore some major themes in transfer learning, while doing so in the context of relatively simple models of the class of eventual interest, i.e., shallow neural networks.\n",
        "\n",
        "Roughly speaking, categorization is based on whether transfer occurs between different languages, tasks or data domains. Each of these types of categorization is usually correspondingly referred to as cross-lingual learning, multi-task learning and domain adaptation.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transfer-learning-for-natural-language-processing/shallow-transfer-learning.png?raw=1' width='800'/>\n",
        "\n",
        "The methods we will look at here will involve components that are neural networks in one way or another.these neural networks do not have many\n",
        "layers. This is the reason why the label “shallow” is appropriate to describe this collection of methods.\n",
        "\n",
        "A common form of semi-supervised learning that employs pretrained word embeddings such as word2vec that they produce a single vector per word, regardless of context.\n",
        "\n",
        "We revisit the IMDB movie review sentiment classification. Recall that this example is concerned with classifying movie reviews from IMDB into positive or negative sentiments expressed. It is a prototypical sentiment analysis example that has been used widely in the literature to study many algorithms. We combine feature vectors generated by pretrained word embeddings for each review with some traditional machine learning classification methods, namely random forests and logistic regression.\n",
        "\n",
        "We then demonstrate that using higher-level embeddings which vectorize bigger sections of text – such as at the sentence-level, paragraphlevel and document-level – can lead to improved performance. The general idea of vectorizing text and then applying a traditional machine learning classification method to the resulting vectors.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transfer-learning-for-natural-language-processing/semi-supervised-learning.png?raw=1' width='800'/>\n",
        "\n",
        "**Multi-task learning**\n",
        "\n",
        "Subsequently, we introduce the reader to multi-task learning. We demonstrate how one can train a single system simultaneously to perform multiple tasks, email spam classification and IMDB movie review sentiment analysis. \n",
        "\n",
        "There are several potential benefits to multi-task learning. By\n",
        "training a single machine learning model for multiple tasks, a shared representation is learned on a larger and more varied collection of data from the combined data pool, which can lead to performance improvements. Moreover, it has been widely observed that this shared representation has a better ability to generalize to tasks beyond those that were trained on, and\n",
        "this improvement can be achieved without any increase in model size.\n",
        "\n",
        "Specifically, we focus on shallow neural multitask learning, where a single additional dense layer, as well as a classification layer, is trained\n",
        "for each specific task in the setup. Different tasks also share a layer between them, a setup typically referred to as hard-parameter sharing.\n",
        "\n",
        "**Domain adaptation**\n",
        "\n",
        "Assume that we are given one source domain, which can be defined as a particular distribution of data for a specific task, and a classifier that has been trained to perform well on data in that domain for that task. The goal of domain adaptation is to modify, or adapt, data in a different target domain in such a way that the pretrained knowledge from the source domain can aid\n",
        "learning in the target domain. We apply a simple autoencoding approach to “project” samples in the target domain into the source domain feature space.\n",
        "\n",
        "An autoencoder is a system that learns to reconstruct inputs with very high accuracy, typically by encoding them into an efficient latent representation and learning to decode the said representation efficiently. They have traditionally been heavily used in model reduction applications, since the latent representation is often of smaller dimension than the original space\n",
        "from which the encoding happens, and the said dimension value can also be picked to strike the right balance of computational efficiency and accuracy.\n",
        "\n",
        "In the extreme scenario, improvements can be obtained with no labelled data in the target domain being used for training. This is typically referred to as zero-shot domain adaptation, where learning happens with no labeled\n",
        "data in the target domain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyDqdxsIrU-c"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUCq5eAN9lGL",
        "outputId": "ed078221-ee0a-4a91-9352-085905dc3388",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0TABXTr16LE"
      },
      "source": [
        "# install sent2vec\n",
        "!pip install git+https://github.com/epfml/sent2vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMiwOYhVrWTc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8a7993a-95d2-4975-b168-e2c80fc6f967"
      },
      "source": [
        "import numpy as np  # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import email\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, concatenate\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import sent2vec\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxaKpdoQsMTl"
      },
      "source": [
        "Download IMDB Movie Review Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-v_6vdlsIsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6900a2aa-7b5f-4947-ac3e-3c388e510798"
      },
      "source": [
        "%%shell\n",
        "\n",
        "wget -q \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "tar xzf aclImdb_v1.tar.gz\n",
        "\n",
        "rm -rf aclImdb_v1.tar.gz\n",
        "rm -rf aclImdb/train/unsup"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJDB_TEr268H"
      },
      "source": [
        "Let's download sent2vec word Embedding from [Kaggle](https://www.kaggle.com/maxjeblick/sent2vec)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWCkjW6_27i5",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "b44a2217-94cb-43c9-ccb2-d7c1c3ab1ae9"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload() # upload kaggle.json file"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0208893d-c4fc-4b09-b535-d99b9fb725d2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0208893d-c4fc-4b09-b535-d99b9fb725d2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"rahiakela\",\"key\":\"484f91b2ebc194b0bff8ab8777c1ebff\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQmD0SLz2990",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5df53c59-53e0-41c5-cfd1-4cf843a1f92f"
      },
      "source": [
        "%%shell\n",
        "\n",
        "mkdir -p ~/.kaggle\n",
        "mv kaggle.json ~/.kaggle/\n",
        "ls ~/.kaggle\n",
        "chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# download word embeddings from kaggle\n",
        "kaggle datasets download -d maxjeblick/sent2vec/wiki_unigrams.bin\n",
        "unzip -qq sent2vec.zip\n",
        "rm -rf sent2vec.zip\n",
        "\n",
        "# download dataset from kaggle\n",
        "kaggle datasets download -d wcukierski/enron-email-dataset\n",
        "unzip -qq enron-email-dataset.zip\n",
        "\n",
        "kaggle datasets download -d rtatman/fraudulent-email-corpus\n",
        "unzip -qq fraudulent-email-corpus.zip\n",
        "\n",
        "rm -rf enron-email-dataset.zip fraudulent-email-corpus.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n",
            "Downloading sent2vec.zip to /content\n",
            "100% 4.42G/4.43G [01:50<00:00, 68.1MB/s]\n",
            "100% 4.43G/4.43G [01:50<00:00, 43.1MB/s]\n",
            "Downloading enron-email-dataset.zip to /content\n",
            " 99% 353M/358M [00:11<00:00, 39.6MB/s]\n",
            "100% 358M/358M [00:11<00:00, 32.9MB/s]\n",
            "Downloading fraudulent-email-corpus.zip to /content\n",
            " 91% 5.00M/5.52M [00:00<00:00, 12.8MB/s]\n",
            "100% 5.52M/5.52M [00:00<00:00, 13.9MB/s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxBE6zjK18YQ"
      },
      "source": [
        "def extract_messages(df):\n",
        "  messages = []\n",
        "  for item in df[\"message\"]:\n",
        "    # Return a message object structure from a string\n",
        "    e = email.message_from_string(item)\n",
        "    # get message body\n",
        "    message_body = e.get_payload()\n",
        "    messages.append(message_body)\n",
        "  print(\"Successfully retrieved message body from e-mails!\")\n",
        "  return messages"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve9bdhwlsZYD"
      },
      "source": [
        "## Preprocessing IMDB Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfJ_dH6ysi_i"
      },
      "source": [
        "Before proceeding, we must decide how many samples to draw from each class. We must also decide the maximum number of tokens per email, and the maximum length of each token. This is done by setting the following overarching hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg7YwUgZsm5i"
      },
      "source": [
        "n_sample = 1000    # number of samples to generate in each class\n",
        "maxtokens = 200    # the maximum number of tokens per document\n",
        "maxtokenlen = 100  # the maximum length of each token"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hXCbqnZsutN"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLgxb7mDs2XN"
      },
      "source": [
        "Let’s proceed by defining a function to tokenize text by splitting them into \n",
        "words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoKGXGwDsyHE"
      },
      "source": [
        "def tokenize(row):\n",
        "  if row is None or row is \"\":\n",
        "    tokens = \"\"\n",
        "  else:\n",
        "    tokens = str(row).split(\" \")[:maxtokens]\n",
        "  return tokens"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fxn9b_3s_EE"
      },
      "source": [
        "### Remove punctuation and unnecessary characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSlPjAVatAaI"
      },
      "source": [
        "**In order to ensure that classification is done based on language content only, we have to remove punctuation marks and other non-word characters from the emails.** We do this by employing regular expressions with the Python regex library. We also normalize words by turning them into lower case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KFGweAXtEn0"
      },
      "source": [
        "def reg_expressions(row):\n",
        "  tokens = []\n",
        "  try:\n",
        "    for token in row:\n",
        "      token = token.lower()          # make all characters lower case\n",
        "      token = re.sub(r\"[\\W\\d]\", \"\", token)\n",
        "      token = token[:maxtokenlen]    # truncate all tokens to hyperparameter maxtokenlen\n",
        "      tokens.append(token)\n",
        "  except:\n",
        "    token = \"\"\n",
        "    tokens.append(token)\n",
        "  return tokens"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX10nyiitTJq"
      },
      "source": [
        "### Stop-word removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qzq_2XGJtUS-"
      },
      "source": [
        "Stop-words are also removed. Stop-words are words that are very common in text but offer no useful information that can be used to classify the text. Words such as is, and, the, are are examples of stop-words. The NLTK library contains a list of 127 English stop-words and can be used to filter our tokenized strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfJ7DLGatWyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "343cb8e9-cb4a-4203-cc6a-0b8801b5c446"
      },
      "source": [
        "stop_words = stopwords.words(\"english\")\n",
        "print(stop_words)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82F_djDNuV7i"
      },
      "source": [
        "# print(stopwords) # see default stopwords\n",
        "# it may be beneficial to drop negation words from the removal list, as they can change the positive/negative meaning\n",
        "# of a sentence\n",
        "# stopwords.remove(\"no\")\n",
        "# stopwords.remove(\"nor\")\n",
        "# stopwords.remove(\"not\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJItvg2suYML"
      },
      "source": [
        "def stop_word_removal(row):\n",
        "  token = [token for token in row if token not in stop_words]\n",
        "  token = filter(None, token)\n",
        "\n",
        "  return token"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHgiw5cGIDlO"
      },
      "source": [
        "### Load pre-trained sent2vec embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQIRWWMnIJ28"
      },
      "source": [
        "Quite naturally, just as in the case of the pretrained word embeddings, the next step is to obtain the pretrained sent2vec sentence embedding to be loaded by the particular implementation/framework installed.\n",
        "\n",
        "We choose the smallest 600-dimensional embedding `wiki_unigrams.bin`, approximately 5 Gigabytes in size, which captures just the unigram information on Wikipedia.\n",
        "\n",
        "Now let's load the pre-trained embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4FOzCbDIQzN",
        "outputId": "f4f9ca34-d557-4cf4-e10a-cb9232eea324"
      },
      "source": [
        "# load sent2vec embedding\n",
        "model = sent2vec.Sent2vecModel()\n",
        "\n",
        "start = time.time()\n",
        "model.load_model(\"wiki_unigrams.bin\")\n",
        "end = time.time()\n",
        "\n",
        "print(\"Loading the sent2vec embedding took %d seconds\" % (end - start))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading the sent2vec embedding took 142 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gtdu7KhIXoP"
      },
      "source": [
        "### Extract corresponding vectors from the pretrained word embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxq3oU8gIYmY"
      },
      "source": [
        "Next, we define a function to generate vectors for a collection of reviews. It is essentially a simpler form of the function presented in Listing 3.2 for pretrained word embeddings – it is simpler as we do not need to worry about out-of-vocabulary words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuRIMSVhIci_"
      },
      "source": [
        "def assemble_embedding_vectors(data):\n",
        "  out = None\n",
        "  for item in data:    # Loop through every IMDB review\n",
        "    vec = model.embed_sentence(\" \".join(item))     # Extract embedding vectors for every word in review, now we dont need to handle out-of-vocab words\n",
        "    if vec is not None:                            # Edge case handling\n",
        "      if out is not None:\n",
        "        out = np.concatenate((out, vec), axis=0)    # Concatenate row vector to output Numpy array\n",
        "      else:\n",
        "        out = vec\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  return out"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wUQr44PInYC"
      },
      "source": [
        "### Preparing and assembling Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO9jKVyDIqP9"
      },
      "source": [
        "# shuffle raw data first\n",
        "def unison_shuffle_data(data, header):\n",
        "    p = np.random.permutation(len(header))\n",
        "    data = data[p]\n",
        "    header = np.asarray(header)[p]\n",
        "    return data, header\n",
        "\n",
        "# load data in appropriate form\n",
        "def load_data(path):\n",
        "  data, sentiments = [], []\n",
        "  for folder, sentiment in ((\"neg\", 0), (\"pos\", 1)):\n",
        "    folder = os.path.join(path, folder)\n",
        "    for name in os.listdir(folder):    # Go through every file in current folder\n",
        "      with open(os.path.join(folder, name), \"r\") as reader:\n",
        "        text = reader.read()\n",
        "      # Apply tokenization, stopword analysis routines\n",
        "      text = tokenize(text)\n",
        "      text = stop_word_removal(text)\n",
        "      text = reg_expressions(text)\n",
        "      # Track corresponding text and sentiment labels\n",
        "      data.append(text)\n",
        "      sentiments.append(sentiment)\n",
        "  # Convert to Numpy array\n",
        "  #print(data)\n",
        "  data_np = np.array(data)\n",
        "  #print(data_np[:10])\n",
        "  data, sentiments = unison_shuffle_data(data_np, sentiments)\n",
        "\n",
        "  return data, sentiments"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Evvyc8voIvt0",
        "outputId": "51740834-e2fb-4f9d-bf2d-0034f0e1ceeb"
      },
      "source": [
        "train_path = os.path.join(\"aclImdb\", \"train\")\n",
        "test_path = os.path.join(\"aclImdb\", \"test\")\n",
        "raw_data, raw_header = load_data(train_path)\n",
        "\n",
        "print(raw_data.shape)\n",
        "print(len(raw_header))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000,)\n",
            "25000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jv_t7XjWI5V1",
        "outputId": "de2c096f-490c-424c-b941-f97818805c13"
      },
      "source": [
        "# Subsample required number of samples\n",
        "random_indices = np.random.choice(range(len(raw_header)), size=(n_sample * 2,), replace=False)\n",
        "data_train = raw_data[random_indices]\n",
        "header = raw_header[random_indices]\n",
        "\n",
        "print(\"DEBUG::data_train::\")\n",
        "print(data_train[:10])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEBUG::data_train::\n",
            "[list(['ah', 'kellysinatra', 'sailorsuit', 'musical', 'so', 'familiar', 'right', 'yes', 'one', 'usually', 'hear', 'about', 'on', 'the', 'towns', 'thataway', 'but', 'stick', 'around', 'might', 'learn', 'something', 'okay', 'probably', 'not', 'anyway', 'anchors', 'aweigh', 'tells', 'story', 'two', 'sailors', 'three', 'fourday', 'leave', 'joe', 'sea', 'wolf', 'clarence', 'bookish', 'type', 'begs', 'joe', 'get', 'dame', 'now', 'theyre', 'picked', 'coppers', 'get', 'little', 'donald', 'home', 'thats', 'meet', 'susie', 'temptress', 'jezebel', 'just', 'kidding', 'clarence', 'falls', 'love', 'her', 'at', 'least', 'thinks', 'does', 'is', 'right', 'or', 'moron', 'or', 'misguided', 'society', 'find', 'watch', 'trumpet', 'fanfare', 'anchors', 'aweighbr', 'br', 'ps', 'if', 'want', 'see', 'kathryn', 'grayson', 'anything', 'sickeningly', 'sweet', 'try', 'kiss', 'me', 'kate', ''])\n",
            " list(['while', 'people', 'watching', 'movie', 'little', 'interest', 'many', 'hundreds', 'movies', 'dealing', 'magic', 'occult', 'one', 'form', 'another', 'one', 'probably', 'best', 'many', 'waysbr', 'br', 'from', 'the', 'golem', 'the', 'craft', 'subject', 'seems', 'endless', 'interest', 'movie', 'industry', 'the', 'majority', 'movies', 'touch', 'way', 'childishly', 'for', 'example', 'witchboard', 'true', 'piece', 'utter', 'garbage', 'every', 'way', 'either', 'taking', 'transcendental', 'elements', 'cheap', 'excuses', 'cheesy', 'special', 'effects', 'cardboard', 'cutout', 'villians', 'cf', 'warlock', 'more', 'frequently', 'subject', 'comes', 'hysterical', 'religious', 'context', 'in', 'various', 'revelationsoriented', 'movies', 'antichrist', 'inevitably', 'advocate', 'kind', 'newage', 'style', 'practice', 'rarely', 'movie', 'seems', 'show', 'least', 'passing', 'experience', 'magic', 'practiced', 'real', 'life', 'presentation', 'occult', 'movies', 'best', 'described', 'allegorical', 'literal', 'symbolic', '', 'quite', 'rightbr', 'br', 'i', 'watched', 'movie', 'many', 'years', 'tonight', 'i', 'seen', 'vhs'])\n",
            " list(['i', 'recently', 'bought', 'movie', 'bunch', 'laserdiscs', 'ebay', 'usually', 'i', 'war', 'action', 'movies', 'occasionally', 'i', 'enjoy', 'romantic', 'comediesbr', 'br', 'if', 'bored', 'todays', 'special', 'fx', 'films', 'high', 'gloss', 'romantic', 'comedies', 'check', 'shop', 'around', 'corner', 'quiet', 'evening', 'what', 'i', 'like', 'movie', 'characters', 'lot', 'decency', 'there', 'nothing', 'fake', 'pretentious', 'them', 'take', 'mr', 'matuschek', 'example', 'when', 'finds', 'wife', 'cheating', 'one', 'employees', 'tries', 'shot', 'himself', 'not', 'humiliation', 'unjust', 'character', 'stewart', 'ok', 'weired', 'example', 'br', 'br', 'yes', 'focus', 'movie', 'narrow', 'plot', 'predictable', 'yet', 'still', 'i', 'liked', 'lot', 'if', 'likes', 'notting', 'hill', 'like', 'shop', 'around', 'corner', 'fact', 'hugh', 'grant', 'reminds', 'lot', 'jimmy', 'stewart'])\n",
            " list(['there', 'bit', 'trivia', 'pointed', 'scene', 'early', 'movie', 'homer', 'watches', 'attempt', 'december', '', '', 'at', 'least', 'video', 'used', 'tv', 'watching', 'showed', 'vangard', 'launch', 'attempt', 'failedbr', 'br', 'he', 'next', 'shown', 'reading', 'dictating', 'letter', 'dr', 'von', 'braun', 'offering', 'condolences', 'failurebr', 'br', 'von', 'braun', 'marshall', 'space', 'flight', 'center', 'huntsville', 'working', 'army', 'the', 'vanguard', 'project', 'early', 'nasa', 'team', 'soon', 'became', 'goddard', 'space', 'flight', 'centerbr', 'br', 'the', 'army', 'rushed', 'jupiterc', 'essentially', 'us', 'made', 'v', 'technology', 'worked', 'launch', 'satellite', 'response', 'russias', 'success', 'sputnikbr', 'br', 'this', 'error', 'may', 'actually', 'made', 'homer', 'notoriety', 'von', 'braun', 'team', 'attempt', 'fail', 'in', 'fact', 'underlying', 'redstone', 'flying', '', 'first', 'us', 'man', 'rated', 'booster', 'used', 'shepards', 'sub', 'orbital', 'flight', 'well', 'grissomsbr', 'br', 'this', 'sort', 'movie', 'good', 'hopefully', 'inspire', 'people', 'read'])\n",
            " list(['toi', 'le', 'venin', 'robert', 'hosseins', 'masterpieceand', 'one', 'great', 'thrillers', 'fiftiesbased', 'frederic', 'dard', 'novela', 'writer', 'director', 'often', 'worked', 'see', 'also', 'le', 'montecharge', 'hossein', 'direct', 'lead', 'toothe', 'screenplay', 'grabs', 'first', 'pictures', 'desert', 'road', 'night', 'beautiful', 'blonde', 'might', 'fieriest', 'criminals', 'mysterious', 'house', 'finds', 'femme', 'fatale', 'and', 'sisterthen', 'begins', 'cat', 'mouse', 'play', 'one', 'sisters', 'wheelchair', 'but', 'really', 'disabledwhich', 'one', 'criminal', 'tried', 'kill', 'hero', 'night', 'br', 'br', 'the', 'two', 'actressesmarina', 'vlady', 'late', 'odile', 'versois', 'sistersbr', 'br', 'turn', 'lights', 'watchinghighly', 'suspenseful'])\n",
            " list(['if', 'movie', 'written', 'directed', 'produced', 'intention', 'creating', 'bad', 'movie', 'cult', 'classic', 'might', 'i', 'say', 'might', 'hit', 'have', 'ever', 'sat', 'watched', 'movie', 'absolutely', 'awful', 'becomes', 'fascinating', 'terms', 'faults', 'well', 'it', 'every', 'one', 'acting', 'cast', 'nominated', 'worst', 'acting', 'performance', 'awards', 'it', 'would', 'hard', 'find', 'another', 'film', 'kind', 'production', 'budget', 'contains', 'little', 'value', 'whatsoever', 'the', 'whole', 'thing', 'opening', 'scenes', 'defies', 'logic', 'dialogue', 'completely', 'unbelievable', 'illogical', 'ditto', 'behaviour', 'general', 'storyline', 'film', 'itself', 'what', 'really', 'mind', 'boggling', 'buffoons', 'boardroom', 'actually', 'made', 'decisions', 'spend', 'money', 'piece', 'trash', 'wow'])\n",
            " list(['sometimes', 'realism', 'work', 'effectiveness', 'film', 'thats', 'problem', 'here', 'the', 'sets', 'cheesy', 'inside', 'out', 'the', 'fog', 'ubiquitous', 'half', 'disguising', 'shabbiness', 'production', 'if', 'theres', 'bar', 'name', 'painted', 'front', 'window', 'says', 'simply', 'wine', 'and', 'spirits', 'the', 'result', 'claustrophobic', 'set', 'scenes', 'not', 'single', 'shot', 'city', 'even', 'fake', 'skyline', 'thats', 'kind', 'dublin', 'story', 'about', 'jack', 'ripper', 'movies', 'seedy', 'foggy', 'cobblestoned', 'whitechapel', 'who', 'would', 'want', 'way', 'how', 'could', 'way', 'bankable', 'stars', 'minuscule', 'budget', 'fourweek', 'shooting', 'schedule', 'the', 'acting', 'follows', 'suit', '', 'outrageously', 'hammy', 'everyones', 'part', 'sometimes', 'god', 'positively', 'excruciating', 'mrs', 'mcphillips', 'moaning', 'frankie', 'shot', 'dead', 'outside', 'house', 'victor', 'mclaglin', 'however', 'delivers', 'exactly', 'right', 'kind', 'overdone', 'performance', 'wardrobe', 'stuffed', 'toosmall', 'jacket', 'seems', 'bursting', 'like', 'frankensteins', 'monster', 'his', 'every', 'movement', 'seems', 'go', 'little', 'farther', 'intended', 'to', 'when', 'slaps', 'cap', 'head', 'bops'])\n",
            " list(['i', 'cant', 'remember', 'many', 'films', 'bumbling', 'idiot', 'hero', 'funny', 'throughout', 'leslie', 'cheung', 'antithesis', 'hero', 'hes', 'dense', 'seduced', 'gorgeous', 'vampire', 'i', 'good', 'luck', 'see', 'big', 'screen', 'find', 'video', 'watch', 'again', ''])\n",
            " list(['this', 'worthless', 'sequel', 'great', 'action', 'movie', 'cheap', 'looking', 'worst', 'all', 'boring', 'action', 'scenes', 'the', 'decent', 'thing', 'movie', 'last', 'fight', 'sequence', 'only', '', 'minutes', 'feels', 'like', 'goes', 'forever', 'even', 'diehard', 'van', 'damme', 'fanslike', 'myself', 'avoid', 'one'])\n",
            " list(['largely', 'dense', 'road', 'movie', 'comic', 'relief', 'provided', 'excellent', 'john', 'cleese', 'although', 'really', 'sending', 'performance', 'fawlty', 'towers', 'seems', 'flip', 'top', 'slapstick', 'slushy', 'sentimentality', 'drop', 'hat', 'worst', 'part', 'film', 'martin', 'hawn', 'find', 'themselves', 'are', 'etc', 'see', 'peril'])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOt9NUPPJBhK"
      },
      "source": [
        "Display sentiments and their frequencies in the dataset, to ensure it is roughly balanced between classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In4l1jWbJCDl",
        "outputId": "439a7025-7907-4058-8d1b-60c1ebb84827"
      },
      "source": [
        "unique_elements, counts_elements = np.unique(header, return_counts=True)\n",
        "\n",
        "print(\"Sentiments and their frequencies:\")\n",
        "print(unique_elements)\n",
        "print(counts_elements)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiments and their frequencies:\n",
            "[0 1]\n",
            "[ 958 1042]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BPUN5yjJQ_u"
      },
      "source": [
        "We can now use this function to extract sent2vec embedding vectors for each review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKjuBHsQJRd9",
        "outputId": "a26e978f-5e8c-4e38-838e-3c339e1723f7"
      },
      "source": [
        "embedding_vectors = assemble_embedding_vectors(data_train) \n",
        "print(embedding_vectors)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.03518312 -0.20093317  0.05013918 ...  0.08352327 -0.06436302\n",
            "   0.09644231]\n",
            " [-0.13478053 -0.24570955  0.01786834 ... -0.06769671  0.00131936\n",
            "   0.1521685 ]\n",
            " [ 0.07780235  0.0006177   0.00827089 ...  0.01655485  0.08689536\n",
            "   0.00155434]\n",
            " ...\n",
            " [ 0.02962787 -0.04588524 -0.07604289 ...  0.07706061 -0.13126795\n",
            "   0.18768099]\n",
            " [ 0.12826902 -0.07406035  0.03565962 ... -0.09084704 -0.14573774\n",
            "   0.12216499]\n",
            " [-0.17812063  0.08628934 -0.23646048 ...  0.1671205  -0.22869939\n",
            "  -0.14114092]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIufakvAJa1P"
      },
      "source": [
        "These can now be used as feature vectors for the same logistic regression and random forest.\n",
        "\n",
        "As the very last step of preparing the sentiment dataset for training by our baseline classifiers, we split it into independent training and testing or validation sets. This will allow us to evaluate the performance of the classifier on a set of data that was not used for training, an important thing\n",
        "to ensure in machine learning practice. We elect to use 70% of the data for training, and 30% for testing/validation afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AK1Oug7qJbWg",
        "outputId": "8abb71ab-6299-4da7-833b-f4b6f8120d8c"
      },
      "source": [
        "data = embedding_vectors\n",
        "del embedding_vectors\n",
        "\n",
        "idx = int(0.7 * data.shape[0])\n",
        "\n",
        "\n",
        "# 70% of data for training\n",
        "train_x = data[:idx, :]\n",
        "train_y = header[:idx]\n",
        "\n",
        "# remaining 30% for testing\n",
        "test_x = data[idx:, :]\n",
        "test_y = header[idx:]\n",
        "\n",
        "print(\"train_x/train_y list details, to make sure it is of the right form:\")\n",
        "print(len(train_x))\n",
        "print(train_x[:5])\n",
        "print(len(train_y))\n",
        "print(train_y[:5])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_x/train_y list details, to make sure it is of the right form:\n",
            "1400\n",
            "[[ 0.03518312 -0.20093317  0.05013918 ...  0.08352327 -0.06436302\n",
            "   0.09644231]\n",
            " [-0.13478053 -0.24570955  0.01786834 ... -0.06769671  0.00131936\n",
            "   0.1521685 ]\n",
            " [ 0.07780235  0.0006177   0.00827089 ...  0.01655485  0.08689536\n",
            "   0.00155434]\n",
            " [-0.05747809 -0.06960996  0.1839919  ...  0.17964846 -0.04227288\n",
            "   0.01319464]\n",
            " [ 0.03982377 -0.20946196 -0.04318017 ... -0.02148014  0.07441714\n",
            "   0.02773258]]\n",
            "1400\n",
            "[1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEJyB3902BXQ"
      },
      "source": [
        "## Preprocessing Email Spam Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2ahm1XR2Er6"
      },
      "source": [
        "### Loading and Visualizing the Fraudulent Email Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZqo9Zwe2dTL"
      },
      "source": [
        "filepath = \"./fradulent_emails.txt\"\n",
        "with open(filepath, \"r\", encoding=\"latin1\") as file:\n",
        "  data = file.read()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P4d5od-2fVZ",
        "outputId": "ff956e3e-0934-4322-c1f8-a329f2d28300",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "fraud_emails = data.split(\"From r\")\n",
        "del data\n",
        "\n",
        "print(\"Successfully loaded {} spam emails!\".format(len(fraud_emails)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully loaded 3978 spam emails!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAX70AH12jdG",
        "outputId": "6edec063-45a6-4bb8-cd3d-7609de0fed7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "fraud_bodies = extract_messages(pd.DataFrame(fraud_emails, columns=[\"message\"]))\n",
        "del fraud_emails\n",
        "\n",
        "fraud_bodies_df = pd.DataFrame(fraud_bodies[1:])\n",
        "del fraud_bodies\n",
        "\n",
        "fraud_bodies_df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully retrieved message body from e-mails!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>FROM:MR. JAMES NGOLA.\\nCONFIDENTIAL TEL: 233-2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dear Friend,\\n\\nI am Mr. Ben Suleman a custom ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF EL...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF EL...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Dear sir, \\n \\nIt is with a heart full of hope...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0\n",
              "0  FROM:MR. JAMES NGOLA.\\nCONFIDENTIAL TEL: 233-2...\n",
              "1  Dear Friend,\\n\\nI am Mr. Ben Suleman a custom ...\n",
              "2  FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF EL...\n",
              "3  FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF EL...\n",
              "4  Dear sir, \\n \\nIt is with a heart full of hope..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8tXX0BH2l2F"
      },
      "source": [
        "### Loading and Visualizing the Enron Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LINnhmb92mRo",
        "outputId": "dd40dc41-9b44-459f-c4a0-8eb0f18f52f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "filepath = \"./emails.csv\"\n",
        "\n",
        "# Read the enron data into a pandas.DataFrame called emails\n",
        "emails = pd.read_csv(filepath)\n",
        "print(\"Successfully loaded {} rows and {} columns!\".format(emails.shape[0], emails.shape[1]))\n",
        "print(emails.head())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully loaded 517401 rows and 2 columns!\n",
            "                       file                                            message\n",
            "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
            "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
            "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
            "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
            "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgSIZUHr2opV",
        "outputId": "829ebf7c-6411-4683-8ba4-351eb1d52c41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# take a closer look at the first email\n",
        "print(emails.loc[0][\"message\"])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Message-ID: <18782981.1075855378110.JavaMail.evans@thyme>\n",
            "Date: Mon, 14 May 2001 16:39:00 -0700 (PDT)\n",
            "From: phillip.allen@enron.com\n",
            "To: tim.belden@enron.com\n",
            "Subject: \n",
            "Mime-Version: 1.0\n",
            "Content-Type: text/plain; charset=us-ascii\n",
            "Content-Transfer-Encoding: 7bit\n",
            "X-From: Phillip K Allen\n",
            "X-To: Tim Belden <Tim Belden/Enron@EnronXGate>\n",
            "X-cc: \n",
            "X-bcc: \n",
            "X-Folder: \\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Sent Mail\n",
            "X-Origin: Allen-P\n",
            "X-FileName: pallen (Non-Privileged).pst\n",
            "\n",
            "Here is our forecast\n",
            "\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lmx8BJkt2qsm",
        "outputId": "a5960ed8-2632-45fe-c6cd-75f203c49a95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bodies = extract_messages(emails)\n",
        "\n",
        "# no longer needed, get rid of them\n",
        "del emails"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully retrieved message body from e-mails!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhdrzU1o2u4J",
        "outputId": "2c409239-7f35-419f-abdb-2edbe365bb00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "# extract random 10000 enron email bodies for building dataset\n",
        "bodies_df = pd.DataFrame(random.sample(bodies, 10000))\n",
        "# these are huge, no longer needed, get rid of them\n",
        "del bodies \n",
        "\n",
        "# expand default pandas display options to make emails more clearly visible when printed\n",
        "pd.set_option(\"display.max_colwidth\", 300)\n",
        "# you could do print(bodies_df.head()), but Jupyter displays this nicer for pandas DataFrames\n",
        "bodies_df.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>------------------------------------------------------------------------------\\n------------------------\\nW E E K E N D   S Y S T E M S   A V A I L A B I L I T Y\\n\\nF O R\\n\\nMay 11, 2001 5:00pm through May 14, 2001 12:00am\\n------------------------------------------------------------------------...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Louise would like for you to be in attendance at  the PHASE 2 meeting \\nscheduled for Thursday &amp; Friday, February 10th &amp; 11th.  These meetings will \\nconvene at the Doubletree Hotel beginning at 8:00 a.m. on Thursday morning.   \\nIf you are unable to attend for any reason, please contact me righ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Chris:\\nThanks for your prompt response.  I think that we do need to register the \\ncharge document which in this case would be the confirmation that will be \\nexecuted. Can you provide me with an estimate of the total cost?\\n\\nDoes everyone at Enron agree?\\n\\nCaroline,\\nI will redraft the Bermu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\nHi Jonathan, \\nA couple of questions for you: \\n1) Have you proceeded with opening an account at Dane Rauscher yet? \\n2) Your RSP account has a small debit of $50.00, probably due to RSP holding fees charged to the account for 2001.  We either need a cheque from you for the $50.00 or we need t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Attached please find the latest versions of the above-referenced documents, \\nmarked to show changes from the versions distributed 12/9, except in the case \\nof Annexes C ( Guaranty and Indemnification Agreement) and D ( form of the \\ndrawing certificate to the CCSI letter of credit, the complet...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                             0\n",
              "0  ------------------------------------------------------------------------------\\n------------------------\\nW E E K E N D   S Y S T E M S   A V A I L A B I L I T Y\\n\\nF O R\\n\\nMay 11, 2001 5:00pm through May 14, 2001 12:00am\\n------------------------------------------------------------------------...\n",
              "1  Louise would like for you to be in attendance at  the PHASE 2 meeting \\nscheduled for Thursday & Friday, February 10th & 11th.  These meetings will \\nconvene at the Doubletree Hotel beginning at 8:00 a.m. on Thursday morning.   \\nIf you are unable to attend for any reason, please contact me righ...\n",
              "2  Chris:\\nThanks for your prompt response.  I think that we do need to register the \\ncharge document which in this case would be the confirmation that will be \\nexecuted. Can you provide me with an estimate of the total cost?\\n\\nDoes everyone at Enron agree?\\n\\nCaroline,\\nI will redraft the Bermu...\n",
              "3  \\nHi Jonathan, \\nA couple of questions for you: \\n1) Have you proceeded with opening an account at Dane Rauscher yet? \\n2) Your RSP account has a small debit of $50.00, probably due to RSP holding fees charged to the account for 2001.  We either need a cheque from you for the $50.00 or we need t...\n",
              "4  Attached please find the latest versions of the above-referenced documents, \\nmarked to show changes from the versions distributed 12/9, except in the case \\nof Annexes C ( Guaranty and Indemnification Agreement) and D ( form of the \\ndrawing certificate to the CCSI letter of credit, the complet..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMNMTDBH3VMV"
      },
      "source": [
        "### Preparing and assembling Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhKQWyyP3Ysq"
      },
      "source": [
        "We are now going to put all these functions together to build the single dataset representing both classes. Most methods expect this dataset to be a Numpy array in order to process it, so we convert it to that form after combining the emails.\n",
        "\n",
        "Now, putting all the preprocessing steps together we assemble our dataset..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLAHnjtc3cvz"
      },
      "source": [
        "# Convert everything to lower-case, truncate to maxtokens and truncate each token to maxtokenlen\n",
        "\n",
        "# Apply predefined processing functions\n",
        "enron_emails = bodies_df.iloc[:, 0].apply(tokenize)\n",
        "enron_emails = enron_emails.apply(stop_word_removal)\n",
        "enron_emails = enron_emails.apply(reg_expressions)\n",
        "# sample the right number of emails from each class.\n",
        "enron_emails = enron_emails.sample(n_sample)\n",
        "\n",
        "del bodies_df\n",
        "\n",
        "# Apply predefined processing functions\n",
        "spam_emails = fraud_bodies_df.iloc[:, 0].apply(tokenize)\n",
        "spam_emails = spam_emails.apply(stop_word_removal)\n",
        "spam_emails = spam_emails.apply(reg_expressions)\n",
        "# sample the right number of emails from each class.\n",
        "spam_emails = spam_emails.sample(n_sample)\n",
        "\n",
        "del fraud_bodies_df\n",
        "\n",
        "# convert to Numpy array\n",
        "raw_data = pd.concat([enron_emails, spam_emails], axis=0).values"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlBxqs963gUP"
      },
      "source": [
        "Now, let’s take a peek at the result to make sure things are proceeding as expected:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3YPaLx63gtz",
        "outputId": "84986ca8-d112-4c17-c026-dd5123bcd4c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Shape of combined data is:\", raw_data.shape)\n",
        "print(\"Data is:\")\n",
        "print(raw_data)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of combined data is: (2000,)\n",
            "Data is:\n",
            "[list(['louise', 'may', 'aware', 'vng', 'agreement', 'provides', 'annual', 'payment', 'shared', 'value', 'sequent', 'energy', 'november', '', 'this', 'years', 'payment', 'scheduled', 'made', 'november', '', '', 'amount', '', 'given', 'previous', 'discussions', 'i', 'felt', 'appropriate', 'give', 'heads', 'matterfrank'])\n",
            " list(['have', 'trying', 'run', 'gordon', 'white', 'several', 'days', 'regarding', 'ameeting', 'finally', 'talked', 'today', 'he', 'town', 'north', 'carolinathe', 'rest', 'week', 'leaves', 'sweden', 'almost', 'two', 'weeks', 'thisweekend', 'the', 'earliest', 'date', 'could', 'set', 'meeting', 'accomodates', 'hisschedule', 'wednesday', 'dec', 'th', 'gordons', 'offices', 'austin', 'i', 'wanted', 'doit', 'earlier', 'gordon', 'essentially', 'pocket', 'almost', 'three', 'weeksdoes', 'date', 'work', 'you', 'confidentiality', 'noticethe', 'information', 'email', 'may', 'confidential', 'andor', 'privileged', 'thisemail', 'intended', 'reviewed', 'individual', 'organizationnamed', 'above', 'if', 'intended', 'recipient', 'authorizedrepresentative', 'intended', 'recipient', 'hereby', 'notified', 'anyreview', 'dissemination', 'copying', 'email', 'attachments', 'anyor', 'information', 'contained', 'herein', 'prohibited', 'if', 'receivedthis', 'email', 'error', 'please', 'immediately', 'notify', 'sender', 'return', 'emailand', 'delete'])\n",
            " list(['mark', '', 'fyi', 'discussed', 'thought', 'agreement', 'allows', 'client', 'trade', 'straight', 'liffe', 'account', 'using', 'liffe', 'connect', 'terminal', 'might', 'interest', 'justin', 'too', 'regardspaul', 'forwarded', 'paul', 'simonslonect', '', '', 'paul', 'simons', 'sent', 'by', 'nina', 'edmondsto', 'paul', 'sentancelonectectcc', 'subject', 'armajaroplease', 'see', 'attached'])\n",
            " ...\n",
            " list(['dear', 'beloved', '', 'i', 'mrs', 'christina', 'holden', 'am', 'married', 'dr', 'donald', 'holden', 'liberian', 'worked', 'shell', 'oil', 'company', 'nigeriahe', 'died', 'brief', 'illness', 'lasted', 'four', 'days', 'before', 'death', 'married', '', 'good', 'years', 'without', 'child', 'born', 'christians', '', 'since', 'death', 'i', 'decidednot', 'remarry', 'get', 'child', 'outside', 'matrimonial', 'home', 'bible', 'forbids', '', 'when', 'late', 'husband', 'alive', 'deposited', 'sum', '', 'million', 'ten', 'million', 'five', 'hundred', 'thousand', 'usdollars', 'a', 'bankpresently', 'this', 'money', 'still', 'safe', 'keeping', 'reserve', 'finance', 'company', 'recently', 'doctor', 'told', 'i', 'would', 'notlast', 'next', 'three', 'months', 'due', 'cancer', 'problems', 'though', 'disturbs', 'now', 'recently', 'strokehaving', 'known', 'my', 'condition', 'i', 'decided', 'donate', 'fund', 'church', 'betterstill', 'christian', 'muslim', 'individual', 'that', 'utilize', 'money', 'theway', 'i', 'going', 'instruct', 'in', 'i', 'want', 'church', 'use', 'fund', 'onorphanages', 'deaf', 'dumb', 'poor', 'widows', 'propagating', 'word', 'god', 'and', 'ensure', 'house', 'god'])\n",
            " list(['fromajames', 'desouzaregional', 'directornational', 'trust', 'security', 'company', 'sarllometogoetela', '', '', 'eattnadear', 'friendc', 'i', 'wish', 'proposal', 'come', 'surprisee', 'i', 'james', 'desouzac', 'regional', 'director', 'national', 'trust', 'security', 'company', 'seaerel', 'regional', 'office', 'lometogoe', 'we', 'foreign', 'client', 'mohamed', 'mohd', 'salaheldinwho', 'deposited', 'huge', 'sum', 'of', 'amountc', 'usemillionfour', 'million', 'five', 'hundred', 'thousand', 'united', 'states', 'dollars', 'companye', 'unfortunatelycthis', 'client', 'among', 'victims', 'egyptair', 'flight', 'noe', 'crashed', 'in', 'ueseae', 'confirmable', 'websiteawwwecnnecomfusfffegyptairelistfindexehtml', 'butc', 'since', 'body', 'come', 'claims', 'next', 'kine', 'a', 'sitaution', 'i', 'monitored', 'closely', 'position', 'companye', 'nowc', 'monitored', 'deposit', 'managed', 'years', 'deathc', 'hence', 'nobody', 'showed', 'next', 'kin', 'past', 'two', 'years', 'plusc', 'i', 'removed', 'file', 'private', 'vaulte', 'i', 'solicit', 'assistance', 'present', 'you', 'next', 'kin', 'every', 'arrangement', 'concluded', 'i', 'waiting', 'foreigner', 'enable', 'move', 'funds', 'accounte', 'this'])\n",
            " list(['emailmessagemessage', 'object', 'xfedd'])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQBaAtxo3itR"
      },
      "source": [
        "We see that the resulting array has divided the text into word units, as we intended to.\n",
        "\n",
        "Let’s create the headers corresponding to these emails, consisting of n_sample=1000 of spam emails followed by n_sample=1000 of non-spam emails:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBS0nlQI3lWg"
      },
      "source": [
        "categories = [\"spam\", \"notspam\"]\n",
        "header = ([1] * n_sample)\n",
        "header.extend(([0] * n_sample)) "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5pYi1RP80Sj"
      },
      "source": [
        "We are now ready to convert these into numerical vectors!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM1-dmn883ki",
        "outputId": "2da2befe-1858-4b6a-f883-f5aff13aba65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "embedding_vectors = assemble_embedding_vectors(raw_data)\n",
        "print(embedding_vectors)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.13003325 -0.05259239  0.05535318 ... -0.06819534 -0.30347332\n",
            "   0.17158867]\n",
            " [-0.20624852 -0.15659373  0.17093782 ...  0.00211835 -0.09384697\n",
            "  -0.15397482]\n",
            " [-0.26698807 -0.22275618  0.21200468 ...  0.0697145  -0.03027352\n",
            "  -0.0491537 ]\n",
            " ...\n",
            " [ 0.04502249 -0.10238895  0.0200396  ...  0.1996169  -0.00941314\n",
            "   0.06404965]\n",
            " [-0.01586943 -0.02585013 -0.00210908 ...  0.00710615 -0.12967923\n",
            "  -0.12611614]\n",
            " [-1.1175805  -0.751737    0.55355763 ... -0.6607673   0.21230514\n",
            "  -0.30602062]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqWWw_6D9BZq"
      },
      "source": [
        "# shuffle raw data first\n",
        "def unison_shuffle_data(data, header):\n",
        "  p = np.random.permutation(len(header))\n",
        "  data = data[p,:]\n",
        "  header = np.asarray(header)[p]\n",
        "  return data, header"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXRBszDE38oY",
        "outputId": "7823d776-163d-4954-8014-ab1a16d1dfd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "raw_data, header = unison_shuffle_data(embedding_vectors, header)\n",
        "\n",
        "# split into independent 70% training and 30% testing sets\n",
        "idx = int(0.7 * raw_data.shape[0])  # get 70% index value\n",
        "\n",
        "# 70% of data for training\n",
        "train_x2 = raw_data[:idx, :]\n",
        "train_y2 = header[:idx]\n",
        "\n",
        "# remaining 30% for testing\n",
        "test_x2 = raw_data[idx:, :]\n",
        "test_y2 = header[idx:]\n",
        "\n",
        "print(\"train_x2/train_y2 list details, to make sure they are of the right form:\")\n",
        "print(len(train_x2))\n",
        "print(train_x2)\n",
        "print(len(train_y2))\n",
        "print(train_y2[:5])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_x2/train_y2 list details, to make sure they are of the right form:\n",
            "1400\n",
            "[[-0.0844844  -0.02524753 -0.04236837 ...  0.04642141 -0.17143312\n",
            "  -0.06707019]\n",
            " [-0.04295615  0.01730135  0.04939496 ... -0.0560524  -0.00499381\n",
            "  -0.04114064]\n",
            " [ 0.00597835 -0.11049199 -0.02723659 ...  0.13010345 -0.244281\n",
            "  -0.01854577]\n",
            " ...\n",
            " [-0.16114262  0.11494704  0.52283984 ... -0.1570149   0.00378184\n",
            "   0.09888405]\n",
            " [ 0.02190978 -0.09102036  0.31848645 ...  0.08235449 -0.09449536\n",
            "   0.13476945]\n",
            " [-1.1175805  -0.751737    0.55355763 ... -0.6607673   0.21230514\n",
            "  -0.30602062]]\n",
            "1400\n",
            "[0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDsUOgVOuiAM"
      },
      "source": [
        "## Multi-Task Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xhBEN9ful36"
      },
      "source": [
        "Traditionally, **machine learning algorithms have been trained to perform a single task at a time, with the data collected and trained on independent for each separate task.** This is somewhat antithetical to the way humans and other animals learn, where training for multiple tasks occurs\n",
        "simultaneously and information from training on one task may inform and accelerate learning of other different tasks. **This additional information may improve performance not just on the current tasks being trained on, but also on future tasks, and sometimes even in cases where no labeled data is available on such future tasks. This scenario of transfer learning with no labeled data in the target domain is often referred to as zero-shot transfer learning.**\n",
        "\n",
        "In machine learning, multi-task learning has historically appeared in a number of settings – from multi-objective optimization to l2 and other forms of regularization (which itself can be framed as a form of multi-objective optimization).\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transfer-learning-for-natural-language-processing/neural-multi-task-learning.png?raw=1' width='800'/>\n",
        "\n",
        "In the other prominent type of neural multi-task learning, soft parameter sharing, all tasks have their own layers/parameters, which are not shared. Instead, they are encouraged to be similar via various constraints imposed on the task-specific layers across the various tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VsyzEDia64R"
      },
      "source": [
        "## Problem Setup and Shallow Neural Single-Task Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GamXMBu7a9OF"
      },
      "source": [
        "Let's consider now again, with 2 tasks only, the first task being IMDB movie review classification and the second task being email spam classification.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transfer-learning-for-natural-language-processing/neural-multi-task-hard-parameter-sharing.png?raw=1' width='800'/>\n",
        "\n",
        "Before proceeding, we must decide how the inputs to the resulting neural network will be converted into numbers for analysis. One popular choice is to encode the input at the characterlevel using one-hot encoding, where each character is replaced by a sparse vector of dimension equal to the total number of possible characters. This vector contains 1 in the column\n",
        "corresponding to the character and 0 otherwise. An illustration of this method, which aims to help the reader concisely visualize the process of one-hot encoding.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transfer-learning-for-natural-language-processing/one-hot-encoding-characters.png?raw=1' width='800'/>\n",
        "\n",
        "Before proceeding to the exact two-task setup, we perform another\n",
        "baseline. We use the IMDB movie classification task as the only one present, to see how the task-specific shallow neural classifier compares with the model from the previous section.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgnxik4YbGiE"
      },
      "source": [
        "input_shape = (len(train_x2[0]), )\n",
        "\n",
        "# Input must match the dimension of the sent2vec vectors\n",
        "sent2vec_vectors = Input(shape=input_shape)\n",
        "# Dense neural layer trained on top of the sent2vec vectors\n",
        "dense = Dense(512, activation=\"relu\")(sent2vec_vectors)\n",
        "# Apply dropout to reduce overfitting\n",
        "dense = Dropout(0.3)(dense)\n",
        "\n",
        "# Output indicates a single binary classifier - is review “positive” or “negative”?\n",
        "output = Dense(1, activation=\"sigmoid\")(dense)\n",
        "\n",
        "model = Model(inputs=sent2vec_vectors, outputs=output)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jn6zWv3OHth",
        "outputId": "1d06e9ac-1a23-427d-d94a-c13dee9eda2f"
      },
      "source": [
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "history = model.fit(train_x2, train_y2, validation_data=(test_x2, test_y2), batch_size=32, epochs=10, shuffle=True)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 3s 10ms/step - loss: 0.3392 - accuracy: 0.8926 - val_loss: 0.0555 - val_accuracy: 0.9850\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 0s 5ms/step - loss: 0.0363 - accuracy: 0.9904 - val_loss: 0.0460 - val_accuracy: 0.9883\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 0s 5ms/step - loss: 0.0246 - accuracy: 0.9940 - val_loss: 0.0444 - val_accuracy: 0.9883\n",
            "Epoch 4/10\n",
            "44/44 [==============================] - 0s 5ms/step - loss: 0.0110 - accuracy: 0.9982 - val_loss: 0.0451 - val_accuracy: 0.9817\n",
            "Epoch 5/10\n",
            "44/44 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 0.9972 - val_loss: 0.0465 - val_accuracy: 0.9883\n",
            "Epoch 6/10\n",
            "44/44 [==============================] - 0s 5ms/step - loss: 0.0071 - accuracy: 0.9976 - val_loss: 0.0497 - val_accuracy: 0.9883\n",
            "Epoch 7/10\n",
            "44/44 [==============================] - 0s 5ms/step - loss: 0.0078 - accuracy: 0.9946 - val_loss: 0.0506 - val_accuracy: 0.9833\n",
            "Epoch 8/10\n",
            "44/44 [==============================] - 0s 5ms/step - loss: 0.0053 - accuracy: 0.9986 - val_loss: 0.0525 - val_accuracy: 0.9833\n",
            "Epoch 9/10\n",
            "44/44 [==============================] - 0s 5ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.0538 - val_accuracy: 0.9833\n",
            "Epoch 10/10\n",
            "44/44 [==============================] - 0s 5ms/step - loss: 0.0043 - accuracy: 0.9977 - val_loss: 0.0549 - val_accuracy: 0.9833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNM2-FlePMJd"
      },
      "source": [
        "We found that the performance of this classifier was about 82% at the hyperparameter values specified.\n",
        "\n",
        "This is higher than the baseline of bag-of-words combined with logistic regression, and approximately equal to sent2vec combined with logistic regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLaR7EhprYDN"
      },
      "source": [
        "## Dual Task Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVbev8il84H_"
      },
      "source": [
        "We now introduce another task the task of determining if an email is spam or not. \n",
        "\n",
        "Assuming the availability of the `sent2vec` vectors `train_x2` corresponding to the emails in the data sample, It shows how one can create a multipleoutput\n",
        "shallow neural model to train it simultaneously for email spam classification and the classification of IMDB movie reviews via hard-parameter sharing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcaPrNio85Bd"
      },
      "source": [
        "input1_shape = (len(train_x[0]))\n",
        "input2_shape = (len(train_x2[0]))\n",
        "\n",
        "sent2vec_vectors1 = Input(shape=input1_shape)\n",
        "sent2vec_vectors2 = Input(shape=input2_shape)\n",
        "\n",
        "combined = concatenate([sent2vec_vectors1, sent2vec_vectors2])\n",
        "\n",
        "dense1 = Dense(512, activation=\"relu\")(combined)\n",
        "drop_out = Dropout(0.3)(dense1)\n",
        "\n",
        "output1 = Dense(1, activation=\"sigmoid\", name=\"classification1\")(drop_out)\n",
        "output2 = Dense(1, activation=\"sigmoid\", name=\"classification2\")(drop_out)\n",
        "\n",
        "model = Model(inputs=[sent2vec_vectors1, sent2vec_vectors2], outputs=[output1, output2])"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWcyRJ1c9Swi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7b86ec5-8fe7-4b4e-f24d-919dc60ae3a4"
      },
      "source": [
        "# Specify two loss functions (both binary_crossentropy in our case)\n",
        "model.compile(loss={\"classification1\": \"binary_crossentropy\", \"classification2\": \"binary_crossentropy\"}, \n",
        "              optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "# Specify training and validation data for each input\n",
        "history = model.fit([train_x, train_x2], [train_y, train_y2], validation_data=([test_x, test_x2], [test_y, test_y2]),\n",
        "                    batch_size=32, epochs=10, shuffle=True)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 1s 14ms/step - loss: 1.0890 - classification1_loss: 0.6867 - classification2_loss: 0.4024 - classification1_accuracy: 0.5805 - classification2_accuracy: 0.8378 - val_loss: 0.6158 - val_classification1_loss: 0.5356 - val_classification2_loss: 0.0802 - val_classification1_accuracy: 0.7400 - val_classification2_accuracy: 0.9817\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 0s 7ms/step - loss: 0.5355 - classification1_loss: 0.4742 - classification2_loss: 0.0613 - classification1_accuracy: 0.8008 - classification2_accuracy: 0.9888 - val_loss: 0.5419 - val_classification1_loss: 0.4856 - val_classification2_loss: 0.0562 - val_classification1_accuracy: 0.7533 - val_classification2_accuracy: 0.9817\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 0s 8ms/step - loss: 0.4453 - classification1_loss: 0.4083 - classification2_loss: 0.0370 - classification1_accuracy: 0.8326 - classification2_accuracy: 0.9952 - val_loss: 0.5817 - val_classification1_loss: 0.5305 - val_classification2_loss: 0.0513 - val_classification1_accuracy: 0.7600 - val_classification2_accuracy: 0.9850\n",
            "Epoch 4/10\n",
            "44/44 [==============================] - 0s 8ms/step - loss: 0.4152 - classification1_loss: 0.3969 - classification2_loss: 0.0183 - classification1_accuracy: 0.8205 - classification2_accuracy: 0.9985 - val_loss: 0.4933 - val_classification1_loss: 0.4431 - val_classification2_loss: 0.0502 - val_classification1_accuracy: 0.7833 - val_classification2_accuracy: 0.9850\n",
            "Epoch 5/10\n",
            "44/44 [==============================] - 0s 8ms/step - loss: 0.3334 - classification1_loss: 0.3169 - classification2_loss: 0.0166 - classification1_accuracy: 0.8673 - classification2_accuracy: 0.9986 - val_loss: 0.5460 - val_classification1_loss: 0.4919 - val_classification2_loss: 0.0541 - val_classification1_accuracy: 0.7533 - val_classification2_accuracy: 0.9867\n",
            "Epoch 6/10\n",
            "44/44 [==============================] - 0s 8ms/step - loss: 0.3180 - classification1_loss: 0.3045 - classification2_loss: 0.0135 - classification1_accuracy: 0.8696 - classification2_accuracy: 0.9990 - val_loss: 0.5150 - val_classification1_loss: 0.4599 - val_classification2_loss: 0.0551 - val_classification1_accuracy: 0.8050 - val_classification2_accuracy: 0.9800\n",
            "Epoch 7/10\n",
            "44/44 [==============================] - 0s 8ms/step - loss: 0.2584 - classification1_loss: 0.2490 - classification2_loss: 0.0094 - classification1_accuracy: 0.9100 - classification2_accuracy: 1.0000 - val_loss: 0.5369 - val_classification1_loss: 0.4841 - val_classification2_loss: 0.0527 - val_classification1_accuracy: 0.8000 - val_classification2_accuracy: 0.9883\n",
            "Epoch 8/10\n",
            "44/44 [==============================] - 0s 8ms/step - loss: 0.2588 - classification1_loss: 0.2533 - classification2_loss: 0.0055 - classification1_accuracy: 0.9064 - classification2_accuracy: 1.0000 - val_loss: 0.5845 - val_classification1_loss: 0.5310 - val_classification2_loss: 0.0535 - val_classification1_accuracy: 0.7733 - val_classification2_accuracy: 0.9867\n",
            "Epoch 9/10\n",
            "44/44 [==============================] - 0s 8ms/step - loss: 0.2506 - classification1_loss: 0.2438 - classification2_loss: 0.0068 - classification1_accuracy: 0.8953 - classification2_accuracy: 1.0000 - val_loss: 0.5934 - val_classification1_loss: 0.5381 - val_classification2_loss: 0.0553 - val_classification1_accuracy: 0.7367 - val_classification2_accuracy: 0.9883\n",
            "Epoch 10/10\n",
            "44/44 [==============================] - 0s 8ms/step - loss: 0.2014 - classification1_loss: 0.1970 - classification2_loss: 0.0044 - classification1_accuracy: 0.9341 - classification2_accuracy: 1.0000 - val_loss: 0.5824 - val_classification1_loss: 0.5282 - val_classification2_loss: 0.0542 - val_classification1_accuracy: 0.7550 - val_classification2_accuracy: 0.9883\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8YWlnanFIh2"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHOwQobpD0Wq"
      },
      "source": [
        "We found, upon training the multi-task system, that the IMDB classification performance dropped slightly from approximately 82% in the single-task shallow setup to about 80%. The email classification accuracy similarly dropped from 98.7% to 98.2%.\n",
        "\n",
        "First of all, observe that trained model can be deployed independently for each task, by simply replacing the omitted task input with zeros in order to respect the expected overall input dimension, and ignoring the corresponding output. Moreover, we expect the shared pretrained layer dense1 of the multi-task setup to be more readily generalizable to arbitrary new tasks. This is because it has been trained to be predictive on a more varied and general set of data and tasks.\n",
        "\n",
        "To make this more concrete, consider replacing either or both task-specific layers with new dense1 ones, initializing the shared layer to pretrained weights from the experiment above and fine-tuning the resulting model on the new task dataset. Having seen a broader range of task data, potentially similar to the newly added tasks, these shared weights are more likely to\n",
        "contain useful information for the downstream tasks being considered."
      ]
    }
  ]
}
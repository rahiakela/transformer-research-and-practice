{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-semi-supervised-learning-with-pretrained-word-embeddings.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPYHYKHrkb37XcHgIunLGUf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transfer-learning-for-natural-language-processing/blob/main/3-shallow-transfer-learning-for-nlp/1_semi_supervised_learning_with_pretrained_word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47P5vUyRmABC"
      },
      "source": [
        "# Semi-supervised Learning with Pretrained Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqn_lp-LmA5-"
      },
      "source": [
        "In this notebook, we will cover some prominent shallow transfer learning approaches and concepts. This allows us to explore some major themes in transfer learning, while doing so in the context of relatively simple models of the class of eventual interest, i.e., shallow neural networks.\r\n",
        "\r\n",
        "Roughly speaking, categorization is based on whether transfer occurs between different languages, tasks or data domains. Each of these types of categorization is usually correspondingly referred to as cross-lingual learning, multi-task learning and domain adaptation.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transfer-learning-for-natural-language-processing/shallow-transfer-learning.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "The methods we will look at here will involve components that are neural networks in one way or another.these neural networks do not have many\r\n",
        "layers. This is the reason why the label “shallow” is appropriate to describe this collection of methods.\r\n",
        "\r\n",
        "A common form of semi-supervised learning that employs pretrained word embeddings such as word2vec that they produce a single vector per word, regardless of context.\r\n",
        "\r\n",
        "We revisit the IMDB movie review sentiment classification. Recall that this example is concerned with classifying movie reviews from IMDB into positive or negative sentiments expressed. It is a prototypical sentiment analysis example that has been used widely in the literature to study many algorithms. We combine feature vectors generated by pretrained word embeddings for each review with some traditional machine learning classification methods, namely random forests and logistic regression.\r\n",
        "\r\n",
        "We then demonstrate that using higher-level embeddings which vectorize bigger sections of text – such as at the sentence-level, paragraphlevel and document-level – can lead to improved performance. The general idea of vectorizing text and then applying a traditional machine learning classification method to the resulting vectors.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transfer-learning-for-natural-language-processing/semi-supervised-learning.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "**Multi-task learning**\r\n",
        "\r\n",
        "Subsequently, we introduce the reader to multi-task learning. We demonstrate how one can train a single system simultaneously to perform multiple tasks, email spam classification and IMDB movie review sentiment analysis. \r\n",
        "\r\n",
        "There are several potential benefits to multi-task learning. By\r\n",
        "training a single machine learning model for multiple tasks, a shared representation is learned on a larger and more varied collection of data from the combined data pool, which can lead to performance improvements. Moreover, it has been widely observed that this shared representation has a better ability to generalize to tasks beyond those that were trained on, and\r\n",
        "this improvement can be achieved without any increase in model size.\r\n",
        "\r\n",
        "Specifically, we focus on shallow neural multitask learning, where a single additional dense layer, as well as a classification layer, is trained\r\n",
        "for each specific task in the setup. Different tasks also share a layer between them, a setup typically referred to as hard-parameter sharing.\r\n",
        "\r\n",
        "**Domain adaptation**\r\n",
        "\r\n",
        "Assume that we are given one source domain, which can be defined as a particular distribution of data for a specific task, and a classifier that has been trained to perform well on data in that domain for that task. The goal of domain adaptation is to modify, or adapt, data in a different target domain in such a way that the pretrained knowledge from the source domain can aid\r\n",
        "learning in the target domain. We apply a simple autoencoding approach to “project” samples in the target domain into the source domain feature space.\r\n",
        "\r\n",
        "An autoencoder is a system that learns to reconstruct inputs with very high accuracy, typically by encoding them into an efficient latent representation and learning to decode the said representation efficiently. They have traditionally been heavily used in model reduction applications, since the latent representation is often of smaller dimension than the original space\r\n",
        "from which the encoding happens, and the said dimension value can also be picked to strike the right balance of computational efficiency and accuracy.\r\n",
        "\r\n",
        "In the extreme scenario, improvements can be obtained with no labelled data in the target domain being used for training. This is typically referred to as zero-shot domain adaptation, where learning happens with no labeled\r\n",
        "data in the target domain.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyDqdxsIrU-c"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMiwOYhVrWTc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c3c3111-4dcb-4376-e9b5-dc608f2e5030"
      },
      "source": [
        "import numpy as np  # linear algebra\r\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
        "import random\r\n",
        "import re\r\n",
        "import os\r\n",
        "import time\r\n",
        "\r\n",
        "from gensim.models import FastText, KeyedVectors\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.ensemble import RandomForestClassifier      # random forest classifier library\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from IPython.display import HTML"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxaKpdoQsMTl"
      },
      "source": [
        "Download IMDB Movie Review Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-v_6vdlsIsX",
        "outputId": "636a217d-7ac2-4028-e06a-d943739b3ad5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\r\n",
        "\r\n",
        "wget -q \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\r\n",
        "tar xzf aclImdb_v1.tar.gz\r\n",
        "\r\n",
        "rm -rf aclImdb_v1.tar.gz\r\n",
        "rm -rf aclImdb/train/unsup"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "757fKEy0vJh6"
      },
      "source": [
        "Let's download fastText word Embedding from [Kaggle](https://www.kaggle.com/yangjia1991/jigsaw#)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "Zu3W8TKmvJwL",
        "outputId": "4f8142ca-192c-40ed-b8f5-85612a4d37de"
      },
      "source": [
        "from google.colab import files\r\n",
        "files.upload() # upload kaggle.json file"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-216deee9-1fd5-480b-9a70-9f2a01bf775c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-216deee9-1fd5-480b-9a70-9f2a01bf775c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"rahiakela\",\"key\":\"484f91b2ebc194b0bff8ab8777c1ebff\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5ylCPXDvcnH",
        "outputId": "94a54438-0255-4dc2-9eb0-96b321618e13"
      },
      "source": [
        "%%shell\r\n",
        "\r\n",
        "mkdir -p ~/.kaggle\r\n",
        "mv kaggle.json ~/.kaggle/\r\n",
        "ls ~/.kaggle\r\n",
        "chmod 600 /root/.kaggle/kaggle.json\r\n",
        "\r\n",
        "# download dataset from kaggle\r\n",
        "kaggle datasets download -d yangjia1991/jigsaw/wiki.en.vec\r\n",
        "unzip -qq jigsaw.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n",
            "Downloading jigsaw.zip to /content\n",
            "100% 2.36G/2.37G [00:29<00:00, 127MB/s]\n",
            "100% 2.37G/2.37G [00:29<00:00, 86.8MB/s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve9bdhwlsZYD"
      },
      "source": [
        "## Preprocessing IMDB Movie Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfJ_dH6ysi_i"
      },
      "source": [
        "Before proceeding, we must decide how many samples to draw from each class. We must also decide the maximum number of tokens per email, and the maximum length of each token. This is done by setting the following overarching hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg7YwUgZsm5i"
      },
      "source": [
        "n_sample = 1000   # number of samples to generate in each class\r\n",
        "maxtokens = 200    # the maximum number of tokens per document\r\n",
        "maxtokenlen = 100  # the maximum length of each token"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hXCbqnZsutN"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLgxb7mDs2XN"
      },
      "source": [
        "Let’s proceed by defining a function to tokenize text by splitting them into \r\n",
        "words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoKGXGwDsyHE"
      },
      "source": [
        "def tokenize(row):\r\n",
        "  if row is None or row is \"\":\r\n",
        "    tokens = \"\"\r\n",
        "  else:\r\n",
        "    tokens = row.split(\" \")[:maxtokens]\r\n",
        "  return tokens"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fxn9b_3s_EE"
      },
      "source": [
        "### Remove punctuation and unnecessary characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSlPjAVatAaI"
      },
      "source": [
        "**In order to ensure that classification is done based on language content only, we have to remove punctuation marks and other non-word characters from the emails.** We do this by employing regular expressions with the Python regex library. We also normalize words by turning them into lower case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KFGweAXtEn0"
      },
      "source": [
        "def reg_expressions(row):\r\n",
        "  tokens = []\r\n",
        "  try:\r\n",
        "    for token in row:\r\n",
        "      token = token.lower()          # make all characters lower case\r\n",
        "      token = re.sub(r\"[\\W\\d]\", \"\", token)\r\n",
        "      token = token[:maxtokenlen]    # truncate all tokens to hyperparameter maxtokenlen\r\n",
        "      tokens.append(token)\r\n",
        "  except:\r\n",
        "    token = \"\"\r\n",
        "    tokens.append(token)\r\n",
        "  return tokens"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX10nyiitTJq"
      },
      "source": [
        "### Stop-word removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qzq_2XGJtUS-"
      },
      "source": [
        "Stop-words are also removed. Stop-words are words that are very common in text but offer no useful information that can be used to classify the text. Words such as is, and, the, are are examples of stop-words. The NLTK library contains a list of 127 English stop-words and can be used to filter our tokenized strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfJ7DLGatWyN",
        "outputId": "b913fcf3-29c6-4529-c3cf-a2f651300b89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "stop_words = stopwords.words(\"english\")\r\n",
        "print(stop_words)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82F_djDNuV7i"
      },
      "source": [
        "# print(stopwords) # see default stopwords\r\n",
        "# it may be beneficial to drop negation words from the removal list, as they can change the positive/negative meaning\r\n",
        "# of a sentence\r\n",
        "# stopwords.remove(\"no\")\r\n",
        "# stopwords.remove(\"nor\")\r\n",
        "# stopwords.remove(\"not\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJItvg2suYML"
      },
      "source": [
        "def stop_word_removal(row):\r\n",
        "  token = [token for token in row if token not in stop_words]\r\n",
        "  token = filter(None, token)\r\n",
        "\r\n",
        "  return token"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDsUOgVOuiAM"
      },
      "source": [
        "## Semi-supervised Learning with fastText Embedding Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xhBEN9ful36"
      },
      "source": [
        "The concept of word embeddings is central to the field of NLP. It is a name given to a collection of techniques which produce a set of vectors of real numbers for each word that needs to be analyzed. A major consideration in word embedding design is the dimension of the vector generated. Bigger vectors generally can achieve better representation capability of words within\r\n",
        "a language and thereby better performance on many tasks, while naturally being more expensive computationally.\r\n",
        "\r\n",
        "As was outlined, this important sub-area of NLP research has a rich history originating with the term-vector model of information retrieval in the 1960s. This culminated with pretrained shallow neural-network-based techniques such as fastText, GloVe and word2vec – which came in several variants in mid 2010s including:\r\n",
        "\r\n",
        "- Continuous Bag of Words(CBOW) \r\n",
        "- Skip-Gram\r\n",
        "\r\n",
        "Both CBOW and Skip-Gram are extracted from shallow neural networks that were trained for various goals. **Skip-Gram attempts to predict words neighboring any target word in a sliding window, while CBOW attempts to predict the target word given the neighbors.**\r\n",
        "\r\n",
        "GloVe - which stands for “Global Vectors” - attempts to extend word2vec by incorporating global information into the embeddings. It optimizes the embeddings such that the cosine product between words reflects the number of times they co-occur, with the goal of making the resulting vectors more interpretable. \r\n",
        "\r\n",
        "**fastText**\r\n",
        "\r\n",
        "The technique fastText attempts to enhance word2vec by repeating the Skip-Gram methods on character n-grams (versus word n-grams) thereby being able to\r\n",
        "handle previously unseen words.\r\n",
        "\r\n",
        "To reiterate, **fastText is known for its ability to handle out-of-vocabulary words, which comes from it having been designed to embed sub-word character n-grams or sub-words** (versus entire words as is the case with word2vec). **This enables it to build embeddings up for previously unseen words by aggregating composing character n-gram embeddings. That comes at the\r\n",
        "expense of a larger pretrained embedding, and higher computing resource requirement and cost.**\r\n",
        "\r\n",
        "For these reasons, we elect to use the fastText framework as the representative pretrained word embedding computing method in this notebook, albeit with the word2vec input format. This allows us to keep the computing cost lower, making the exercise easier for the reader, while also showcasing how out-of-vocabulary issues would be handled and providing a solid experience platform from which the reader can venture into sub-word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VsyzEDia64R"
      },
      "source": [
        "## Load pre-trained fastText embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GamXMBu7a9OF"
      },
      "source": [
        "Once the embedding is available, it can be loaded using the following code snippet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgnxik4YbGiE",
        "outputId": "a23a5e65-d8a6-4db4-cc1e-747b8daf95ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "start = time.time()\r\n",
        "fasttext_embedding = KeyedVectors.load_word2vec_format(\"wiki.en.vec\")\r\n",
        "end = time.time()\r\n",
        "\r\n",
        "print(\"Loading the embedding took %d seconds\" % (end - start))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading the embedding took 681 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2kMgC-DAgXZ"
      },
      "source": [
        "In practice, in such a situation, it is not uncommon to load the embedding\r\n",
        "once into memory and then serve access to it using an approach such as flask for as long as it is needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S0x5dnjAUzw"
      },
      "source": [
        "## Extract corresponding vectors from the fastText pretrained word embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYn8y74ogp0x"
      },
      "source": [
        "Since our embedding of choice does not handle out-of-vocabulary words out-of-the-box, the next thing we do is to develop a methodology for addressing this situation. The simplest thing to do, quite naturally, is to simply skip any such words. Since the fastText framework errors out when such a word is encountered, we will use a try and except block to catch these errors without interrupting execution. Assume that you are given a pretrained input embedding that serves as a dictionary, with words as keys and corresponding vectors as values, and an input list of words in a review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFHTi3QkdSQY"
      },
      "source": [
        "def handle_out_of_vocab(embedding, in_text):\r\n",
        "  out = None\r\n",
        "  for word in in_text:    # Loop through every word\r\n",
        "    try:\r\n",
        "      tmp = embedding[word]              # Extract corresponding embedding vector and enforce “row shape”\r\n",
        "      tmp = tmp.reshape(1, len(tmp))\r\n",
        "\r\n",
        "      if out is None:     # Handle edge case of the first vector and an empty out array\r\n",
        "        out = tmp\r\n",
        "      else:\r\n",
        "        out = np.concatenate((out, tmp), axis=0)    # Concatenate row embedding vector to output Numpy array\r\n",
        "    except:     # Skip execution on current word and continue execution from the next word when out-of-vocabulary errors occur\r\n",
        "      pass\r\n",
        "\r\n",
        "  return out"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Se001UmitsD"
      },
      "source": [
        "However, before doing so we must decide how we will combine or aggregate\r\n",
        "the embedding vectors for individual words in a review into a single vector representing the entire review. It has been found in practice that the heuristic of simply averaging the words works as a very strong baseline. Since the embeddings were trained in a way that ensures that similar words are closer to each other in the resulting vector space, it makes intuitive sense that\r\n",
        "their average would represent the average meaning of the collection. The averaging baseline for summarization/aggregation is often recommended as a first step in embedding bigger sections of text from word embeddings.\r\n",
        "\r\n",
        "Effectively, this code calls the above function repeatedly on every review in the corpus, averages the output and concatenates the resulting vectors into a single 2-dimensional Numpy array. The rows of this resulting array correspond to\r\n",
        "aggregated-by-averaging embedding vectors for each review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1xkfl5TjESe"
      },
      "source": [
        "def assemble_embedding_vectors(data):\r\n",
        "  out = None\r\n",
        "  for item in data:    # Loop through every IMDB review\r\n",
        "    tmp = handle_out_of_vocab(fasttext_embedding, item)     # Extract embedding vectors for every word in review, making sure to handle out-of-vocab words\r\n",
        "    if tmp is not None:\r\n",
        "      dim = tmp.shape[1]\r\n",
        "      if out is not None:\r\n",
        "        vec = np.mean(tmp, axis=0)                  # Average word vectors in each review\r\n",
        "        vec = vec.reshape((1, dim))\r\n",
        "        out = np.concatenate((out, vec), axis=0)    # Concatenate average row vector to output Numpy array\r\n",
        "      else:\r\n",
        "        out = np.mean(tmp, axis=0).reshape((1, dim))\r\n",
        "    else:\r\n",
        "      pass     # Every-word-out-of- vocab edge case handling\r\n",
        "\r\n",
        "  return out"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IWd13c6g1sd"
      },
      "source": [
        "Having obtained and loaded the pre-trained embedding, let’s look back at the IMDB movie review classification example, which we will be analyzing in this section.\r\n",
        "\r\n",
        "If you have already proceeded to generate a simple bag-of-words representation for the output Numpy array – which simply counts occurrence frequencies of possible word tokens in each review. We then used the resulting vectors as numerical features for further machine learning tasks. \r\n",
        "\r\n",
        "Here, instead of the bag-of-words representation, we extract corresponding vectors from the pretrained embedding instead.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0B6ES3pgRk-"
      },
      "source": [
        "# shuffle raw data first\r\n",
        "def unison_shuffle_data(data, header):\r\n",
        "    p = np.random.permutation(len(header))\r\n",
        "    data = data[p]\r\n",
        "    header = np.asarray(header)[p]\r\n",
        "    return data, header\r\n",
        "\r\n",
        "# load data in appropriate form\r\n",
        "def load_data(path):\r\n",
        "  data, sentiments = [], []\r\n",
        "  for folder, sentiment in ((\"neg\", 0), (\"pos\", 1)):\r\n",
        "    folder = os.path.join(path, folder)\r\n",
        "    for name in os.listdir(folder):    # Go through every file in current folder\r\n",
        "      with open(os.path.join(folder, name), \"r\") as reader:\r\n",
        "        text = reader.read()\r\n",
        "      # Apply tokenization, stopword analysis routines\r\n",
        "      text = tokenize(text)\r\n",
        "      text = stop_word_removal(text)\r\n",
        "      text = reg_expressions(text)\r\n",
        "      # Track corresponding text and sentiment labels\r\n",
        "      data.append(text)\r\n",
        "      sentiments.append(sentiment)\r\n",
        "  # Convert to Numpy array\r\n",
        "  #print(data)\r\n",
        "  data_np = np.array(data)\r\n",
        "  #print(data_np[:10])\r\n",
        "  data, sentiments = unison_shuffle_data(data_np, sentiments)\r\n",
        "\r\n",
        "  return data, sentiments"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZVjioXgAvwM"
      },
      "source": [
        "## Preparing and assembling Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0Ui4P9amO9X",
        "outputId": "681747de-0cad-4e4b-fbc0-a838de8fc761",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_path = os.path.join(\"aclImdb\", \"train\")\r\n",
        "test_path = os.path.join(\"aclImdb\", \"test\")\r\n",
        "raw_data, raw_header = load_data(train_path)\r\n",
        "\r\n",
        "print(raw_data.shape)\r\n",
        "print(len(raw_header))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000,)\n",
            "25000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUaZAzHKn8nt",
        "outputId": "9640feba-3444-4942-aa65-80a4defff22e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Subsample required number of samples\r\n",
        "random_indices = np.random.choice(range(len(raw_header)), size=(n_sample * 2,), replace=False)\r\n",
        "data_train = raw_data[random_indices]\r\n",
        "header = raw_header[random_indices]\r\n",
        "\r\n",
        "print(\"DEBUG::data_train::\")\r\n",
        "print(data_train[:10])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEBUG::data_train::\n",
            "[list(['this', 'first', 'guinea', 'pig', 'series', 'one', 'infamous', 'films', 'collectionbr', 'br', 'it', 'took', 'long', 'time', 'finally', 'man', 'get', 'hands', 'copy', 'notorious', 'group', 'films', 'i', 'bought', 'guinea', 'pig', 'box', 'set', 'decided', 'watch', 'collection', 'order', 'release', 'date', 'so', 'i', 'popped', 'sucker', 'sat', 'downbr', 'br', 'from', 'i', 'read', 'internet', 'realizing', 'content', 'involved', 'film', 'i', 'expecting', 'test', 'nerves', 'full', 'force', 'this', 'ended', 'casebr', 'br', 'the', 'film', 'focuses', 'group', 'men', 'kidnap', 'woman', 'begin', 'torturing', 'hopes', 'discovering', 'human', 'breaking', 'point', 'long', 'human', 'tolerate', 'painbr', 'br', 'sounds', 'like', 'one', 'sick', 'flick', 'right', 'wrong', 'the', 'film', 'fails', 'shock', 'this', 'may', 'i', 'become', 'desensitized', 'years', 'obsession', 'horror', 'i', 'think', 'safe', 'say', 'true', 'gorehound', 'could', 'sit', 'easebr', 'br', 'on', 'hand', 'individual'])\n",
            " list(['just', 'saw', 'movie', 'scary', 'thing', 'was', 'people', 'talking', 'movie', 'sounded', 'like', 'actors', 'the', 'movie', 'moments', 'also', 'lagged', 'rather', 'sick', 'it', 'meant', 'farce', 'see', 'pathetic', 'lives', 'people', 'movie', 'think', 'people', 'like', 'around', 'us', 'all', 'attempts', 'getting', 'audiences', 'sympathy', 'dashed', 'actors', 'one', 'stupid', 'thing', 'another', 'on', 'plus', 'side', 'great', 'and', 'funny', 'insults', 'i', 'think', 'i', 'would', 'wait', 'video', 'good', 'laugh', 'warning', 'jerry', 'takes', 'shirt', 'movie', 'not', 'pretty', 'sight'])\n",
            " list(['several', 'years', 'ago', 'i', 'first', 'watched', 'grey', 'gardens', 'i', 'remember', 'laughing', 'finding', 'hilarious', 'camp', 'years', 'later', 'i', 'still', 'laugh', 'loud', 'i', 'watch', 'it', 'many', 'viewings', 'ive', 'come', 'see', 'beauty', 'strange', 'twisted', 'relationship', 'inseparable', 'big', 'edith', 'bouvier', 'beale', 'daughter', 'little', 'edith', 'bouvier', 'bealebr', 'br', 'mother', 'daughter', 'living', 'together', 'decaying', '', 'room', 'east', 'hampton', 'mansion', 'add', 'whole', 'new', 'meaning', 'term', 'shabby', 'chic', 'with', 'innumerable', 'cats', 'raccoons', 'opossums', 'roommates', 'aunt', 'niece', 'jackie', 'o', 'allowed', 'filmmakers', 'albert', 'david', 'maysles', 'mansion', 'film', 'living', 'life', 'day', 'day', 'the', 'result', 'hilarious', 'beautiful', 'sad', 'moving', 'account', 'true', 'love', 'anarchy', 'rulebr', 'br', 'the', 'relationship', 'big', 'little', 'edie', 'testament', 'unbreakable', 'bonds', 'love', 'and', 'lives', 'example', 'drive', 'determination', 'freewill', 'this', 'movie', 'recommend', 'i', 'put', 'words', 'it', 'rare', 'experience', 'must', 'see', 'yourselfbr', 'br', ''])\n",
            " list(['i', 'think', 'ill', 'ever', 'understand', 'hate', 'renny', 'harlin', 'die', 'hard', '', 'cool', 'gave', 'world', 'cliffhanger', 'one', 'awesome', 'action', 'movies', 'ever', 'thats', 'right', 'little', 'punks', 'cliffhanger', 'rules', 'know', 'itbr', 'br', 'sly', 'plays', 'gabe', 'walker', 'former', 'rescue', 'climber', 'just', 'visiting', 'old', 'town', 'asked', 'help', 'former', 'friend', 'hal', 'tucker', 'michael', 'rooker', 'assist', 'rescue', 'mountain', 'peak', 'walker', 'obviously', 'came', 'back', 'convenient', 'time', 'stranded', 'people', 'actually', 'sophisticated', 'team', 'thieves', 'led', 'eric', 'qualen', 'john', 'lithgow', 'qualen', '', 'co', 'lost', 'whole', 'lot', 'money', 'stole', 'us', 'government', 'somewhere', 'rocky', 'mountains', 'really', 'would', 'like', 'backbr', 'br', 'essentially', 'cliffhanger', 'another', 'die', 'hard', 'clone', 'just', 'trade', 'confines', 'nakatomi', 'plaza', 'open', 'mountain', 'ranges', 'rocky', 'mountains', 'complete', 'scenes', 'created', 'point', 'weaknesses', 'hero', 'keep', 'mortal', 'naturally', 'set', 'totally', 'ripped', 'shreds', 'soon', 'enough', 'stallones', 'character', 'avoids', 'quite', 'large', 'number', 'bullets', 'ease', 'slams', 'facefirst'])\n",
            " list(['br', 'br', 'this', 'without', 'doubt', 'funniest', 'comedy', 'year', 'everybody', 'brilliant', 'the', 'acting', 'superb', 'you', 'see', 'actors', 'enjoyed', 'making', 'film', 'its', 'shame', 'spoil', 'film', 'give', 'aways', 'rent', 'laugh', 'ass', 'offbr', 'br', '', '', ''])\n",
            " list(['', 'years', 'old', 'musical', 'comedy', 'fantasy', 'might', 'look', 'age', 'wears', 'dignitybr', 'br', 'this', 'film', 'still', 'great', 'fun', 'crosby', 'never', 'really', 'romantic', 'lead', 'material', 'delivers', 'material', 'lightly', 'humorous', 'edge', 'needs', 'bendix', 'plays', 'broad', 'huge', 'fun', 'part', 'calls', 'upon', 'strengths', 'hardwicke', '', 'joyous', 'knight', 'realm', '', 'genuine', 'one', '', 'throw', 'caperings', 'like', 'abandon', 'and', 'rhonda', 'fleming', 'enjoys', 'least', 'showy', 'main', 'roles', 'only', 'murvyn', 'vye', 'disappoints', 'unconvincing', 'merlinbr', 'br', 'though', 'musical', 'songs', 'good', 'dance', 'routine', 'accompanying', 'busy', 'doing', 'nothing', 'perfect', '', 'funny', 'appropriate', 'dexterous', 'without', 'challenging', 'making', 'virtue', 'crosbys', 'musical', 'movement', 'which', 'lets', 'fair', 'inherently', 'amusing', 'due', 'ti', 'never', 'greatest', 'strengthbr', 'br', 'the', 'colour', 'fine', 'sound', 'little', 'muddy', 'placesbr', 'br', 'and', 'story', '', 'well', 'takes', 'liberties', 'original', 'i', 'suspect', 'mr', 'clemens', 'might', 'well', 'pleased', 'result'])\n",
            " list(['probably', 'one', 'boriest', 'slasher', 'movies', 'ever', 'badly', 'acted', 'badly', 'writtenbr', 'br', 'the', 'plot', 'five', 'students', 'staying', 'behind', 'holidays', 'closing', 'dorm', 'somebody', 'designs', 'starts', 'killing', 'one', 'one', 'main', 'suspect', 'creepy', 'groundskeeper', 'john', 'hemmitt', 'played', 'woody', 'roll', 'could', 'one', 'five', 'charactersbr', 'br', 'acting', 'not', 'bad', 'great', 'either', 'apart', 'daphne', 'zungia', 'dies', 'way', 'quickly', 'main', 'heroine', 'rest', 'well', 'quite', 'dull', 'although', 'laura', 'lapinski', 'main', 'heroine', 'sometimes', 'charm', 'feel', 'sorry', 'endbr', 'br', 'the', 'kills', 'cant', 'really', 'see', 'banned', 'this', 'kills', 'look', 'fake', 'mostly', 'one', 'guy', 'hand', 'sliced', 'half', 'beginning', 'looks', 'really', 'fake', 'others', 'quite', 'nasty', 'like', 'one', 'girl', 'gets', 'head', 'run', 'car', 'one', 'girl', 'gets', 'boiled', 'alive', 'another', 'gets', 'burned', 'alivebr', 'br', 'overall', 'not', 'really', 'great', 'slasher', 'could', 'lot', 'better'])\n",
            " list(['i', 'saw', 'film', 'watching', 'capote', 'infamous', 'it', 'incredible', 'homosexual', 'relationships', 'author', 'protagonists', 'sublimated', 'movie', 'the', 'reporter', 'straight', 'protagonists', 'beatniks', 'gaybr', 'br', 'the', 'film', 'starts', 'slowly', 'reviewing', 'second', 'time', 'get', 'sorts', 'interesting', 'information', 'similes', 'writerdirector', 'brooks', 'createsbr', 'br', 'notice', 'incredible', 'cutting', 'beginning', 'killers', 'tobekilled', 'linked', 'cutter', 'phone', 'matchedcut', 'perry', 'phone', 'cutter', 'washing', 'face', 'matchedcut', 'perry', 'washing', 'face', 'only', 'perrys', 'looking', 'mirror', 'seeing', 'eroticized', 'male', 'body', 'sets', 'fantasy', 'playing', 'guitar', 'las', 'vegas', 'empty', 'chairs', 'this', 'failurefantasy', 'matches', 'failurefantasy', 'perry', 'tells', 'us', 'father', 'built', 'beautiful', 'motel', 'alaska', 'find', 'perpetually', 'emptybr', 'br', 'dick', 'talks', 'shooting', 'pheasants', 'fact', 'pheasants', 'know', 'theyre', 'going', 'die', 'cut', 'cluttersbr', 'br', 'perry', 'talks', 'dream', 'yellow', 'bird', 'taller', 'jesus', 'attacks', 'nuns', 'persecuted', 'them', 'the', 'nuns', 'begged'])\n",
            " list(['i', 'know', 'stars', 'modern', 'chinese', 'teenage', 'music', '', 'i', 'know', 'thoroughly', 'entertaining', 'movie', 'i', 'see', 'onebr', 'br', 'kung', 'fu', 'dunk', 'pure', 'hollywood', 'values', '', 'played', 'laughs', 'love', 'great', 'blend', 'kung', 'fu', 'basketballbr', 'br', 'everybody', 'looks', 'like', 'lot', 'fun', 'making', '', 'production', 'values', 'excellent', '', 'modern', 'china', 'looks', 'glossier', 'los', 'angeles', 'herebr', 'br', 'the', 'plot', 'abandoned', 'orphan', 'grows', 'kung', 'fu', 'school', 'kicked', 'discover', 'superstardom', 'basketball', 'play', 'and', 'love', 'etc', 'great', '', 'fresh', 'fun', 'immensely', 'entertainingbr', 'br', 'with', 'great', 'action', 'good', 'dialogue', 'one', 'simply', 'enjoy', '', 'ages', '', 'money', 'one', 'best', 'family', 'movies', 'were', 'seen', 'long', 'timebr', 'br', 'please', 'ignore', 'negative', 'reviews', 'give', 'dunk', 'chance', '', 'really', 'glad', '', 'good', 'sports', 'comedy', 'movie'])\n",
            " list(['when', 'i', 'saw', 'movie', 'circa', '', 'became', 'first', 'movie', 'i', 'ever', 'walked', 'middle', 'there', 'nothing', 'worse', 'comedy', 'misses', 'funny', 'misses', 'every', 'time', 'although', 'i', 'cant', 'speak', 'last', '', 'minutes', 'movie', 'there', 'nothing', 'original', 'skits', 'while', 'i', 'enjoy', 'racy', 'humor', 'appropriate', 'skits', 'needlessly', 'vulgar', 'what', 'even', 'irritating', 'movie', 'advertised', 'robin', 'williams', 'first', 'movie', 'capitalizing', 'new', 'found', 'fame', 'mork', 'mindy', 'television', 'series', 'yet', 'role', 'turned', 'minor', 'cannot', 'even', 'notice', 'onscreen'])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ4l4h4woI-D"
      },
      "source": [
        "Display sentiments and their frequencies in the dataset, to ensure it is roughly balanced between classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK5spFrgoJo0",
        "outputId": "e4c83e54-09f6-44e6-dfd8-37c5c0fc8c5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "unique_elements, counts_elements = np.unique(header, return_counts=True)\r\n",
        "\r\n",
        "print(\"Sentiments and their frequencies:\")\r\n",
        "print(unique_elements)\r\n",
        "print(counts_elements)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiments and their frequencies:\n",
            "[0 1]\n",
            "[1004  996]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5Ovl30oof9e"
      },
      "source": [
        "We can now assemble embedding vectors for the whole dataset using the function call:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRFIQACaoht_",
        "outputId": "0c3cd4cf-cc7b-4391-9ba6-5ea1b5656a46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "embedding_vectors = assemble_embedding_vectors(data_train) \r\n",
        "print(embedding_vectors)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.07434048 -0.0556974  -0.15833633 ...  0.1433645   0.0877356\n",
            "  -0.01411985]\n",
            " [-0.11002415 -0.01271672 -0.16738765 ...  0.11665359  0.12732764\n",
            "   0.02757509]\n",
            " [-0.05130713 -0.03334703 -0.13207616 ...  0.14222325  0.11938123\n",
            "   0.05124996]\n",
            " ...\n",
            " [-0.07599656 -0.02869177 -0.20326632 ...  0.11224462  0.09450471\n",
            "   0.02920728]\n",
            " [-0.11688604  0.03800548 -0.15369247 ...  0.10817185  0.06381336\n",
            "   0.02729191]\n",
            " [-0.07387373 -0.03575005 -0.13989347 ...  0.14872488  0.06448585\n",
            "  -0.0261879 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTdwKCH3orwX"
      },
      "source": [
        "These can now be used as feature vectors for the same logistic regression and random forest.\r\n",
        "\r\n",
        "As the very last step of preparing the sentiment dataset for training by our baseline classifiers, we split it into independent training and testing or validation sets. This will allow us to evaluate the performance of the classifier on a set of data that was not used for training, an important thing\r\n",
        "to ensure in machine learning practice. We elect to use 70% of the data for training, and 30% for testing/validation afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKGoLpWJqof9",
        "outputId": "2ecc8e9b-9fb8-4955-b78f-62b663f15385",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data = embedding_vectors\r\n",
        "\r\n",
        "idx = int(0.7 * data.shape[0])\r\n",
        "\r\n",
        "\r\n",
        "# 70% of data for training\r\n",
        "train_x = data[:idx, :]\r\n",
        "train_y = header[:idx]\r\n",
        "\r\n",
        "# remaining 30% for testing\r\n",
        "test_x = data[idx:, :]\r\n",
        "test_y = header[idx:]\r\n",
        "\r\n",
        "print(\"train_x/train_y list details, to make sure it is of the right form:\")\r\n",
        "print(len(train_x))\r\n",
        "print(train_x[:5])\r\n",
        "print(len(train_y))\r\n",
        "print(train_y[:5])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_x/train_y list details, to make sure it is of the right form:\n",
            "1400\n",
            "[[-0.07434048 -0.0556974  -0.15833633 ...  0.1433645   0.0877356\n",
            "  -0.01411985]\n",
            " [-0.11002415 -0.01271672 -0.16738765 ...  0.11665359  0.12732764\n",
            "   0.02757509]\n",
            " [-0.05130713 -0.03334703 -0.13207616 ...  0.14222325  0.11938123\n",
            "   0.05124996]\n",
            " [-0.05703479 -0.06725627 -0.11381611 ...  0.16412966  0.10293766\n",
            "   0.00378084]\n",
            " [-0.15607004 -0.0485235  -0.17547995 ...  0.11286472  0.05861365\n",
            "   0.01738076]]\n",
            "1400\n",
            "[1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLaR7EhprYDN"
      },
      "source": [
        "## Logistic Regression Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVbev8il84H_"
      },
      "source": [
        "Logistic regression models the relationship between a categorical output variable and a set of input variables by estimating probabilities with the logistic function. Assuming the existence of a single input variables x, and a single output binary variable y with associated probability $P(y=1)=p$.\r\n",
        "\r\n",
        "Now, let’s go ahead and build our classifier using the popular library scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcaPrNio85Bd"
      },
      "source": [
        "def fit(train_x, train_y):\r\n",
        "  model = LogisticRegression()\r\n",
        "\r\n",
        "  try:\r\n",
        "    model.fit(train_x, train_y)\r\n",
        "  except:\r\n",
        "    pass\r\n",
        "  \r\n",
        "  return model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWcyRJ1c9Swi",
        "outputId": "fa717bfa-1986-4884-dc9d-dd38a30ffdfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = fit(train_x, train_y)\r\n",
        "\r\n",
        "predicted_labels = model.predict(test_x)\r\n",
        "print(\"DEBUG::The logistic regression predicted labels are::\")\r\n",
        "print(predicted_labels)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEBUG::The logistic regression predicted labels are::\n",
            "[1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1\n",
            " 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1\n",
            " 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0\n",
            " 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0\n",
            " 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1\n",
            " 1 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0\n",
            " 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1\n",
            " 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0\n",
            " 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0\n",
            " 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0\n",
            " 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 1\n",
            " 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1\n",
            " 0 0 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0\n",
            " 1 0 1 1 0 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqcD5AQR95sQ",
        "outputId": "e6946fdf-1b17-406f-f148-36283ad7e0c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "acc_score = accuracy_score(test_y, predicted_labels)\r\n",
        "print(\"The logistic regression accuracy score is::\")\r\n",
        "print(acc_score)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The logistic regression accuracy score is::\n",
            "0.785\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ox5vV5l-AQJ"
      },
      "source": [
        "## Random Forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MlSViQI-Nrf"
      },
      "source": [
        "Random Forests (RFs) provide a practical machine learning method for applying decision trees. It involves generating a very large number of specialized trees and ensembling their outputs. RFs are extremely flexible and widely applicable, making them often the second algorithm practitioners try after logistic regression for baselining."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JuYvJW4-VQx",
        "outputId": "676b3c8d-c8c7-4ae4-d445-b4366e53469f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create a random forest Classifier. By convention, clf means 'Classifier'\r\n",
        "clf = RandomForestClassifier(n_jobs=1, random_state=0)\r\n",
        "\r\n",
        "# Train the Classifier to take the training features and learn how they relate to the training y (spam, not spam?)\r\n",
        "start_time = time.time()\r\n",
        "clf.fit(train_x, train_y)\r\n",
        "end_time = time.time()\r\n",
        "print(\"Training the Random Forest Classifier took %3d seconds\"%(end_time-start_time))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training the Random Forest Classifier took   1 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QHhV-a2C-sQ",
        "outputId": "3918aa90-2160-4db7-cfc5-26ccb72b1eed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "predicted_labels = clf.predict(test_x)\r\n",
        "print(\"DEBUG::The RF predicted labels are::\")\r\n",
        "print(predicted_labels)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEBUG::The RF predicted labels are::\n",
            "[1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1\n",
            " 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1\n",
            " 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0\n",
            " 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1\n",
            " 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1\n",
            " 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 1 1 0 0\n",
            " 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 0 0\n",
            " 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1\n",
            " 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0\n",
            " 1 0 1 1 0 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK_uykXWDGsE",
        "outputId": "90e1134a-9276-4565-b106-d5a40a493ddb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "acc_score = accuracy_score(test_y, predicted_labels)\r\n",
        "print(\"DEBUG::The RF testing accuracy score is::\")\r\n",
        "print(acc_score)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEBUG::The RF testing accuracy score is::\n",
            "0.7483333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8YWlnanFIh2"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHOwQobpD0Wq"
      },
      "source": [
        "We found the corresponding accuracy scores to be 78% and 74% respectively\r\n",
        "when the hyperparameters maxtokens and maxtokenlen are set to 200 and 100 respectively, and the value of n_samp, i.e., the number of samples from each class, being equal to 1000.\r\n",
        "\r\n",
        "These are only slightly lower than the corresponding values obtained from the bag-of-words baseline(corresponding to accuracy scores of 79% and 67% respectively). \r\n",
        "\r\n",
        "We hypothesize that this slight deprecation is likely due to the aggregation of individual word vectors by the naive averaging approach that was described. In\r\n",
        "the next notebook, we attempt to perform more intelligent aggregation using embedding methods that were designed to embed at a higher text level."
      ]
    }
  ]
}
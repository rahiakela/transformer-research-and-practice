{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4-transfer-learning-using-domain-adaptation.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPc2frDphLxxqDfOBL3c/A8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transfer-learning-for-natural-language-processing/blob/main/3-shallow-transfer-learning-for-nlp/4_transfer_learning_using_domain_adaptation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47P5vUyRmABC"
      },
      "source": [
        "## Transfer Learning using Domain Adaptation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqn_lp-LmA5-"
      },
      "source": [
        "In this notebook, we will cover some prominent shallow transfer learning approaches and concepts. This allows us to explore some major themes in transfer learning, while doing so in the context of relatively simple models of the class of eventual interest, i.e., shallow neural networks.\n",
        "\n",
        "Roughly speaking, categorization is based on whether transfer occurs between different languages, tasks or data domains. Each of these types of categorization is usually correspondingly referred to as cross-lingual learning, multi-task learning and domain adaptation.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transfer-learning-for-natural-language-processing/shallow-transfer-learning.png?raw=1' width='800'/>\n",
        "\n",
        "The methods we will look at here will involve components that are neural networks in one way or another.these neural networks do not have many\n",
        "layers. This is the reason why the label “shallow” is appropriate to describe this collection of methods.\n",
        "\n",
        "A common form of semi-supervised learning that employs pretrained word embeddings such as word2vec that they produce a single vector per word, regardless of context.\n",
        "\n",
        "We revisit the IMDB movie review sentiment classification. Recall that this example is concerned with classifying movie reviews from IMDB into positive or negative sentiments expressed. It is a prototypical sentiment analysis example that has been used widely in the literature to study many algorithms. We combine feature vectors generated by pretrained word embeddings for each review with some traditional machine learning classification methods, namely random forests and logistic regression.\n",
        "\n",
        "We then demonstrate that using higher-level embeddings which vectorize bigger sections of text – such as at the sentence-level, paragraphlevel and document-level – can lead to improved performance. The general idea of vectorizing text and then applying a traditional machine learning classification method to the resulting vectors.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transfer-learning-for-natural-language-processing/semi-supervised-learning.png?raw=1' width='800'/>\n",
        "\n",
        "**Multi-task learning**\n",
        "\n",
        "Subsequently, we introduce the reader to multi-task learning. We demonstrate how one can train a single system simultaneously to perform multiple tasks, email spam classification and IMDB movie review sentiment analysis. \n",
        "\n",
        "There are several potential benefits to multi-task learning. By\n",
        "training a single machine learning model for multiple tasks, a shared representation is learned on a larger and more varied collection of data from the combined data pool, which can lead to performance improvements. Moreover, it has been widely observed that this shared representation has a better ability to generalize to tasks beyond those that were trained on, and\n",
        "this improvement can be achieved without any increase in model size.\n",
        "\n",
        "Specifically, we focus on shallow neural multitask learning, where a single additional dense layer, as well as a classification layer, is trained\n",
        "for each specific task in the setup. Different tasks also share a layer between them, a setup typically referred to as hard-parameter sharing.\n",
        "\n",
        "**Domain adaptation**\n",
        "\n",
        "Assume that we are given one source domain, which can be defined as a particular distribution of data for a specific task, and a classifier that has been trained to perform well on data in that domain for that task. The goal of domain adaptation is to modify, or adapt, data in a different target domain in such a way that the pretrained knowledge from the source domain can aid\n",
        "learning in the target domain. We apply a simple autoencoding approach to “project” samples in the target domain into the source domain feature space.\n",
        "\n",
        "An autoencoder is a system that learns to reconstruct inputs with very high accuracy, typically by encoding them into an efficient latent representation and learning to decode the said representation efficiently. They have traditionally been heavily used in model reduction applications, since the latent representation is often of smaller dimension than the original space\n",
        "from which the encoding happens, and the said dimension value can also be picked to strike the right balance of computational efficiency and accuracy.\n",
        "\n",
        "In the extreme scenario, improvements can be obtained with no labelled data in the target domain being used for training. This is typically referred to as zero-shot domain adaptation, where learning happens with no labeled\n",
        "data in the target domain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyDqdxsIrU-c"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUCq5eAN9lGL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fb58ea4-2b20-4d0d-e067-466cb9cdf9d3"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0TABXTr16LE"
      },
      "source": [
        "# install sent2vec\n",
        "!pip install git+https://github.com/epfml/sent2vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMiwOYhVrWTc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06e482c1-eb56-4cdd-8adc-51d846302a71"
      },
      "source": [
        "import numpy as np  # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import email\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, concatenate\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import sent2vec\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxaKpdoQsMTl"
      },
      "source": [
        "Download IMDB Movie Review Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-v_6vdlsIsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25d9e99e-0d35-4a7a-e640-69be3ad5f4aa"
      },
      "source": [
        "%%shell\n",
        "\n",
        "wget -q \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "tar xzf aclImdb_v1.tar.gz\n",
        "\n",
        "rm -rf aclImdb_v1.tar.gz\n",
        "rm -rf aclImdb/train/unsup"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wukokb1HJ-ty"
      },
      "source": [
        "Download Book Reviews dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJ6qPYnFKD0p",
        "outputId": "d1d5aabf-cda4-434e-baa7-e89e38c1fffe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "\n",
        "wget -q \"https://github.com/rahiakela/transfer-learning-for-natural-language-processing/raw/main/3-shallow-transfer-learning-for-nlp/datasets/books.review.zip\"\n",
        "unzip -o books.review.zip\n",
        "\n",
        "rm -rf books.review.zip"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  books.review.zip\n",
            "  inflating: books.negative.review   \n",
            "  inflating: books.positive.review   \n",
            "  inflating: dvd.negative.review     \n",
            "  inflating: dvd.positive.review     \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJDB_TEr268H"
      },
      "source": [
        "Let's download sent2vec word Embedding from [Kaggle](https://www.kaggle.com/maxjeblick/sent2vec)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWCkjW6_27i5"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload() # upload kaggle.json file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQmD0SLz2990",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f06cf6a7-4fbd-4cdd-82df-4f3e3071c844"
      },
      "source": [
        "%%shell\n",
        "\n",
        "mkdir -p ~/.kaggle\n",
        "mv kaggle.json ~/.kaggle/\n",
        "ls ~/.kaggle\n",
        "chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# download word embeddings from kaggle\n",
        "kaggle datasets download -d maxjeblick/sent2vec/wiki_unigrams.bin\n",
        "unzip -qq sent2vec.zip\n",
        "rm -rf sent2vec.zip"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n",
            "Downloading sent2vec.zip to /content\n",
            "100% 4.42G/4.43G [01:27<00:00, 69.4MB/s]\n",
            "100% 4.43G/4.43G [01:27<00:00, 54.3MB/s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxBE6zjK18YQ"
      },
      "source": [
        "def extract_messages(df):\n",
        "  messages = []\n",
        "  for item in df[\"message\"]:\n",
        "    # Return a message object structure from a string\n",
        "    e = email.message_from_string(item)\n",
        "    # get message body\n",
        "    message_body = e.get_payload()\n",
        "    messages.append(message_body)\n",
        "  print(\"Successfully retrieved message body from e-mails!\")\n",
        "  return messages"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve9bdhwlsZYD"
      },
      "source": [
        "## Preprocessing IMDB Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfJ_dH6ysi_i"
      },
      "source": [
        "Before proceeding, we must decide how many samples to draw from each class. We must also decide the maximum number of tokens per email, and the maximum length of each token. This is done by setting the following overarching hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg7YwUgZsm5i"
      },
      "source": [
        "n_sample = 1000    # number of samples to generate in each class\n",
        "maxtokens = 200    # the maximum number of tokens per document\n",
        "maxtokenlen = 100  # the maximum length of each token"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hXCbqnZsutN"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLgxb7mDs2XN"
      },
      "source": [
        "Let’s proceed by defining a function to tokenize text by splitting them into \n",
        "words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoKGXGwDsyHE"
      },
      "source": [
        "def tokenize(row):\n",
        "  if row is None or row is \"\":\n",
        "    tokens = \"\"\n",
        "  else:\n",
        "    tokens = str(row).split(\" \")[:maxtokens]\n",
        "  return tokens"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fxn9b_3s_EE"
      },
      "source": [
        "### Remove punctuation and unnecessary characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSlPjAVatAaI"
      },
      "source": [
        "**In order to ensure that classification is done based on language content only, we have to remove punctuation marks and other non-word characters from the emails.** We do this by employing regular expressions with the Python regex library. We also normalize words by turning them into lower case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KFGweAXtEn0"
      },
      "source": [
        "def reg_expressions(row):\n",
        "  tokens = []\n",
        "  try:\n",
        "    for token in row:\n",
        "      token = token.lower()          # make all characters lower case\n",
        "      token = re.sub(r\"[\\W\\d]\", \"\", token)\n",
        "      token = token[:maxtokenlen]    # truncate all tokens to hyperparameter maxtokenlen\n",
        "      tokens.append(token)\n",
        "  except:\n",
        "    token = \"\"\n",
        "    tokens.append(token)\n",
        "  return tokens"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX10nyiitTJq"
      },
      "source": [
        "### Stop-word removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qzq_2XGJtUS-"
      },
      "source": [
        "Stop-words are also removed. Stop-words are words that are very common in text but offer no useful information that can be used to classify the text. Words such as is, and, the, are are examples of stop-words. The NLTK library contains a list of 127 English stop-words and can be used to filter our tokenized strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfJ7DLGatWyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28ab423a-4a60-445a-f527-e2b79b05f9df"
      },
      "source": [
        "stop_words = stopwords.words(\"english\")\n",
        "print(stop_words)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82F_djDNuV7i"
      },
      "source": [
        "# print(stopwords) # see default stopwords\n",
        "# it may be beneficial to drop negation words from the removal list, as they can change the positive/negative meaning\n",
        "# of a sentence\n",
        "# stopwords.remove(\"no\")\n",
        "# stopwords.remove(\"nor\")\n",
        "# stopwords.remove(\"not\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJItvg2suYML"
      },
      "source": [
        "def stop_word_removal(row):\n",
        "  token = [token for token in row if token not in stop_words]\n",
        "  token = filter(None, token)\n",
        "\n",
        "  return token"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHgiw5cGIDlO"
      },
      "source": [
        "### Load pre-trained sent2vec embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQIRWWMnIJ28"
      },
      "source": [
        "Quite naturally, just as in the case of the pretrained word embeddings, the next step is to obtain the pretrained sent2vec sentence embedding to be loaded by the particular implementation/framework installed.\n",
        "\n",
        "We choose the smallest 600-dimensional embedding `wiki_unigrams.bin`, approximately 5 Gigabytes in size, which captures just the unigram information on Wikipedia.\n",
        "\n",
        "Now let's load the pre-trained embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4FOzCbDIQzN",
        "outputId": "3045dd9d-5b38-43e4-d68e-311c8ddc885f"
      },
      "source": [
        "# load sent2vec embedding\n",
        "s2v_model = sent2vec.Sent2vecModel()\n",
        "\n",
        "start = time.time()\n",
        "s2v_model.load_model(\"wiki_unigrams.bin\")\n",
        "end = time.time()\n",
        "\n",
        "print(\"Loading the sent2vec embedding took %d seconds\" % (end - start))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading the sent2vec embedding took 17 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gtdu7KhIXoP"
      },
      "source": [
        "### Extract corresponding vectors from the pretrained word embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxq3oU8gIYmY"
      },
      "source": [
        "Next, we define a function to generate vectors for a collection of reviews. It is essentially a simpler form of the function for pretrained word embeddings – it is simpler as we do not need to worry about out-of-vocabulary words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuRIMSVhIci_"
      },
      "source": [
        "def assemble_embedding_vectors(data):\n",
        "  out = None\n",
        "  for item in data:    # Loop through every IMDB review\n",
        "    vec = s2v_model.embed_sentence(\" \".join(str(i) for i in item if i))     # Extract embedding vectors for every word in review, now we dont need to handle out-of-vocab words\n",
        "    if vec is not None:                            # Edge case handling\n",
        "      if out is not None:\n",
        "        out = np.concatenate((out, vec), axis=0)    # Concatenate row vector to output Numpy array\n",
        "      else:\n",
        "        out = vec\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  return out"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wUQr44PInYC"
      },
      "source": [
        "### Preparing and assembling Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO9jKVyDIqP9"
      },
      "source": [
        "# shuffle raw data first\n",
        "def unison_shuffle_data(data, header):\n",
        "    p = np.random.permutation(len(header))\n",
        "    data = data[p]\n",
        "    header = np.asarray(header)[p]\n",
        "    return data, header\n",
        "\n",
        "# load data in appropriate form\n",
        "def load_data(path):\n",
        "  data, sentiments = [], []\n",
        "  for folder, sentiment in ((\"neg\", 0), (\"pos\", 1)):\n",
        "    folder = os.path.join(path, folder)\n",
        "    for name in os.listdir(folder):    # Go through every file in current folder\n",
        "      with open(os.path.join(folder, name), \"r\") as reader:\n",
        "        text = reader.read()\n",
        "      # Apply tokenization, stopword analysis routines\n",
        "      text = tokenize(text)\n",
        "      text = stop_word_removal(text)\n",
        "      text = reg_expressions(text)\n",
        "      # Track corresponding text and sentiment labels\n",
        "      data.append(text)\n",
        "      sentiments.append(sentiment)\n",
        "  # Convert to Numpy array\n",
        "  #print(data)\n",
        "  data_np = np.array(data)\n",
        "  #print(data_np[:10])\n",
        "  data, sentiments = unison_shuffle_data(data_np, sentiments)\n",
        "\n",
        "  return data, sentiments"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Evvyc8voIvt0",
        "outputId": "7eacc8a3-4410-4cc1-ec3b-127be24bcdce"
      },
      "source": [
        "train_path = os.path.join(\"aclImdb\", \"train\")\n",
        "test_path = os.path.join(\"aclImdb\", \"test\")\n",
        "raw_data, raw_header = load_data(train_path)\n",
        "\n",
        "print(raw_data.shape)\n",
        "print(len(raw_header))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000,)\n",
            "25000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jv_t7XjWI5V1",
        "outputId": "6619e51e-d56a-472d-90f7-004659d2a31c"
      },
      "source": [
        "# Subsample required number of samples\n",
        "random_indices = np.random.choice(range(len(raw_header)), size=(n_sample * 2,), replace=False)\n",
        "data_train = raw_data[random_indices]\n",
        "header = raw_header[random_indices]\n",
        "\n",
        "del raw_data, raw_header # huge and no longer needed, get rid of it\n",
        "\n",
        "print(\"DEBUG::data_train::\")\n",
        "print(data_train[:10])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEBUG::data_train::\n",
            "[list(['i', 'know', 'i', 'missed', 'here', 'i', 'cant', 'believe', 'positive', 'comments', 'many', 'people', 'film', 'i', 'thought', 'silly', 'bit', 'top', 'i', 'like', 'performances', 'gregg', 'henry', 'michael', 'rooker', 'however', 'others', 'just', 'boringbr', 'br', 'now', 'i', 'like', 'b', 'movies', 'i', 'really', 'do', 'bit', 'alphabet', 'me', 'i', 'saw', 'someone', 'compare', 'humor', 'horror', 'army', 'of', 'darkness', 'shaun', 'dead', 'well', 'on', 'par', 'the', 'reanimator', 'you', 'must', 'joking', 'i', 'find', 'film', 'funny', 'tried', 'make', 'effort', 'possibly', 'much', 'effort', 'failed', 'opinion', 'by', 'time', 'i', 'hit', 'rd', 'th', 'oneliner', 'i', 'rolling', 'eyes', 'checking', 'watchbr', 'br', 'there', 'definitely', 'homages', 'made', 'several', 'films', 'always', 'cool', 'kind', 'like', 'inside', 'joke', 'us', 'horror', 'fans', 'but', 'may', 'lack', 'original', 'thought', 'admittedly', 'nice', 'special', 'effects', 'good', 'gore', 'cant', 'carry', 'entire', 'movie', 'the'])\n",
            " list(['this', 'painful', 'example', 'cheap', 'boring', 'unoriginal', 'show', 'produced', 'australian', 'tv', 'stations', 'fulfil', 'local', 'content', 'quotas', 'the', 'writing', 'truly', 'terrible', 'im', 'surprised', 'writers', 'responsible', 'worst', 'australian', 'film', 'recent', 'memory', 'honerable', 'wally', 'normanbr', 'br', 'nothing', 'tv', 'series', 'funny', '', 'ever', '', 'even', 'mildly', 'amusing', 'it', 'tired', 'bad', 'and', 'worst', 'all', 'really', 'thought', 'funny', 'it', 'simply', 'embarrassing', 'watchbr', 'br', 'there', 'something', 'suss', 'show', 'given', '', '', 'imdb', 'try', 'find', 'real', 'review', 'real', 'australian', 'viewer', 'show', 'there', 'many', 'impossible', 'or', 'crazy', 'there', 'loafs', 'bread', 'funnier', 'show', 'br', 'br', 'avoid', 'show', 'cost', 'come', 'dvd', 'remember', 'laughter', 'that', 'deserves', 'unintentional'])\n",
            " list(['andie', 'mcdowell', 'beautiful', 'ish', 'woman', 'whose', 'late', 'start', 'serious', 'relationship', 'leads', 'considerably', 'younger', 'man', 'subsequenet', 'fallingout', '', 'longtime', 'best', 'girldfriendsbr', 'br', 'seeing', 'gigologolddigger', 'sincere', 'young', 'man', 'girlfriends', 'deadset', 'terminating', 'silly', 'relationship', 'go', 'beyond', 'call', 'duty', 'helping', 'out', 'friend', 'who', 'obviously', 'blinded', 'gigolos', 'tricky', 'gamebr', 'br', 'a', 'short', 'succession', 'situations', 'absolutely', 'ridiculous', 'far', 'fetched', 'longer', 'covers', 'it', 'without', 'unbelievable', 'scenes', 'may', 'hope', 'sweet', 'love', 'story', 'instead', 'viewer', 'left', 'involuntary', 'shaking', 'head', '', 'things', 'happen', 'without', 'giving', 'away', 'cliffhanger', 'details', 'i', 'warn', 'viewer', 'high', 'expectations', 'film', 'like', 'me', 'disappointed', 'on', 'scale', '', '', 'one', 'ranks', 'weak', '', 'me', 'there', 'much', 'better', 'material', 'there', 'this', 'one', 'worth', 'time'])\n",
            " list(['i', 'picked', 'movie', 'cover', 'alone', 'thinking', 'adventure', 'level', 'indiana', 'jones', 'the', 'temple', 'doom', 'unfortunately', 'i', 'virtual', 'yawn', 'not', 'like', 'yawn', 'though', 'this', 'yawn', 'large', 'could', 'barely', 'find', 'anything', 'quality', 'movie', 'the', 'cover', 'described', 'amazing', 'special', 'effects', 'there', 'none', 'the', 'movie', 'lightweight', 'even', 'stereotypes', 'awfully', 'portrayed', 'it', 'give', 'idea', 'solve', 'problems', 'violence', 'good', 'want', 'teach', 'kids', 'that', 'i', 'dont', 'keep', 'away', 'one', 'if', 'looking', 'family', 'entertainment', 'might', 'find', 'something', 'inspiring', 'elsewhere'])\n",
            " list(['the', 'bfg', 'one', 'roald', 'dahls', 'cherished', 'books', 'animated', 'adaptation', 'magic', 'there', 'this', 'version', 'remains', 'pretty', 'faithful', 'dahls', 'original', 'story', 'one', 'cant', 'lay', 'blame', 'john', 'hambleys', 'script', 'if', 'anything', 'fault', 'lies', 'colourless', 'animation', 'lethargic', 'pace', 'generally', 'lacklustre', 'voiceovers', 'one', 'would', 'right', 'expect', 'story', 'make', 'happy', 'vibrant', 'funfilled', 'movie', 'instead', 'film', 'hopelessly', 'dull', 'affair', 'becomes', 'quite', 'tedious', 'watch', 'children', 'familiar', 'story', 'definitely', 'read', 'book', 'first', 'all', 'film', 'achieve', 'put', 'read', 'actually', 'childrens', 'classicbr', 'br', 'young', 'orphan', 'sophie', 'voice', 'amanda', 'root', 'lives', 'nonetoofriendly', 'orphanage', 'cruel', 'supervision', 'mrs', 'clonkers', 'one', 'evening', 'peering', 'window', 'spots', 'massive', 'figure', 'walking', 'stealthily', 'village', 'street', 'the', 'figure', 'realises', 'seen', 'reaches', 'window', 'scoops', 'sophie', 'bed', 'placing', 'enormous', 'pocket', 'fleeing', 'night', 'sophie', 'soon', 'discovers', 'kidnapped', 'giant'])\n",
            " list(['this', 'ludicrous', 'laughable', 'thriller', 'ive', 'ever', 'seen', 'ohwhere', 'startbr', 'br', 'plot', 'what', 'little', 'is', 'clayton', 'beresford', 'jr', 'hayden', 'christensen', 'young', 'billionaire', 'bad', 'heart', 'desperately', 'need', 'transplant', 'clay', 'secretly', 'engaged', 'mothers', 'pa', 'samantha', 'played', 'jessica', 'alba', 'on', 'night', 'two', 'secretly', 'get', 'married', 'happens', 'heart', 'donor', 'rare', 'blood', 'type', 'found', 'go', 'figure', 'odds', 'one', 'once', 'operating', 'table', 'clay', 'finds', 'anesthesia', 'working', 'feel', 'everything', 'hear', 'everythingbr', 'br', 'fortunately', 'clay', 'seems', 'able', 'filter', 'pain', 'razor', 'sharp', 'scalpel', 'cutting', 'open', 'chest', 'simply', 'concentrating', 'memories', 'samantha', 'told', 'hes', 'annoying', 'voiceover', 'never', 'seems', 'stopbr', 'br', 'if', 'burst', 'laughter', 'yet', 'surely', 'start', 'see', 'surgical', 'scenes', 'br', 'br', 'how', 'could', 'young', 'billionaire', 'agree', 'heart', 'transplant', 'performed', 'one', 'surgeon', 'one', 'nurse', 'attending', 'physician', 'drunk', 'anesthesiologist', 'there', 'people'])\n",
            " list(['i', 'noticed', 'movie', 'really', 'based', 'dodie', 'smiths', 'novel', 'in', 'case', 'nice', 'idea', 'pongo', 'perditas', 'son', 'puppies', 'the', 'cutest', 'dalmatians', 'was', 'course', 'little', 'snowball', 'completely', 'spotless', 'till', 'end', 'film', 'br', 'br', 'to', 'honest', 'i', 'know', 'think', 'cruella', 'de', 'vil', 'seemed', 'changed', 'completely', 'kind', 'in', 'fact', 'i', 'often', 'thought', 'possibility', 'could', 'become', 'friendly', 'quickly', 'changed', 'herself', 'announced', 'cruella', 'more', 'i', 'almost', 'began', 'really', 'worried', 'chloes', 'dalmatiansbr', 'br', 'actually', 'scene', 'puppies', 'watched', 'lady', 'tramp', 'chloe', 'kevin', 'dinner', 'much', 'better', 'i', 'expected', 'i', 'also', 'fond', 'parrot', 'played', 'dog', 'incredible', 'dogs', 'learned', 'many', 'tricks', 'movie', 'br', 'br', 'of', 'course', 'i', 'content', 'end', 'dalmatians', 'saved', 'again', 'i', 'would', 'liked', 'know', 'going', 'happen'])\n",
            " list(['theres', 'enough', 'star', 'power', 'the', 'house', 'of', 'spirits', 'create', 'another', 'galaxy', 'yet', 'final', 'product', 'pretty', 'debatable', 'the', 'film', 'messages', 'noble', 'i', 'think', 'perhaps', 'would', 'agree', 'them', 'liberal', 'democracy', 'good', 'violent', 'fascist', 'regime', 'bad', 'openmindedness', 'good', 'racism', 'bad', 'etc', 'unfortunately', 'were', 'battered', 'head', 'toe', 'these', 'much', 'subtlety', 'used', 'ive', 'described', 'them', 'br', 'br', 'ultimately', 'left', 'watching', 'noble', 'people', 'without', 'flaws', 'squaring', 'nasty', 'cretins', 'redeeming', 'qualities', 'it', 'radiates', 'suspense', 'badly', 'orchestrated', 'pro', 'wrestling', 'matchbr', 'br', 'jeremy', 'irons', 'plays', 'patron', 'man', 'many', 'contradictions', 'meryl', 'streep', 'gifted', 'bride', 'glenn', 'close', 'sister', 'law', 'when', 'camera', 'stays', 'folks', 'movie', 'tends', 'move', 'quite', 'enjoyable', 'unfortunately', 'the', 'house', 'of', 'spirits', 'engages', 'simply', 'way', 'many', 'subplots', 'characters', 'pop', 'picture', 'like', 'shooting', 'gallery', 'targets', 'we', 'get', 'know', 'them', 'hence', 'get', 'care', 'them', 'the', 'result', 'boredom', 'br', 'br', 'if', 'bille', 'august', 'director', 'screenwriter', 'from', 'isabel', 'allendes'])\n",
            " list(['i', 'suppose', 'today', 'film', 'relevance', 'early', 'sofia', 'loren', 'film', 'she', '', 'years', 'old', 'film', 'made', 'br', 'br', 'i', 'viewed', 'film', 'i', 'wanted', 'see', 'sofia', 'lorens', 'early', 'work', 'i', 'surprised', 'came', 'camera', 'skin', 'bronzed', 'brown', 'makeup', 'resemble', 'ethiopian', 'princess', 'surely', 'today', 'would', 'viewed', 'slur', 'avoided', 'movie', 'making', 'it', 'actually', 'became', 'annoying', 'watching', 'ms', 'loren', 'skin', 'color', 'paint', 'throughout', 'filmbr', 'br', 'yes', 'film', 'would', 'better', 'made', 'real', 'opera', 'singers', 'made', 'movie', 'then', 'singing', 'actual', 'facial', 'gestures', 'real', 'artists', 'would', 'apparent', 'i', 'discount', 'comments', 'others', 'whether', 'real', 'opera', 'singers', 'older', 'heavier', 'weightbr', 'br', 'as', 'beautiful', 'ms', 'loren', 'age', '', 'still', 'today', 'film', 'would', 'better', 'received', 'though', 'performed', 'stage', 'after', 'all', 'see', 'beautiful', 'young', 'people', 'stage', 'old', 'opera', 'singers', 'back', 'stage', 'singing', 'behind', 'curtain'])\n",
            " list(['magellan', 'said', 'you', 'much', 'two', 'stars', 'show', 'seen', 'one', 'fellow', 'cast', 'memberbr', 'br', 'i', 'assume', 'then', 'never', 'heard', 'topperbr', 'br', 'which', 'addition', 'two', 'stars', 'could', 'seen', 'one', 'member', 'cast', 'dog', 'dittobr', 'br', 'this', 'kind', 'program', 'not', 'gonna', 'make', 'it', 'written', 'allover', 'first', 'episode', '', 'like', 'arcade', 'video', 'game', 'actually', 'read', 'instructions', 'play', 'noone', 'well', 'us', 'apparently', 'wants', 'watch', 'comedy', 'program', 'basic', 'premise', 'actually', 'requires', 'thought', 'grasp'])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOt9NUPPJBhK"
      },
      "source": [
        "Display sentiments and their frequencies in the dataset, to ensure it is roughly balanced between classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In4l1jWbJCDl",
        "outputId": "f47febab-1bbb-439d-f87c-fa61b9c70bf2"
      },
      "source": [
        "unique_elements, counts_elements = np.unique(header, return_counts=True)\n",
        "\n",
        "print(\"Sentiments and their frequencies:\")\n",
        "print(unique_elements)\n",
        "print(counts_elements)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiments and their frequencies:\n",
            "[0 1]\n",
            "[ 992 1008]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BPUN5yjJQ_u"
      },
      "source": [
        "We can now use this function to extract sent2vec embedding vectors for each review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKjuBHsQJRd9",
        "outputId": "9f2fef39-9f39-4740-ac36-5d97a2a79692"
      },
      "source": [
        "embedding_vectors = assemble_embedding_vectors(data_train) \n",
        "print(embedding_vectors)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.04913538 -0.01056214 -0.04326244 ...  0.09432245 -0.16622916\n",
            "   0.18938614]\n",
            " [ 0.23617683 -0.07827432  0.012788   ... -0.23668842 -0.07756373\n",
            "   0.10961107]\n",
            " [ 0.0593676  -0.09905271 -0.00589911 ... -0.09976954  0.13383257\n",
            "   0.19327204]\n",
            " ...\n",
            " [ 0.01137837  0.03909565 -0.11264261 ... -0.02143067 -0.0557724\n",
            "   0.21194567]\n",
            " [ 0.15456685 -0.14718585  0.01531324 ...  0.02665597 -0.02107718\n",
            "   0.18258451]\n",
            " [-0.07354186  0.08375151 -0.10282756 ...  0.02766571 -0.03726078\n",
            "   0.27708924]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIufakvAJa1P"
      },
      "source": [
        "These can now be used as feature vectors for the same logistic regression and random forest.\n",
        "\n",
        "As the very last step of preparing the sentiment dataset for training by our baseline classifiers, we split it into independent training and testing or validation sets. This will allow us to evaluate the performance of the classifier on a set of data that was not used for training, an important thing\n",
        "to ensure in machine learning practice. We elect to use 70% of the data for training, and 30% for testing/validation afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AK1Oug7qJbWg",
        "outputId": "3bfefb3d-8a9a-4a3a-da9d-69eba52f1008"
      },
      "source": [
        "data = embedding_vectors\n",
        "del embedding_vectors\n",
        "\n",
        "idx = int(0.7 * data.shape[0])\n",
        "\n",
        "\n",
        "# 70% of data for training\n",
        "train_x = data[:idx, :]\n",
        "train_y = np.array(header[:idx])\n",
        "\n",
        "# remaining 30% for testing\n",
        "test_x = data[idx:, :]\n",
        "test_y = np.array(header[idx:])\n",
        "\n",
        "print(\"train_x/train_y list details, to make sure it is of the right form:\")\n",
        "print(len(train_x))\n",
        "print(train_x[:5])\n",
        "print(len(train_y))\n",
        "print(train_y[:5])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_x/train_y list details, to make sure it is of the right form:\n",
            "1400\n",
            "[[ 0.04913538 -0.01056214 -0.04326244 ...  0.09432245 -0.16622916\n",
            "   0.18938614]\n",
            " [ 0.23617683 -0.07827432  0.012788   ... -0.23668842 -0.07756373\n",
            "   0.10961107]\n",
            " [ 0.0593676  -0.09905271 -0.00589911 ... -0.09976954  0.13383257\n",
            "   0.19327204]\n",
            " [-0.02144677 -0.10993782 -0.11229798 ...  0.11676594 -0.10136148\n",
            "   0.12036694]\n",
            " [-0.09403957 -0.12461621  0.00794382 ...  0.04140427 -0.07364064\n",
            "  -0.0288259 ]]\n",
            "1400\n",
            "[0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDsUOgVOuiAM"
      },
      "source": [
        "## Domain Adaptation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xhBEN9ful36"
      },
      "source": [
        "The idea of domain adaptation, one of the oldest and mostprominent\n",
        "ideas in transfer learning. An implicit assumption is made often by machine learning practitioners – that the data during the inference phase will come from the same distribution as the data that was used for training. This is of course rarely true in practice.\n",
        "\n",
        "Enter domain adaptation to attempt to address this issue. Let’s define domain as a particular distribution of data for a specific task. Assume that we are given a source domain and an algorithm that has been trained to perform well on data in that domain. The goal of domain adaptation is to modify, or adapt, data in a different target domain in such a way that the pretrained knowledge from the source domain can be applicable to faster learning and/or direct\n",
        "inference in the target domain. \n",
        "\n",
        "A variety of approaches ranging from multi-task learning - where learning on different data distributions occurs simultaneously, to coordinate transformations that enable more effective learning on a single combined feature space, and methods that exploit measures of similarity between the source and target domains to select which data should be used for training, have been explored.\n",
        "\n",
        "We apply a simple autoencoding approach to “project” samples in the target domain into the source domain feature space. An autoencoder is a system that can learn to reconstruct inputs with very high accuracy, typically by encoding them into an efficient latent representation and learning to decode the said representation. \n",
        "\n",
        "A technical way of describing the process of reconstructing input is “learning the identity function”. They have traditionally been heavily used\n",
        "in model dimensionality reduction applications, since the latent representation is often of smaller dimension than the original space from which the encoding happens, and the said dimension value can also be picked to strike the right balance between computational efficiency and\n",
        "accuracy14. In the extreme favorable scenario, improvements can be obtained with no labeled data in the target domain. This is typically referred to as **zero-shot domain adaptation**.\n",
        "\n",
        "The idea of zero-short transfer learning arises in many contexts. You can think of it as a sort of “holy grail” of transfer learning, since obtaining labeled data in the target domain can be an expensive exercise. \n",
        "\n",
        "Here, we will explore whether a classifier trained to predict the polarity of\n",
        "IMDB movie reviews can be used to predict the polarity of reviews in some other domain. For example, can a classifier trained on IMDB review data predict polarity of book reviews or DVD reviews which are obtained from a completely different data source?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VsyzEDia64R"
      },
      "source": [
        "## Train Shallow Model for IMDB Reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GamXMBu7a9OF"
      },
      "source": [
        "Let's train classification model on IMDB movie review dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgnxik4YbGiE"
      },
      "source": [
        "input_shape = (len(train_x[0]), )\n",
        "\n",
        "# Input must match the dimension of the sent2vec vectors\n",
        "sent2vec_vectors = Input(shape=input_shape)\n",
        "# Dense neural layer trained on top of the sent2vec vectors\n",
        "dense = Dense(512, activation=\"relu\")(sent2vec_vectors)\n",
        "# Apply dropout to reduce overfitting\n",
        "dense = Dropout(0.3)(dense)\n",
        "\n",
        "# Output indicates a single binary classifier - is review “positive” or “negative”?\n",
        "output = Dense(1, activation=\"sigmoid\")(dense)\n",
        "\n",
        "model = Model(inputs=sent2vec_vectors, outputs=output)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jn6zWv3OHth",
        "outputId": "fe40f330-5628-41d3-c407-e1b1a824fd12"
      },
      "source": [
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "history = model.fit(train_x, train_y, validation_data=(test_x, test_y), batch_size=32, epochs=10, shuffle=True)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 4s 7ms/step - loss: 0.6277 - accuracy: 0.6713 - val_loss: 0.4511 - val_accuracy: 0.7917\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.4310 - accuracy: 0.8155 - val_loss: 0.3934 - val_accuracy: 0.8200\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.3725 - accuracy: 0.8376 - val_loss: 0.3857 - val_accuracy: 0.8333\n",
            "Epoch 4/10\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.3202 - accuracy: 0.8766 - val_loss: 0.4140 - val_accuracy: 0.8133\n",
            "Epoch 5/10\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.3123 - accuracy: 0.8713 - val_loss: 0.3820 - val_accuracy: 0.8300\n",
            "Epoch 6/10\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.2857 - accuracy: 0.8816 - val_loss: 0.3921 - val_accuracy: 0.8267\n",
            "Epoch 7/10\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.2537 - accuracy: 0.9025 - val_loss: 0.3975 - val_accuracy: 0.8250\n",
            "Epoch 8/10\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.2679 - accuracy: 0.8914 - val_loss: 0.4059 - val_accuracy: 0.8283\n",
            "Epoch 9/10\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.2132 - accuracy: 0.9187 - val_loss: 0.4550 - val_accuracy: 0.8133\n",
            "Epoch 10/10\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.2231 - accuracy: 0.9125 - val_loss: 0.4990 - val_accuracy: 0.8100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNM2-FlePMJd"
      },
      "source": [
        "We found that the performance of this classifier was about 81% at the hyperparameter values specified.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSCmWQnWINrK"
      },
      "source": [
        "## Test Trained Model on Book Reviews from MDSD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sShfW0aeIN1u"
      },
      "source": [
        "A rich repository of reviews is available. Indeed, one of the most\n",
        "prominently and heavily explored datasets in the space of NLP domain adaptation happens to be a collection of reviews for different product categories on Amazon - The [Multi-Domain Sentiment Dataset](https://www.cs.jhu.edu/~mdredze/datasets/sentiment/). Many categories are represented within this dataset - a total of 25. Within this dataset, we picked the product category of book reviews, feeling that this was sufficiently different from IMDB reviews to present a challenging test-case.\n",
        "\n",
        "The data in this dataset is contained in a markup language format, where tags are used to define various elements, and is organized by category and polarity into separate files. It suffices to note for our purposes that reviews are contained within appropriately named\n",
        "`<review_text>...</review_text>` tags. \n",
        "\n",
        "Given this information, the code can be used to load positive and negative book reviews and prepare them for analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rMxdtS-NRHt"
      },
      "source": [
        "def parse_MDSD(data):\n",
        "  out_lst = []\n",
        "  for i in range(len(data)):\n",
        "      txt = \"\"\n",
        "      # Locate the first line of review, combine all subsequent characters until end tag into review text\n",
        "      if (data[i]==\"<review_text>\\n\"):\n",
        "          j=i\n",
        "          while(data[j]!=\"</review_text>\\n\"):\n",
        "              txt = txt+data[j]\n",
        "              j = j+1\n",
        "          text = tokenize(txt)\n",
        "          text = stop_word_removal(text)\n",
        "          text = reg_expressions(text)\n",
        "          out_lst.append(text)\n",
        "          \n",
        "          #print(txt)\n",
        "          #print(text)\n",
        "          \n",
        "  return out_lst"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCj0QDCWNkJU",
        "outputId": "235d0ff6-4ae4-4922-e94f-7b848b663bad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Read lines from source text file both positive and negative reviews by leveraging defined function\n",
        "with open(\"books.negative.review\", \"r\", encoding=\"latin1\") as my_file:\n",
        "  data = my_file.readlines()\n",
        "neg_books = parse_MDSD(data)\n",
        "len(neg_books)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_u8s4oIOeFM",
        "outputId": "d9488180-146e-4da8-fd5f-3a2e9c087c53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "with open(\"books.positive.review\", \"r\", encoding=\"latin1\") as my_file:\n",
        "  data = my_file.readlines()\n",
        "pos_books = parse_MDSD(data)\n",
        "len(pos_books)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGrjZOWhOs7I",
        "outputId": "fa616316-cd19-442c-bb82-7203ae7047f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create labels for the positive and negative classes\n",
        "header = [0]*len(neg_books)\n",
        "header.extend([1]*len(pos_books))\n",
        "\n",
        "# append, shuffle and extract corresponding sent2vec vectors\n",
        "neg_books.extend(pos_books)\n",
        "MDSD_data = np.array(neg_books)\n",
        "\n",
        "data, sentiments = unison_shuffle_data(np.array(MDSD_data), header)\n",
        "\n",
        "len(sentiments)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yURsdjb0PNbE"
      },
      "source": [
        "Having loaded the book review text and prepared it for further processing, we now test the trained IMDB classifier from the previous section directly on the target data to see how accurate it is without any processing.\n",
        "\n",
        "Let's try using the IMDB classifier directly on book review data..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilZyt7xZPPvi",
        "outputId": "0f453cee-be1e-4ec4-a143-c644dfcd1b54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "embedding_vectors = assemble_embedding_vectors(data)\n",
        "print(embedding_vectors)\n",
        "sentiments = np.asarray(sentiments)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.10247996  0.05756732  0.24177921 ... -0.03724968 -0.2285876\n",
            "   0.09678674]\n",
            " [-0.00950656 -0.12373678 -0.08567187 ...  0.17987318 -0.16593893\n",
            "   0.15151091]\n",
            " [-0.12742344 -0.19568436 -0.1897929  ... -0.10396274 -0.13525589\n",
            "   0.22625637]\n",
            " ...\n",
            " [-0.03482994 -0.13481432  0.01015367 ...  0.03452443 -0.30464894\n",
            "   0.24166287]\n",
            " [-0.15266658 -0.10943983 -0.16178188 ... -0.00949576 -0.12323287\n",
            "   0.07225277]\n",
            " [-0.15517686 -0.4002986   0.24433103 ... -0.02800336 -0.19798745\n",
            "   0.18141942]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMLyN5Y7Q7uf",
        "outputId": "8d147661-6c46-4a4e-d5a5-092136238031",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# evaluate IMDB classifier on books directly\n",
        "print(model.evaluate(x=embedding_vectors, y=sentiments))\n",
        "print(model.metrics_names)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 0s 2ms/step - loss: 0.6135 - accuracy: 0.7510\n",
            "[0.6135140657424927, 0.7509999871253967]\n",
            "['loss', 'accuracy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLaR7EhprYDN"
      },
      "source": [
        "## Adaptation of Book Review Domain via Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVbev8il84H_"
      },
      "source": [
        "We train an autoencoder to reconstruct the IMDB data. The autoencoder takes the form of a shallow neural network that is similar to the multi-task layers.\n",
        "\n",
        "A major difference from the previous neural networks is that since this is a regression problem, there is no activation in the output layer. The encoding dimension `encoding_dim` was empirically tuned for the right balance of accuracy and computing cost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcaPrNio85Bd"
      },
      "source": [
        "encoding_dim = 30\n",
        "\n",
        "# Input size must be the same as the dimension of the sent2vec vectors\n",
        "input_shape = (len(train_x[0]), )\n",
        "\n",
        "sent2vec_vectors = Input(shape=input_shape)\n",
        "\n",
        "# Encode into a space of specified latent dimension encoding_dim\n",
        "encoder = Dense(encoding_dim, activation=\"relu\")(sent2vec_vectors)\n",
        "drop_out = Dropout(0.3)(encoder)\n",
        "\n",
        "# Decode from space of specified latent dimension back into the sent2vec space\n",
        "decoder = Dense(encoding_dim, activation=\"relu\")(drop_out)\n",
        "drop_out = Dropout(0.3)(decoder)\n",
        "\n",
        "output = Dense(len(train_x[0]))(drop_out)\n",
        "\n",
        "autoencoder = Model(inputs=sent2vec_vectors, outputs=output)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrPkQyo6S6Ru"
      },
      "source": [
        "We train the autoencoder for 50 epochs, which takes only a few seconds, by setting both the input and output to the IMDB sent2vec vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWcyRJ1c9Swi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d3a5581-0042-471d-8814-5ae03a13d7c2"
      },
      "source": [
        "# Specify two loss functions (both binary_crossentropy in our case)\n",
        "autoencoder.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mse\", \"mae\"])\n",
        "# Specify training and validation data for each input\n",
        "autoencoder.fit(train_x, train_x, validation_data=(test_x, test_x), batch_size=32, epochs=50, shuffle=True)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "44/44 [==============================] - 1s 7ms/step - loss: 0.0084 - mse: 0.0084 - mae: 0.0718 - val_loss: 0.0080 - val_mse: 0.0080 - val_mae: 0.0702\n",
            "Epoch 2/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0084 - mse: 0.0084 - mae: 0.0718 - val_loss: 0.0080 - val_mse: 0.0080 - val_mae: 0.0701\n",
            "Epoch 3/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0081 - mse: 0.0081 - mae: 0.0708 - val_loss: 0.0080 - val_mse: 0.0080 - val_mae: 0.0700\n",
            "Epoch 4/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0082 - mse: 0.0082 - mae: 0.0709 - val_loss: 0.0080 - val_mse: 0.0080 - val_mae: 0.0700\n",
            "Epoch 5/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0713 - val_loss: 0.0080 - val_mse: 0.0080 - val_mae: 0.0699\n",
            "Epoch 6/50\n",
            "44/44 [==============================] - 0s 5ms/step - loss: 0.0081 - mse: 0.0081 - mae: 0.0708 - val_loss: 0.0079 - val_mse: 0.0079 - val_mae: 0.0698\n",
            "Epoch 7/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0082 - mse: 0.0082 - mae: 0.0711 - val_loss: 0.0079 - val_mse: 0.0079 - val_mae: 0.0697\n",
            "Epoch 8/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0082 - mse: 0.0082 - mae: 0.0711 - val_loss: 0.0079 - val_mse: 0.0079 - val_mae: 0.0696\n",
            "Epoch 9/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0081 - mse: 0.0081 - mae: 0.0707 - val_loss: 0.0079 - val_mse: 0.0079 - val_mae: 0.0695\n",
            "Epoch 10/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0081 - mse: 0.0081 - mae: 0.0707 - val_loss: 0.0079 - val_mse: 0.0079 - val_mae: 0.0698\n",
            "Epoch 11/50\n",
            "44/44 [==============================] - 0s 3ms/step - loss: 0.0080 - mse: 0.0080 - mae: 0.0703 - val_loss: 0.0079 - val_mse: 0.0079 - val_mae: 0.0695\n",
            "Epoch 12/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0080 - mse: 0.0080 - mae: 0.0704 - val_loss: 0.0079 - val_mse: 0.0079 - val_mae: 0.0696\n",
            "Epoch 13/50\n",
            "44/44 [==============================] - 0s 3ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0699 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0694\n",
            "Epoch 14/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0080 - mse: 0.0080 - mae: 0.0699 - val_loss: 0.0079 - val_mse: 0.0079 - val_mae: 0.0694\n",
            "Epoch 15/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0699 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0693\n",
            "Epoch 16/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0699 - val_loss: 0.0079 - val_mse: 0.0079 - val_mae: 0.0695\n",
            "Epoch 17/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0699 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0694\n",
            "Epoch 18/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0699 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0693\n",
            "Epoch 19/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0698 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0693\n",
            "Epoch 20/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0081 - mse: 0.0081 - mae: 0.0705 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0693\n",
            "Epoch 21/50\n",
            "44/44 [==============================] - 0s 3ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0698 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0692\n",
            "Epoch 22/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0080 - mse: 0.0080 - mae: 0.0701 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0691\n",
            "Epoch 23/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0081 - mse: 0.0081 - mae: 0.0703 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0691\n",
            "Epoch 24/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0081 - mse: 0.0081 - mae: 0.0703 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0691\n",
            "Epoch 25/50\n",
            "44/44 [==============================] - 0s 3ms/step - loss: 0.0080 - mse: 0.0080 - mae: 0.0702 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0690\n",
            "Epoch 26/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0081 - mse: 0.0081 - mae: 0.0704 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0690\n",
            "Epoch 27/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0695 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0690\n",
            "Epoch 28/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0078 - mse: 0.0078 - mae: 0.0694 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0691\n",
            "Epoch 29/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0700 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0690\n",
            "Epoch 30/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0697 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0690\n",
            "Epoch 31/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0080 - mse: 0.0080 - mae: 0.0702 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0691\n",
            "Epoch 32/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0697 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0690\n",
            "Epoch 33/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0078 - mse: 0.0078 - mae: 0.0694 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0691\n",
            "Epoch 34/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0080 - mse: 0.0080 - mae: 0.0702 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0691\n",
            "Epoch 35/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0080 - mse: 0.0080 - mae: 0.0700 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0690\n",
            "Epoch 36/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0697 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0689\n",
            "Epoch 37/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0697 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0690\n",
            "Epoch 38/50\n",
            "44/44 [==============================] - 0s 3ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0696 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0689\n",
            "Epoch 39/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0698 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0690\n",
            "Epoch 40/50\n",
            "44/44 [==============================] - 0s 3ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0698 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0691\n",
            "Epoch 41/50\n",
            "44/44 [==============================] - 0s 3ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0698 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0690\n",
            "Epoch 42/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0080 - mse: 0.0080 - mae: 0.0699 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0688\n",
            "Epoch 43/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0695 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0689\n",
            "Epoch 44/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0078 - mse: 0.0078 - mae: 0.0695 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0689\n",
            "Epoch 45/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0080 - mse: 0.0080 - mae: 0.0702 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0689\n",
            "Epoch 46/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0696 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0689\n",
            "Epoch 47/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0699 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0689\n",
            "Epoch 48/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0078 - mse: 0.0078 - mae: 0.0694 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0689\n",
            "Epoch 49/50\n",
            "44/44 [==============================] - 0s 3ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0698 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0690\n",
            "Epoch 50/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0699 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0689\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3fea2af790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYzTK_d-UHX4"
      },
      "source": [
        "We use mean squared error (mse) as loss function for this regression problem, and mean absolute error (mae) as an additional metric. A minimum validation mae value of approximately 0.06 was achieved.\n",
        "\n",
        "We next project the book reviews into the IMDB feature space using the autoencoder trained to reconstruct such features above. This just means we preprocess the book review feature vectors using the autoencoder. We then repeat the accuracy evaluation experiment of the IMDB classifier on these preprocessed vectors as input:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0asdr7lzUQ_P",
        "outputId": "da63de92-8e73-4e1e-e8a6-d532ca94c5e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# transform EmbeddingVectors and sentiments with autoencoder.predict and then evaluate IDMB model again\n",
        "embedding_vectors_projected = autoencoder.predict(embedding_vectors)\n",
        "print(embedding_vectors_projected.shape)\n",
        "\n",
        "print(model.evaluate(x=embedding_vectors_projected,y=sentiments))\n",
        "print(model.metrics_names)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000, 600)\n",
            "63/63 [==============================] - 0s 2ms/step - loss: 0.7478 - accuracy: 0.5945\n",
            "[0.747765302658081, 0.5945000052452087]\n",
            "['loss', 'accuracy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8YWlnanFIh2"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHOwQobpD0Wq"
      },
      "source": [
        "An accuracy of approximately 74% was now observed, demonstrating an improvement about 1% and an instance of zero-shot domain adaptation. Repeating this several times, we found the improvement to consistently stay around 0.5-1%, giving us confidence that the autoencoding domain adaptation did indeed lead to some positive transfer."
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-completion-with-gpt2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMNLyoo9fBdqnpmw2Ap8nWW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transfer-learning-for-natural-language-processing/blob/main/6-text-generation-with-gpt-2-and-gpt-3-models/text_completion_with_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TihxpsDJR1fq"
      },
      "source": [
        "## Text completion with GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nseBpmVGSBw7"
      },
      "source": [
        "This notebook will clone the OpenAI GPT-2 repository, download the 345M parameter GPT-2 transformer model, and interact with it. We will enter context sentences and analyze the text generated by the transformer. The goal is to see how it creates new content.\n",
        "\n",
        "We must activate the GPU to train our GPT-2 345M parameter transformer model.\n",
        "OpenAI has begun to offer transformers as a cloud service. However, we saw that we might be able to run small models with standard machines without going through a cloud service to run a powerful transformer GPT-3.\n",
        "\n",
        "We will use the GPT-2 model but will not train it. We could not train large GPT models even if we had access to the source code because most of us lack the computing power to do it. Because we would need petaflops with\n",
        "the more recent transformer models!\n",
        "\n",
        "An average developer does not have access to this level of machine power. Google Cloud, Microsoft Azure, Amazon Web Services (AWS), for example, can rent a certain level of machine resources in teraflops to cloud customers.\n",
        "\n",
        "If we go a step further, it becomes tougher to train transformers in teraflops. Accessing petaflops calculation power is limited to a restricted number of teams in the world.\n",
        "\n",
        "However, we will see that the results produced by the limited power of Google\n",
        "Colaboratory VMs for our 345M parameter GPT-2 are quite convincing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5JYf_1VUypz"
      },
      "source": [
        "## Step 1: Cloning the OpenAI GPT-2 repository"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1KSjKpDUzxY"
      },
      "source": [
        "OpenAI is still letting us download GPT-2. This may be discontinued in the future, or maybe we will have access to more resources. At this point, the evolution of transformers and their usage moves so fast nobody can foresee how the market will evolve, even the major research labs themselves.\n",
        "\n",
        "We will clone OpenAI's GitHub directory on our VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHakKl1rVP2F",
        "outputId": "c29f7fb1-f451-4aa5-cd79-a9d788739450",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/openai/gpt-2.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 233, done.\u001b[K\n",
            "remote: Total 233 (delta 0), reused 0 (delta 0), pack-reused 233\u001b[K\n",
            "Receiving objects: 100% (233/233), 4.38 MiB | 6.59 MiB/s, done.\n",
            "Resolving deltas: 100% (124/124), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbG7vOrTVzA2"
      },
      "source": [
        "You can see that we do not have the Python training files we need. We will install them when we train the GPT-2 model in the Training a GPT-2 language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFhi6Xn-V4cn"
      },
      "source": [
        "## Step 2: Installing the requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayHpIY5YV6N8"
      },
      "source": [
        "The requirements will be installed automatically:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdE7wTOtXf5c"
      },
      "source": [
        "import os # when the VM restarts import os necessary\n",
        "\n",
        "os.chdir(\"/content/gpt-2\")\n",
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6Mp3IE3YAei"
      },
      "source": [
        "The requirements for this notebook are:\n",
        "\n",
        "- `Fire 0.1.3` to generate command-line interfaces (CLIs)\n",
        "- `regex 2017.4.5` for regex usage\n",
        "- `Requests 2.21.0` an HTTP library\n",
        "- `tqdm 4.31.1` to display a progress meter for loops\n",
        "\n",
        "You may be asked to restart the notebook.\n",
        "\n",
        "Do not restart it now. Let's wait until we check the version of TensorFlow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E78TYMuaYW24"
      },
      "source": [
        "## Step 3: Checking the version of TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "152DPeHKYYry"
      },
      "source": [
        "The GPT-2 transformer 345M transformer model provided by OpenAI uses\n",
        "TensorFlow 1.x. In 2020, GPT models have reached 175 billion\n",
        "parameters, making it impossible for us to train them without having access to a supercomputer.\n",
        "\n",
        "The corporate giants' research labs, such as Facebook AI and OpenAI and Google\n",
        "Research/Brain, are speeding towards super-transformers and are leaving us with what they can for us to learn and understand. They do not have time to go back and update all of the models they share.\n",
        "\n",
        "This is one of the reasons for which Google Colaboratory VMs have preinstalled\n",
        "versions of both TensorFlow 1.x and TensorFlow 2.x.\n",
        "\n",
        "We will be using TensorFlow 1.x in this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaUzYNI0ZOgv",
        "outputId": "a895598f-850a-4c1a-ad97-c42b816f6496",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Colab has tf 1.x and tf 2.x installed\n",
        "#Restart runtime using 'Runtime' -> 'Restart runtime...'\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsu361O0ZiZE"
      },
      "source": [
        "Whether version tf 1.x is displayed or not, rerun the cell to make sure, then restart the VM. Rerun this cell to make sure before continuing.\n",
        "\n",
        "If you encounter a TensforFlow error during the process (ignore the warnings), rerun this cell, restart the VM, and rerun to make sure.\n",
        "\n",
        "Do this every time you restart the VM. The default version of the VM is tf.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNIk5fi1ZqVh"
      },
      "source": [
        "## Step 4: Downloading the 345M parameter GPT-2 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uAosetxZtBj"
      },
      "source": [
        "We will now download a trained 345M parameter GPT-2 model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M2g07NiaJmw",
        "outputId": "1d0607ae-bf81-401b-ff08-aee41729721e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "# Downloading the 345M parameter GPT-2 Model\n",
        "os.chdir(\"/content/gpt-2\")\n",
        "!python3 download_model.py \"345M\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.00kit [00:00, 726kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 1.73Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 700kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [10:59, 2.15Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 7.38Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 1.50Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 926kit/s]                                                        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C8wDi5sv9vn"
      },
      "source": [
        "It contains the information we need to run the model.\n",
        "\n",
        "The `hparams.json` file contains the definition of the GPT-2 model:\n",
        "\n",
        "- `\"n_vocab\": 50257`, the size of the vocabulary of the model\n",
        "- `\"n_ctx\": 1024`, the context size\n",
        "- `\"n_embd\": 1024`, the embedding size\n",
        "- `\"n_head\": 16`, the number of heads\n",
        "- `\"n_layer\": 24`, the number of layers\n",
        "\n",
        "`encoder.json` and `vacab.bpe` contain the tokenized vocabulary and the BPE word pairs.\n",
        "\n",
        "The checkpoint file contains the trained parameters at a checkpoint.\n",
        "\n",
        "The checkpoint file is saved with three other important files:\n",
        "\n",
        "- `model.ckpt.meta` describes the graph structure of the model. It contains\n",
        "`GraphDef, SaverDef`, and so on. We can retrieve the information with\n",
        "`tf.train.import_meta_graph([path]+'model.ckpt.meta')`.\n",
        "- `model.ckpt.index` is a string table. The keys contain the name of a tensor, and the value is `BundleEntryProto`, which contains the metadata of a tensor.\n",
        "- `model.ckpt.data` contains the values of all of the variables in a `TensorBundle` collection.\n",
        "\n",
        "We have downloaded our model. We will now go through some intermediate steps\n",
        "before activating the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmLCY8Gkw-2h"
      },
      "source": [
        "## Steps 5: Intermediate instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q700MwulySAO"
      },
      "source": [
        "We want to print UTF encoded text to the console when we are interacting with the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzd1XZJwyYVn"
      },
      "source": [
        "# Printing UTF encoded text to the console\n",
        "!export PYTHONIOENCODING=UTF-8"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpUe40f5ylMg"
      },
      "source": [
        "We want to make sure we are in the src directory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT7tS4r-ylni"
      },
      "source": [
        "os.chdir(\"/content/gpt-2/src\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bZ0yzGZywdH"
      },
      "source": [
        "We are ready to interact with the GPT-2 model. However, in this section, we will go through the main aspects of the code.\n",
        "\n",
        "`interactive_conditional_samples.py` first imports the necessary modules required to interact with the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yRv4DDMzZwM"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCeWFWVczaXb"
      },
      "source": [
        "We have gone through the intermediate steps leading to the activation of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P036P0RNzeTR"
      },
      "source": [
        "## Steps 6: Importing and defining the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnkmYUrnzgU1"
      },
      "source": [
        "We will now activate the interaction with the model with `interactive_conditional_samples.py`.\n",
        "\n",
        "We need to import three modules that are also in `/content/gpt-2/src`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y_5Uektzs_m"
      },
      "source": [
        "import model, sample, encoder"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RsuOgC7zzEL"
      },
      "source": [
        "- `model.py` defines the model's structure: the hyperparameters, the multiattention `tf.matmul` operations, the activation functions, and all of the other properties.\n",
        "- `sample.py` processes the interaction and controls the sample that will be\n",
        "generated. It makes sure that the tokens are more meaningful.\n",
        "\n",
        "Softmax values can sometimes be blurry, like seeing an image with low\n",
        "definition. `sample.py` contains a variable named temperature that will make\n",
        "the values sharper, increasing the higher probabilities and softening the\n",
        "lower ones.\n",
        "\n",
        "`sample.py` can activate Top-k sampling. Top-k sampling sorts the probability\n",
        "distribution of a predicted sequence. The higher probability values of the\n",
        "head of the distribution are filtered up to the k-th token. The tail containing the lower probabilities is excluded, preventing the model from predicting low-quality tokens.\n",
        "\n",
        "`sample.py` can also activate Top-p sampling for language modeling. Top-p\n",
        "sampling does not sort the probability distribution. It selects the words with\n",
        "high probabilities until the sum of this subset's probabilities or the nucleus of a possible sequence exceeds `p`.\n",
        "- `encoder.py` encodes the sample sequence with the defined model, `encoder.\n",
        "json`, and `vocab.bpe`. It contains both a BPE encoder and a text decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdyvOed_1SQM"
      },
      "source": [
        "## Step 7: Interacting with GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeRQmHaO1Ukn"
      },
      "source": [
        "To interact with the model, run the interact_model cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osSNLOVg1cLD",
        "outputId": "49e2ee1f-4b4d-490e-c10c-6a1dc1888b91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "source": [
        "import interactive_conditional_samples\n",
        "\n",
        "interact_model(\"345M\", None, 1, 1, 300, 1, 0, \"/content/gpt-2/models\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7f5d86c9b0cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minteractive_conditional_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minteract_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"345M\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/gpt-2/models\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'interact_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkNgsReN3yQ1"
      },
      "source": [
        "You will be prompted to enter some context.\n",
        "\n",
        "You can try any type of context you wish since this is a standard GPT-2 model.\n",
        "\n",
        "We can try a sentence written by Emmanuel Kant:\n",
        "\n",
        "```\n",
        "Human reason, in one sphere of its cognition, is called upon to\n",
        "consider questions, which it cannot decline, as they are presented by\n",
        "its own nature, but which it cannot answer, as they transcend every\n",
        "faculty of the mind.\n",
        "```\n",
        "\n",
        "You can also type Ctrl + M to stop generating text, but it may transform the code into text, and you will have to copy it back into a program cell.\n",
        "\n",
        "The output is very rich, and we can observe several facts:\n",
        "- The context we entered conditioned the output generated by the model.\n",
        "- The context was a demonstration for the model. It learned what to say from\n",
        "the model without modifying its parameters.\n",
        "- Text completion is conditioned by context. This opens the door to\n",
        "transformer models that do not require fine-tuning.\n",
        "- From a semantic perspective, the output could be more interesting.\n",
        "- From a grammatical perspective, the output is convincing.\n",
        "\n",
        "\n"
      ]
    }
  ]
}
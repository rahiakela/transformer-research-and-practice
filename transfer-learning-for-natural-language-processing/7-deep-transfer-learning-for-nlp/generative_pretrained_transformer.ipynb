{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generative-pretrained-transformer.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOML99GOm6G1OPk3HRH1neJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transfer-learning-for-natural-language-processing/blob/main/7-deep-transfer-learning-for-nlp/generative_pretrained_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TrnWEH6DNtl"
      },
      "source": [
        "##Generative Pretrained Transformer(GPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJLE4ThGDOXb"
      },
      "source": [
        "The Generative Pretrained Transformer(GPT) was developed by OpenAI and was\n",
        "among the earliest models to apply the transformer architecture to the semisupervised learning scenario.\n",
        "\n",
        "GPT became the state of the art for most of the aforementioned tasks, it has generally come to be preferred as a text-generation model. Unlike BERT and its derivatives, which have come to dominate most other tasks, GPT was trained with the causal modeling objective (CLM) where the next token is predicted,\n",
        "as opposed to BERT’s masked language modeling (MLM) fill-in-the-blanks-type\n",
        "prediction objective.\n",
        "\n",
        "BERT is essentially a stacked set of encoders of the original encoder-decoder transformer architecture. GPT is essentially the converse of that, in the sense that it stacks the decoders instead.\n",
        "\n",
        "<img src='https://github.com/rahiakela/transfer-learning-for-natural-language-processing/blob/main/7-deep-transfer-learning-for-nlp/images/1.png?raw=1' width='800'/>\n",
        "\n",
        "Besides the encoder-decoder attention, the other distinguishing feature of the transformer decoder is that its self-attention layer is “masked,” that is, future tokens are “masked” when computing attention for any given token.\n",
        "\n",
        "In the attention calculation, this just means including only the tokens in “he didnt want to talk about cells” in the calculation and ignoring the rest.In GPT, we lightly modified so you can clearly see the future tokens being masked.\n",
        "\n",
        "\n",
        "<img src='https://github.com/rahiakela/transfer-learning-for-natural-language-processing/blob/main/7-deep-transfer-learning-for-nlp/images/2.png?raw=1' width='800'/>\n",
        "\n",
        "This introduces a sense of causality into the system and suitability for text generation, or predicting the next token. Because there is no encoder, the encoder-decoder attention is also dropped.\n",
        "\n",
        "<img src='https://github.com/rahiakela/transfer-learning-for-natural-language-processing/blob/main/7-deep-transfer-learning-for-nlp/images/3.png?raw=1' width='800'/>\n",
        "\n",
        "Note that the same output can be used for both text prediction/\n",
        "generation and classification for some other task. Indeed, the authors devised an input transformation scheme where multiple tasks could be handled by the same architecture without any architectural changes.\n",
        "\n",
        "Having briefly introduced the architecture of GPT, let’s use a pretrained version of\n",
        "it for some fun coding experiments. We first use it to generate some open-ended\n",
        "text given a prompt. We then also use a modification of GPT built at [Microsoft—\n",
        "DialoGPT](https://arxiv.org/abs/1911.00536)—to perform multiturn conversations with a chatbot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEJAN_djId3Y"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heP6-aAqIl-h"
      },
      "source": [
        "!pip -qq install git+https://github.com/huggingface/transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO8KhJQtI7iA"
      },
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer # you can use these utility classes that automatically load the right classes\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer # or these more specific classes directly\n",
        "\n",
        "import torch"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekFi6WN2JTVM"
      },
      "source": [
        "##Transformers pipelines for text generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg8_lw3MJVra"
      },
      "source": [
        "The first thing we will do in this subsection is generate some open-ended text using GPT. We will also use this opportunity to introduce pipelines—an API exposing the pretrained models in the transformers library for inference.\n",
        "\n",
        "Let’s start by initializing the transformers pipeline to the GPT-2 model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWvnHjfRJLEu"
      },
      "source": [
        "gpt = pipeline(\"text-generation\", model=\"gpt2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQA8RUg7J-tH"
      },
      "source": [
        "By way of reminder, GPT in its original form is well suited for open-ended text generation, such as creative writing of sections of text to complement previous text. \n",
        "\n",
        "Let us see what the model generates when primed by \"Somewhere over the rainbow…\" up to a maximum of 100 tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QXtjihHJvkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3da6a6de-2491-4a4f-f737-99a16df3fdb1"
      },
      "source": [
        "gpt(\"Somewhere over the rainbow\", max_length=100)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using pad_token, but it is not set yet.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"Somewhere over the rainbow, the city of Rangoon was filled with people who, with the help of an enormous wind powered cart filled with people, managed to reach the city, with several more people arriving through this route. But here they could see nothing. The city was like a sea of people, but a different and more peaceful one. There was no room to hide. In the end, it was just a mountain over there. Rangoon City's situation had nothing to do\"}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpXDNkVdKdbB"
      },
      "source": [
        "This seems very semantically correct, even if the message is a bit incoherent. You could imagine a creative writer using this to generate ideas to get around writer’s block!\n",
        "\n",
        "Now, let’s see if we can prime the model with something less \"creative\" something more technical, to see how it will do. \n",
        "\n",
        "Let’s prime the model with the text \"Transfer learning is a field of study\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCUe_qVfKrdM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d21eae08-797e-4f5c-adb7-af2123739433"
      },
      "source": [
        "gpt(\"Transfer learning is a field of study\", max_length=100)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using pad_token, but it is not set yet.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"Transfer learning is a field of study for all students and may only be completed for a limited number of years in the community with a minimal number of extra hours or time (see the section on How to Use Learning at Berkeley with School Year 2006).\\n\\nThis is a subject that will be discussed separately in the next week's issue of Class of '06.\\n\\nTo be included in a final report on Class of '06 this year, the following criteria must be met:\\n\\nYou have\"}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g1gQCoOLtpR"
      },
      "source": [
        "Again, we can see this text is pretty good in terms of semantic coherence, grammatic structure, spelling, punctuation, and so on—indeed, eerily good. However, as it continues, it becomes arguably factually incorrect.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN_krCuvL4iq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca3d3d0b-b7f2-48a4-ccf2-f99871084d50"
      },
      "source": [
        "gpt(\"He didn’t want to talk about cells on the cell phone because he considered it\", max_length=100)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using pad_token, but it is not set yet.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'He didn’t want to talk about cells on the cell phone because he considered it a good idea. If all it would offer were a solution, he said, \"then I\\'ll have to put it to work.\"\\n\\nHe didn\\'t want to go further, however. He didn\\'t want to talk about how he would pay for the service — just the two of them.\\n\\nTalks had started to fall apart.\\n\\nAdvertisement\\n\\nThen, this week, in February'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atS-607AMNSt"
      },
      "source": [
        "##Application to chatbots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3zSDOyaMPZS"
      },
      "source": [
        "It seems intuitively that one should be able to adopt GPT without major modification to\n",
        "this application. Luckily for us, the folks at Microsoft already did this via the model DialoGPT,\n",
        "which was also recently included in the transformers library. Its architecture is\n",
        "the same as GPT’s, with the addition of special tokens to indicate the end of a participant’s\n",
        "turn in a conversation. After seeing such a token, we can add the new contribution\n",
        "of the participant to the priming context text and iteratively repeat the process via\n",
        "direct application of GPT to generate a response from our chatbot. Naturally, the pretrained\n",
        "GPT model was fine-tuned on conversational text to make sure the response\n",
        "would be appropriate.\n",
        "\n",
        "Let’s go ahead and build a chatbot! We will not use pipelines in this case, because\n",
        "this model isn’t yet exposed through that API at the time of writing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwEWmL-aIJYB"
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"microsoft/DialoGPT-medium\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhYykOd3IvO2"
      },
      "source": [
        "A few things are worth highlighting at this stage. First, note that we are using the GPT-2 model classes.\n",
        "\n",
        "Additionally, note that we could have used\n",
        "the classes AutoModelWithLMHead and AutoTokenizer interchangeably with these\n",
        "GPT-specific model classes.\n",
        "\n",
        "Finally, note that the “LMHead” version of the GPT is used here. This means that\n",
        "the output from the vanilla GPT is passed through one linear layer and one normalization\n",
        "layer, followed by a transformation into a vector of probabilities of a dimension\n",
        "equal to the size of the vocabulary. The maximum value corresponds to the next most\n",
        "likely token if the model is correctly trained.\n",
        "\n",
        "We first specify a maximum number of responses of five. We then encode\n",
        "the conversational contribution of the user at each turn, append the contribution to\n",
        "the chat history, and feed that to the loaded pretrained DialoGPT model for generating\n",
        "the next response."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aokegq-UJDfl",
        "outputId": "db5350bf-f731-4cfe-f499-46f6e81f5687"
      },
      "source": [
        "conversation_length = 5  # Chats for five line\n",
        "for step in range(conversation_length):\n",
        "  # encodes new user input, adds an endof-sentence token, and returns Tensor\n",
        "  new_user_inputs_ids = tokenizer.encode(input(\"User: \") + tokenizer.eos_token, return_tensors=\"pt\")\n",
        "  # adds new input to the chat history\n",
        "  bot_input_ids = torch.cat([chat_history_ids, new_user_inputs_ids], dim=1) if step > 0 else new_user_inputs_ids\n",
        "  # generate a response of up to max_length tokens\n",
        "  chat_history_ids = model.generate(bot_input_ids, \n",
        "                                    max_length=1000, \n",
        "                                    pad_token_id=tokenizer.eos_token_id)\n",
        "  # display response\n",
        "  print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]: ][0], skip_special_tokens=True)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User: Hi there\n",
            "DialoGPT: Hi there\n",
            "User: How are you today?\n",
            "DialoGPT: I'm good, how are you?\n",
            "User: Good! How much money do you have?\n",
            "DialoGPT: I have about 100k\n",
            "User: What will you spend it on?\n",
            "DialoGPT: I'm not sure, I'm not sure what I want to spend it on.\n",
            "User: Make a decision, life is short.\n",
            "DialoGPT: I'm going to go with a lot of things\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0p76ZhbNgnA"
      },
      "source": [
        "One could easily play with this bot all day! We had a lot of fun asking it various questions and prompting it in various ways."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2q-iibhNk3Z",
        "outputId": "9799c568-a948-46da-fdf2-3270ed055dc1"
      },
      "source": [
        "conversation_length = 5  # Chats for five line\n",
        "for step in range(conversation_length):\n",
        "  # encodes new user input, adds an endof-sentence token, and returns Tensor\n",
        "  new_user_inputs_ids = tokenizer.encode(input(\"User: \") + tokenizer.eos_token, return_tensors=\"pt\")\n",
        "  # adds new input to the chat history\n",
        "  bot_input_ids = torch.cat([chat_history_ids, new_user_inputs_ids], dim=1) if step > 0 else new_user_inputs_ids\n",
        "  # generate a response of up to max_length tokens\n",
        "  chat_history_ids = model.generate(bot_input_ids, \n",
        "                                    max_length=1000, \n",
        "                                    pad_token_id=tokenizer.eos_token_id)\n",
        "  # display response\n",
        "  print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]: ][0], skip_special_tokens=True)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User: Hi robot\n",
            "DialoGPT: Hello human\n",
            "User: Hum?\n",
            "DialoGPT: Hello human\n",
            "User: Huh?\n",
            "DialoGPT: Hello human\n",
            "User: OK, what is your name?\n",
            "DialoGPT: I'm not human\n",
            "User: All right then.\n",
            "DialoGPT: I'm not human\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvIr_532N8XQ"
      },
      "source": [
        "It’s quite plausible that the entity at the other end of this short conversation is a human, isn’t it? Does that mean it passes the Turing test?\n",
        "\n",
        "As you increase the number of allowable conversational turns, you will find the bot\n",
        "getting stuck in repeated responses that are off-topic. This is analogous to the GPT\n",
        "open-ended text generation becoming more nonsensical as the length of generated\n",
        "text increases. One simple way to improve this is to keep a fixed local context size,\n",
        "where the model is prompted with conversation history only within that context.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlW0nXLUPB7M"
      },
      "source": [
        "##Text Generation with EleutherAI GPT Neo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnAb-BkmPSrX"
      },
      "source": [
        "GPT-Neo from EleutherAI is a recent smaller but worthy open source alternative to GPT3.\n",
        "\n",
        "It is already available in the transformers library and can be used directly\n",
        "by setting the model string to one of the model names provided by EleutherAI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSO-9C1rPdBI"
      },
      "source": [
        "gpt = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-1.3B\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWhIMR9EPrR-"
      },
      "source": [
        "gpt(\"Transfer learning is a field of study\", max_length=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TvNEbp_P1e8"
      },
      "source": [
        "gpt(\"Somewhere over the rainbow\", max_length=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzKcMVrdQZFz"
      },
      "source": [
        "Upon inspection, you should find its performance better but\n",
        "naturally at a significantly higher cost (the weights of the largest model are more than 10 GB in size!)."
      ]
    }
  ]
}
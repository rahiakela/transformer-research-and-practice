{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "huggingface-talk--nlp-workflows-with-keras",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-research-and-practice/blob/main/huggingface-transformers/huggingface-course-event/huggingface_talk_nlp_workflows_with_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbHPheAV3x-d"
      },
      "source": [
        "##NLP workflows with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This colab will show a number of ways to build simple NLP models with Keras, as a companion to the Hugging Face Keras talk on NLP workflows in Keras. We will build four very simple NLP models and see how they compare.\n",
        "\n",
        "<img src='https://i.imgur.com/1vD2az8.png?raw=1' width='800'/>\n",
        "\n",
        "**Reference**:\n",
        "\n",
        "https://www.youtube.com/watch?v=gZIP-_2XYMM\n",
        "\n",
        "https://huggingface.co/course/event/1?fw=pt\n",
        "\n"
      ],
      "metadata": {
        "id": "xSeEhmoKm26c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "Dg3zZZ1Fm6V3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To start, let's download the dataset."
      ],
      "metadata": {
        "id": "olWIC1oUnOu_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1e_v4_L4L3G",
        "outputId": "3ca8f54f-131e-4fd9-cfe9-7b46adfb48ff"
      },
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!rm -r aclImdb/train/unsup  # Delete unlabeled data."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  25.7M      0  0:00:03  0:00:03 --:--:-- 25.7M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzOwNhqHvBJr"
      },
      "source": [
        "We will convert our text file inputs into batched datasets of size 32, and make a small utility to efficiently apply and cache our preprocessing with `tf.data`. This allows us to avoid recomputing our preprocessed dataset each epoch.\n",
        "\n",
        "You can learn more about `tf.data` [here](https://www.tensorflow.org/guide/data) and keras preprocessing [here](https://www.tensorflow.org/guide/keras/preprocessing_layers)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1eY0c7HkB0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b6d8ed2-b371-40b1-da71-b70f91aa8c99"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=32)\n",
        "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=32)\n",
        "\n",
        "def apply_preprocessing(ds, preprocessing_model):\n",
        "  ds = ds.map(\n",
        "      lambda x, y: (preprocessing_model(x), y),\n",
        "      num_parallel_calls=tf.data.AUTOTUNE)\n",
        "  # Cache and prefetch the data.\n",
        "  return ds.cache().prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Take 1: Unigram model"
      ],
      "metadata": {
        "id": "PUmLR7zhnRw0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTrLp2gX4Sdw"
      },
      "source": [
        "Our first attempt at a model is a simple unigram model, also called a bag of words model. We will build up a vocabulary of the 10K most popular words in our dataset, and we will only track which words from our vocabulary are present in each review.\n",
        "\n",
        "We will multi-hot encode a simple unigram representation of our inputs using [TextVecotorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) layer, and build a simple logistic regression over the outputs with the [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4APrJ4-x9QSP",
        "outputId": "a63aeaa1-7fd2-45ef-c61b-a8b5f839e62a"
      },
      "source": [
        "inputs = tf.keras.Input(shape=(1,), dtype='string')\n",
        "\n",
        "# Preprocess inputs.\n",
        "features = train_ds.map(lambda x, y: x)\n",
        "text_vectorizer = tf.keras.layers.TextVectorization(\n",
        "    output_mode='multi_hot', max_tokens=10000)\n",
        "text_vectorizer.adapt(features)\n",
        "preprocessed_inputs = text_vectorizer(inputs)\n",
        "\n",
        "# Apply model layers.\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(preprocessed_inputs)\n",
        "\n",
        "# Split the preprocessing into a separate model to apply it with tf.data.\n",
        "unigram_preprocessing = tf.keras.Model(inputs, preprocessed_inputs)\n",
        "unigram_model = tf.keras.Model(preprocessed_inputs, outputs)\n",
        "\n",
        "# Apply preprocessing asynchonously with tf.data.\n",
        "preprocessed_train_ds = apply_preprocessing(train_ds, unigram_preprocessing)\n",
        "preprocessed_test_ds = apply_preprocessing(test_ds, unigram_preprocessing)\n",
        "\n",
        "# Train the model.\n",
        "unigram_model.compile(loss='binary_crossentropy', metrics='accuracy')\n",
        "unigram_model.summary()\n",
        "unigram_model.fit(\n",
        "    preprocessed_train_ds, validation_data=preprocessed_test_ds, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 10000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 10001     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,001\n",
            "Trainable params: 10,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 23s 25ms/step - loss: 0.4384 - accuracy: 0.8434 - val_loss: 0.3462 - val_accuracy: 0.8770\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.2904 - accuracy: 0.8959 - val_loss: 0.2984 - val_accuracy: 0.8875\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.2480 - accuracy: 0.9091 - val_loss: 0.2815 - val_accuracy: 0.8903\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2248 - accuracy: 0.9175 - val_loss: 0.2744 - val_accuracy: 0.8911\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.2093 - accuracy: 0.9230 - val_loss: 0.2716 - val_accuracy: 0.8909\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2170136710>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U70a6-ew67Uc"
      },
      "source": [
        "## Take 2: Bigram model.\n",
        "\n",
        "As a next step, we can try using a little more of the sequence data available to us by also considering bigrams, that is pairs of words. Passing `ngrams=2` to the `TextVectorization` layer will have it build a vocabulary of the most common words as well as pairs of words in the dataset. We can then train a logistic regression exactly as before.\n",
        "\n",
        "Because our new vocabulary space (all words and pairs of words) is much bigger, we will double our learned vocabulary size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gR2H0SW6HZJ",
        "outputId": "a889c1f7-b27c-4fbf-f05a-89c36bbfdc9b"
      },
      "source": [
        "inputs = tf.keras.Input(shape=(1,), dtype='string')\n",
        "\n",
        "# Preprocess inputs.\n",
        "features = train_ds.map(lambda x, y: x)\n",
        "text_vectorizer = tf.keras.layers.TextVectorization(\n",
        "    output_mode='multi_hot', max_tokens=20000, ngrams=2)\n",
        "text_vectorizer.adapt(features)\n",
        "preprocessed_inputs = text_vectorizer(inputs)\n",
        "\n",
        "# Apply model layers.\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(preprocessed_inputs)\n",
        "\n",
        "# Split the preprocessing into a separate model to apply it with tf.data.\n",
        "bigram_preprocessing = tf.keras.Model(inputs, preprocessed_inputs)\n",
        "bigram_model = tf.keras.Model(preprocessed_inputs, outputs)\n",
        "\n",
        "# Apply preprocessing asynchonously with tf.data.\n",
        "preprocessed_train_ds = apply_preprocessing(train_ds, bigram_preprocessing)\n",
        "preprocessed_test_ds = apply_preprocessing(test_ds, bigram_preprocessing)\n",
        "\n",
        "# Train the model.\n",
        "bigram_model.compile(loss='binary_crossentropy', metrics='accuracy')\n",
        "bigram_model.summary()\n",
        "bigram_model.fit(\n",
        "    preprocessed_train_ds, validation_data=preprocessed_test_ds, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 20001     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,001\n",
            "Trainable params: 20,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 21s 26ms/step - loss: 0.3795 - accuracy: 0.8658 - val_loss: 0.2948 - val_accuracy: 0.8908\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.2297 - accuracy: 0.9202 - val_loss: 0.2571 - val_accuracy: 0.9024\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.1859 - accuracy: 0.9366 - val_loss: 0.2449 - val_accuracy: 0.9052\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.1602 - accuracy: 0.9457 - val_loss: 0.2413 - val_accuracy: 0.9056\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.1425 - accuracy: 0.9526 - val_loss: 0.2419 - val_accuracy: 0.9056\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f20eeee5110>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbTcXePs_JxB"
      },
      "source": [
        "## Take 3: Embedding and pooling.\n",
        "\n",
        "Let's now switch up our preprocessed text representation to preserve the sequence of words in each review. By setting `ouput_mode='int'` on the `TextVectorization` layer (the default), the layer will output an integer index for each word of the input, corresponding to the index of the word in our learned vocabulary.\n",
        "\n",
        "We can use this representation to train an [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) for each word. A very simple approach we can try with our embedded words is to simply average them all together, which we can do with the [GlobalAveragePooling1D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D) layer. To avoid overfitting, we will use two [Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByKuZF3m-KPj",
        "outputId": "87786a63-ec79-42ea-8002-d6074064cd30"
      },
      "source": [
        "inputs = tf.keras.Input(shape=(1,), dtype='string')\n",
        "\n",
        "# Preprocess inputs.\n",
        "features = train_ds.map(lambda x, y: x)\n",
        "text_vectorizer = tf.keras.layers.TextVectorization(\n",
        "    output_mode='int', max_tokens=10000, output_sequence_length=250)\n",
        "text_vectorizer.adapt(features)\n",
        "preprocessed_inputs = text_vectorizer(inputs)\n",
        "\n",
        "# Apply model layers.\n",
        "embedding = tf.keras.layers.Embedding(\n",
        "    text_vectorizer.vocabulary_size(), 32, mask_zero=True)\n",
        "x = embedding(preprocessed_inputs)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Split the preprocessing into a separate model to apply it with tf.data.\n",
        "pooling_preprocessing = tf.keras.Model(inputs, preprocessed_inputs)\n",
        "pooling_model = tf.keras.Model(preprocessed_inputs, outputs)\n",
        "\n",
        "# Apply preprocessing asynchonously with tf.data.\n",
        "preprocessed_train_ds = apply_preprocessing(train_ds, pooling_preprocessing)\n",
        "preprocessed_test_ds = apply_preprocessing(test_ds, pooling_preprocessing)\n",
        "\n",
        "# Train the model.\n",
        "pooling_model.compile(loss='binary_crossentropy', metrics='accuracy')\n",
        "pooling_model.summary()\n",
        "pooling_model.fit(\n",
        "    preprocessed_train_ds, validation_data=preprocessed_test_ds, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 250)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 250, 32)           320000    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 250, 32)           0         \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 32)               0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 20s 22ms/step - loss: 0.6173 - accuracy: 0.7625 - val_loss: 0.5252 - val_accuracy: 0.8014\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.4415 - accuracy: 0.8345 - val_loss: 0.3926 - val_accuracy: 0.8411\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.3441 - accuracy: 0.8628 - val_loss: 0.3397 - val_accuracy: 0.8565\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.3011 - accuracy: 0.8771 - val_loss: 0.3173 - val_accuracy: 0.8668\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.2755 - accuracy: 0.8868 - val_loss: 0.3064 - val_accuracy: 0.8720\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2170629890>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wEhHixuE-ls"
      },
      "source": [
        "## Take 4: Bidirectional LSTM.\n",
        "\n",
        "The last thing we will try is learning an [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) layer over our embedded words. This is almost a line for line equivalent to our pooling model above, but instead of the pooling layer, we swap in an LSTM. This was we can try to allow our model to learn from the sequence order in which our review words appeared."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJStclzLEb5Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "913ff03b-ae9c-4bfc-f0f2-750a67ea725c"
      },
      "source": [
        "inputs = tf.keras.Input(shape=(1,), dtype='string')\n",
        "\n",
        "# Preprocess inputs.\n",
        "features = train_ds.map(lambda x, y: x)\n",
        "text_vectorizer = tf.keras.layers.TextVectorization(\n",
        "    output_mode='int', max_tokens=10000, output_sequence_length=250)\n",
        "text_vectorizer.adapt(features)\n",
        "preprocessed_inputs = text_vectorizer(inputs)\n",
        "\n",
        "# Split the preprocessing into a separate model to apply it with tf.data.\n",
        "embedding = tf.keras.layers.Embedding(\n",
        "    text_vectorizer.vocabulary_size(), 32, mask_zero=True)\n",
        "x = embedding(preprocessed_inputs)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = tf.keras.layers.LSTM(32)(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Split the preprocessing into a separate model to apply it with tf.data.\n",
        "lstm_preprocessing = tf.keras.Model(inputs, preprocessed_inputs)\n",
        "lstm_model = tf.keras.Model(preprocessed_inputs, outputs)\n",
        "\n",
        "# Apply preprocessing asynchonously with tf.data.\n",
        "preprocessed_train_ds = apply_preprocessing(train_ds, lstm_preprocessing)\n",
        "preprocessed_test_ds = apply_preprocessing(test_ds, lstm_preprocessing)\n",
        "\n",
        "# Train the model.\n",
        "lstm_model.compile(loss='binary_crossentropy', metrics='accuracy')\n",
        "lstm_model.summary()\n",
        "lstm_model.fit(\n",
        "    preprocessed_train_ds, validation_data=preprocessed_test_ds, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 250)]             0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 250, 32)           320000    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 250, 32)           0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 32)                8320      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 328,353\n",
            "Trainable params: 328,353\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 64s 72ms/step - loss: 0.4346 - accuracy: 0.7988 - val_loss: 0.3552 - val_accuracy: 0.8603\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.2940 - accuracy: 0.8852 - val_loss: 0.3413 - val_accuracy: 0.8696\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.2553 - accuracy: 0.9045 - val_loss: 0.3411 - val_accuracy: 0.8688\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.2369 - accuracy: 0.9130 - val_loss: 0.3418 - val_accuracy: 0.8686\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.2225 - accuracy: 0.9196 - val_loss: 0.3433 - val_accuracy: 0.8660\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f20f748e9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kj-8hAItARbS"
      },
      "source": [
        "## Results and inference\n",
        "\n",
        "That's it for four quick experiments with different model architectures. Interestingly, the largest model we tried (LSTMs) ended up performing the worst. The bigram model performed the best at over 90% validation accuracy.\n",
        "\n",
        "This is a good insight to gain--without pretraining, on a dataset this size, simple ngram models are going to be very hard to beat. There's a bit more accuracy we could gain using larger vocabularies and larger ngrams--feel free to try it out in this colab!\n",
        "\n",
        "The last thing we will do is demo saving a model for a later inference. We will save the bigram model with preprocessing and training grouped together, so our single saved can directly run directly on raw text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae-EFXWzORa8",
        "outputId": "97bbdd7c-13da-450f-f217-6be05a1babdc"
      },
      "source": [
        "# Group our preprocessing and model into a single model that handles raw data.\n",
        "inputs = bigram_preprocessing.input\n",
        "outputs = bigram_model(bigram_preprocessing(inputs))\n",
        "inference_model = tf.keras.Model(inputs, outputs)\n",
        "inference_model.save(\"final_model\")\n",
        "\n",
        "# Load the model back from disk and make a prediction.\n",
        "loaded_model = tf.keras.models.load_model(\"final_model\")\n",
        "loaded_model.predict(\n",
        "    tf.constant(['Terrible, no good, trash.', 'I loved this movie!']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: final_model/assets\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.27302995],\n",
              "       [0.78468144]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}
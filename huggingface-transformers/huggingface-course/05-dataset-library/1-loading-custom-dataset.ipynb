{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "What if my dataset isn't on the Hub?",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-research-and-practice/blob/main/huggingface-transformers/huggingface-course/05-dataset-library/1-loading-custom-dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFCljkVIhei2"
      },
      "source": [
        "## What if my dataset isn't on the Hub?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rjT2ufWhei5"
      },
      "source": [
        "You know how to use the Hugging Face Hub to download datasets, but youâ€™ll often find yourself working with data that is stored either on your laptop or on a remote server. In this section weâ€™ll show you how ðŸ¤— Datasets can be used to load datasets that arenâ€™t available on the Hugging Face Hub.\n",
        "\n",
        "Let's install the Transformers and Datasets libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jShceIIhei6"
      },
      "source": [
        "!pip -q install datasets transformers[sentencepiece]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XeBdTR-jLYd"
      },
      "source": [
        "from datasets import load_dataset"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ity2mIJ9h9yp"
      },
      "source": [
        "##Working with local and remote datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xctEwMygh-zj"
      },
      "source": [
        "ðŸ¤— Datasets provides loading scripts to handle the loading of local and remote datasets. It supports several common data formats, such as:\n",
        "\n",
        "\n",
        "|Data format\t| Loading script\t| Example |\n",
        "|--|--|--|\n",
        "|CSV & TSV\t| csv\t| load_dataset(\"csv\", data_files=\"my_file.csv\") |\n",
        "|Text files\t| text\t| load_dataset(\"text\", data_files=\"my_file.txt\") |\n",
        "| JSON & JSON Lines\t| json\t| load_dataset(\"json\", data_files=\"my_file.jsonl\") |\n",
        "| Pickled DataFrames\t| pandas\t| load_dataset(\"pandas\", data_files=\"my_dataframe.pkl\") |\n",
        "\n",
        "As shown in the table, for each data format we just need to specify the type of loading script in the load_dataset() function, along with a data_files argument that specifies the path to one or more files. Letâ€™s start by loading a dataset from local files; later weâ€™ll see how to do the same with remote files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pj9F2Gji6UP"
      },
      "source": [
        "##Loading a local dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yLlN1IHi7C0"
      },
      "source": [
        "For this example weâ€™ll use the [SQuAD-it dataset](https://github.com/crux82/squad-it/), which is a large-scale dataset for question answering in Italian.\n",
        "\n",
        "The training and test splits are hosted on GitHub, so we can download them with a simple wget command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sng-kcaghei7"
      },
      "source": [
        "!wget -q https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
        "!wget -q https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARmTnRStja4L"
      },
      "source": [
        "This will download two compressed files called SQuAD_it-train.json.gz and SQuAD_it-test.json.gz, which we can decompress with the Linux gzip command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teCEsW4vhei8",
        "outputId": "5a3059fb-5f39-4de5-9583-aa234209fc20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!gzip -dkv SQuAD_it-*.json.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SQuAD_it-test.json.gz:\t 87.5% -- replaced with SQuAD_it-test.json\n",
            "SQuAD_it-train.json.gz:\t 82.3% -- replaced with SQuAD_it-train.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1PeAzsTjiWz"
      },
      "source": [
        "We can see that the compressed files have been replaced with SQuAD_it-train.json and SQuAD_it-text.json, and that the data is stored in the JSON format.\n",
        "\n",
        "\n",
        "To load a JSON file with the load_dataset() function, we just need to know if weâ€™re dealing with ordinary JSON (similar to a nested dictionary) or JSON Lines (line-separated JSON). \n",
        "\n",
        "Like many question answering datasets, SQuAD-it uses the nested format, with all the text stored in a data field. This means we can load the dataset by specifying the field argument as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-1vi_p3hei8"
      },
      "source": [
        "squad_it_dataset = load_dataset(\"json\", data_files=\"SQuAD_it-train.json\", field=\"data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufFZvJYtjv-q"
      },
      "source": [
        "By default, loading local files creates a DatasetDict object with a train split. We can see this by inspecting the squad_it_dataset object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc9xnlshhei9",
        "outputId": "06f75c87-d3df-4049-d535-3b87e4126276",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "squad_it_dataset"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['title', 'paragraphs'],\n",
              "        num_rows: 442\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9jJPf02j0H7"
      },
      "source": [
        "This shows us the number of rows and the column names associated with the training set. We can view one of the examples by indexing into the train split as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZygVLVLxhei-"
      },
      "source": [
        "#quad_it_dataset[\"train\"][0]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc_kYHyRkJc-"
      },
      "source": [
        "Great, weâ€™ve loaded our first local dataset! But while this worked for the training set, what we really want is to include both the train and test splits in a single DatasetDict object so we can apply Dataset.map() functions across both splits at once. \n",
        "\n",
        "To do this, we can provide a dictionary to the data_files argument that maps each split name to a file associated with that split:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_53lzgihei_"
      },
      "source": [
        "data_files = {\"train\": \"SQuAD_it-train.json\", \"test\": \"SQuAD_it-test.json\"}\n",
        "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnSKYIcdmIft",
        "outputId": "272fc444-b384-4a62-8b4b-88e40932c289",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "squad_it_dataset"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['title', 'paragraphs'],\n",
              "        num_rows: 442\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['title', 'paragraphs'],\n",
              "        num_rows: 48\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bElqvAe0lIfJ"
      },
      "source": [
        "#squad_it_dataset[\"train\"][0]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRoRj_ODlQuT"
      },
      "source": [
        "#squad_it_dataset[\"test\"][0]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3B5_F9Blixb"
      },
      "source": [
        "This is exactly what we wanted. Now, we can apply various preprocessing techniques to clean up the data, tokenize the reviews, and so on.\n",
        "\n",
        ">The data_files argument of the load_dataset() function is quite flexible and can be either a single file path, a list of file paths, or a dictionary that maps split names to file paths. You can also glob files that match a specified pattern according to the rules used by the Unix shell (e.g., you can glob all the JSON files in a directory as a single split by setting data_files=\"*.json\"). See the ðŸ¤— [Datasets documentation](https://huggingface.co/docs/datasets/loading.html#local-and-remote-files) for more details.\n",
        "\n",
        "The loading scripts in ðŸ¤— Datasets actually support automatic decompression of the input files, so we could have skipped the use of gzip by pointing the data_files argument directly to the compressed files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HweD8RA8hejA"
      },
      "source": [
        "data_files = {\"train\": \"SQuAD_it-train.json.gz\", \"test\": \"SQuAD_it-test.json.gz\"}\n",
        "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWSNZJ6KmC2S",
        "outputId": "d1e59ac1-72be-4f40-eb5d-f77363cc6b97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "squad_it_dataset"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['title', 'paragraphs'],\n",
              "        num_rows: 442\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['title', 'paragraphs'],\n",
              "        num_rows: 48\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYXUT4vKmWbs"
      },
      "source": [
        "This can be useful if you donâ€™t want to manually decompress many GZIP files. The automatic decompression also applies to other common formats like ZIP and TAR, so you just need to point data_files to the compressed files and youâ€™re good to go!\n",
        "\n",
        "Now that you know how to load local files on your laptop or desktop, letâ€™s take a look at loading remote files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4Xp5TCtmY0_"
      },
      "source": [
        "##Loading a remote dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5lFVnIDmZpZ"
      },
      "source": [
        "If youâ€™re working as a data scientist or coder in a company, thereâ€™s a good chance the datasets you want to analyze are stored on some remote server. Fortunately, loading remote files is just as simple as loading local ones! Instead of providing a path to local files, we point the data_files argument of load_dataset() to one or more URLs where the remote files are stored. \n",
        "\n",
        "For example, for the SQuAD-it dataset hosted on GitHub, we can just point data_files to the SQuAD_it-*.json.gz URLs as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yoOT4FthejB"
      },
      "source": [
        "url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
        "data_files = {\n",
        "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
        "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
        "}\n",
        "\n",
        "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbNV-AC2m5i_",
        "outputId": "3f2914ef-afcd-42de-ab80-7a23c7f0bf23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "squad_it_dataset"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['title', 'paragraphs'],\n",
              "        num_rows: 442\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['title', 'paragraphs'],\n",
              "        num_rows: 48\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqb5OHARnIbL"
      },
      "source": [
        "This returns the same DatasetDict object obtained above, but saves us the step of manually downloading and decompressing the SQuAD_it-*.json.gz files. This wraps up our foray into the various ways to load datasets that arenâ€™t hosted on the Hugging Face Hub. Now that weâ€™ve got a dataset to play with, letâ€™s get our hands dirty with various data-wrangling techniques!"
      ]
    }
  ]
}
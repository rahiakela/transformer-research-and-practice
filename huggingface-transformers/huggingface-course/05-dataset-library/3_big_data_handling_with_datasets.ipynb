{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-big-data-handling-with-datasets.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMYKzegc5JRov6pzmTCbHQu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-research-and-practice/blob/main/huggingface-transformers/huggingface-course/05-dataset-library/3_big_data_handling_with_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Big data handling with ðŸ¤— Datasets to the rescue!"
      ],
      "metadata": {
        "id": "st4AyN3BaIgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nowadays it is not uncommon to find yourself working with multi-gigabyte datasets, especially if youâ€™re planning to pretrain a transformer like BERT or GPT-2 from scratch. In these cases, even loading the data can be a challenge. For example, the WebText corpus used to pretrain GPT-2 consists of over 8 million documents and 40 GB of text â€” loading this into your laptopâ€™s RAM is likely to give it a heart attack!\n",
        "\n",
        "Fortunately, ðŸ¤— Datasets has been designed to overcome these limitations. It frees you from memory management problems by treating datasets as memory-mapped files, and from hard drive limits by streaming the entries in a corpus.\n",
        "\n",
        "**Reference**:\n",
        "\n",
        "[Big data? ðŸ¤— Datasets to the rescue!](https://huggingface.co/course/chapter5/4?fw=pt)\n",
        "\n",
        "[Apache Arrow: Read DataFrame With Zero Memory](https://towardsdatascience.com/apache-arrow-read-dataframe-with-zero-memory-69634092b1a)"
      ],
      "metadata": {
        "id": "K7g587hAaf1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "wvkKHNinaghL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers[sentencepiece]\n",
        "!pip install zstandard\n",
        "!pip install psutil"
      ],
      "metadata": {
        "id": "r06uHW-oaiFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from datasets import interleave_datasets\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import psutil\n",
        "import timeit\n",
        "from itertools import islice"
      ],
      "metadata": {
        "id": "vom9QhNWmb9r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load dataset"
      ],
      "metadata": {
        "id": "9qKC7ypdmXTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can load the dataset using the method for remote files"
      ],
      "metadata": {
        "id": "U8JffpaHmY1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This takes a few minutes to run, so go grab a tea or coffee while you wait :)\n",
        "data_files = \"https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst\"\n",
        "pubmed_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")"
      ],
      "metadata": {
        "id": "XJxjK6d1mZP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pubmed_dataset"
      ],
      "metadata": {
        "id": "fEeYyaXqm9Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pubmed_dataset[0]"
      ],
      "metadata": {
        "id": "rd4cfhH7n8DY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The magic of memory mapping"
      ],
      "metadata": {
        "id": "YMuOJWT8n-16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process.memory_info is expressed in bytes, so convert to megabytes\n",
        "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
      ],
      "metadata": {
        "id": "f8A2xqiSoHKD",
        "outputId": "f7be0861-436a-4ae0-a6ad-5e3313d5f03e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAM used: 1171.57 MB\n"
          ]
        }
      ]
    }
  ]
}
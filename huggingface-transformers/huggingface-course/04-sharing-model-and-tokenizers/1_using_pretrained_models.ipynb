{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-using-pretrained-models",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/huggingface-transformers-practice/blob/main/huggingface-course/04-sharing-model-and-tokenizers/1_using_pretrained_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH0iF0lrdnNL"
      },
      "source": [
        "# Using pretrained models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUwjSQBUeGYc"
      },
      "source": [
        "The Model Hub makes selecting the appropriate model simple, so that using it in any downstream library can be done in a few lines of code. Let’s take a look at how to actually use one of these models, and how to contribute back to the community.\n",
        "\n",
        "Let’s say we’re looking for a French-based model that can perform mask filling.\n",
        "\n",
        "\n",
        "<img src='https://huggingface.co/course/static/chapter4/camembert.gif?raw=1' width='800'/>\n",
        "\n",
        "We select the `camembert-base` checkpoint to try it out. The identifier `camembert-base` is all we need to start using it! As you’ve seen in previous chapters, we can instantiate it using a `pipeline`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWfGeFu1dnNM"
      },
      "source": [
        "Install the Transformers and Datasets libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ys4TtVKXdnNM"
      },
      "source": [
        "!pip -q install datasets transformers[sentencepiece]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CveyCdldnNN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96035ab0-a909-439f-a0de-594ed2fce324"
      },
      "source": [
        "from transformers import pipeline \n",
        "\n",
        "camembert_fill_mask  = pipeline(\"fill-mask\", model=\"camembert-base\")\n",
        "results = camembert_fill_mask(\"Le camembert est <mask> :)\")\n",
        "\n",
        "results"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.49091005325317383,\n",
              "  'sequence': 'Le camembert est délicieux :)',\n",
              "  'token': 7200,\n",
              "  'token_str': 'délicieux'},\n",
              " {'score': 0.1055697426199913,\n",
              "  'sequence': 'Le camembert est excellent :)',\n",
              "  'token': 2183,\n",
              "  'token_str': 'excellent'},\n",
              " {'score': 0.03453313186764717,\n",
              "  'sequence': 'Le camembert est succulent :)',\n",
              "  'token': 26202,\n",
              "  'token_str': 'succulent'},\n",
              " {'score': 0.0330314114689827,\n",
              "  'sequence': 'Le camembert est meilleur :)',\n",
              "  'token': 528,\n",
              "  'token_str': 'meilleur'},\n",
              " {'score': 0.03007650189101696,\n",
              "  'sequence': 'Le camembert est parfait :)',\n",
              "  'token': 1654,\n",
              "  'token_str': 'parfait'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiyXKgtMfMkA"
      },
      "source": [
        "As you can see, loading a model within a `pipeline` is extremely simple. The only thing you need to watch out for is that the chosen checkpoint is suitable for the task it’s going to be used for. \n",
        "\n",
        "For example, here we are loading the `camembert-base` checkpoint in the `fill-mask` pipeline, which is completely fine. But if we were to load this checkpoint in the `text-classification` pipeline, the results would not make any sense because the head of `camembert-base` is not suitable for this task! We recommend using the task selector in the Hugging Face Hub interface in order to select the appropriate checkpoints:\n",
        "\n",
        "<img src='https://huggingface.co/course/static/chapter4/tasks.png?raw=1' width='800'/>\n",
        "\n",
        "You can also instantiate the checkpoint using the model architecture directly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvoL88K1dnNN"
      },
      "source": [
        "from transformers import CamembertTokenizer, CamembertForMaskedLM \n",
        "\n",
        "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
        "model = CamembertForMaskedLM.from_pretrained(\"camembert-base\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRz9ESDihoP9"
      },
      "source": [
        "However, we recommend using the `Auto*` classes instead, as these are by design architecture-agnostic. While the previous code sample limits users to checkpoints loadable in the CamemBERT architecture, using the `Auto*` classes makes switching checkpoints simple:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMGT8Mf8dnNO"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM \n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"camembert-base\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wWYrOx4h1RQ"
      },
      "source": [
        ">When using a pretrained model, make sure to check how it was trained, on which datasets, its limits, and its biases. All of this information should be indicated on its model card."
      ]
    }
  ]
}
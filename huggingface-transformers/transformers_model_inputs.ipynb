{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformers-model-inputs.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOb3kVVo9qZ0YUV3brUMhJs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bd938845c6a34aadbf177e3bbf8b623c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_95b0c1a4162540dab9e3aa21e20b4868",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_58044cb0c31c41649f3847e0c61d07bc",
              "IPY_MODEL_39d19c7eea3648258ffae365decc2336"
            ]
          }
        },
        "95b0c1a4162540dab9e3aa21e20b4868": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "58044cb0c31c41649f3847e0c61d07bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0ff3ecbf2ae144498b04bebcce9555c6",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d1c667f3dd4442dd9cfd8672195659fc"
          }
        },
        "39d19c7eea3648258ffae365decc2336": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0c79ffeb16fb432785f8108bf8fc42ca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 213k/213k [00:00&lt;00:00, 719kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a4ed4ca00dde434285da6235d0c1e2e7"
          }
        },
        "0ff3ecbf2ae144498b04bebcce9555c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d1c667f3dd4442dd9cfd8672195659fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0c79ffeb16fb432785f8108bf8fc42ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a4ed4ca00dde434285da6235d0c1e2e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-case-studies/blob/master/huggingface-transformers-practice/transformers_model_inputs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIvF7CGYrYwk"
      },
      "source": [
        "## Transformers Model inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKpZmNfXsbME"
      },
      "source": [
        "Every model is different yet bears similarities with the others. Therefore most models use the same inputs, which are detailed here alongside usage examples.\r\n",
        "\r\n",
        "Reference: https://huggingface.co/transformers/glossary.html#model-inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fE7nWAQsd37"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPgan5tHsfGU"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWNfh3LCshwh"
      },
      "source": [
        "from transformers import BertTokenizer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L8WCLINrN3A"
      },
      "source": [
        "## Input IDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqmBLx5PsOB1"
      },
      "source": [
        "The input ids are often the only required parameters to be passed to the model as input. They are token indices, numerical representations of tokens building the sequences that will be used as input by the model.\r\n",
        "\r\n",
        "Each tokenizer works differently but the underlying mechanism remains the same. Here’s an example using the BERT tokenizer, which is a WordPiece tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "bd938845c6a34aadbf177e3bbf8b623c",
            "95b0c1a4162540dab9e3aa21e20b4868",
            "58044cb0c31c41649f3847e0c61d07bc",
            "39d19c7eea3648258ffae365decc2336",
            "0ff3ecbf2ae144498b04bebcce9555c6",
            "d1c667f3dd4442dd9cfd8672195659fc",
            "0c79ffeb16fb432785f8108bf8fc42ca",
            "a4ed4ca00dde434285da6235d0c1e2e7"
          ]
        },
        "id": "rvzvJbEmrOXR",
        "outputId": "c37d0678-e9c7-46cf-d737-b0b675b34f81"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\r\n",
        "\r\n",
        "sequence = \"A Titan RTX has 24GB of VRAM\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd938845c6a34aadbf177e3bbf8b623c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Apznh28tDgv"
      },
      "source": [
        "The tokenizer takes care of splitting the sequence into tokens available in the tokenizer vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AsTDlSwrPi5",
        "outputId": "a9316413-a3ac-4814-95c0-ff8b81b30b65"
      },
      "source": [
        "tokenized_sequence = tokenizer.tokenize(sequence)\r\n",
        "print(tokenized_sequence)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pieir5aFtUsw"
      },
      "source": [
        "The tokens are either words or subwords. Here for instance, “VRAM” wasn’t in the model vocabulary, so it’s been split in “V”, “RA” and “M”. To indicate those tokens are not separate words but parts of the same word, a double-hash prefix is added for “RA” and “M”:\r\n",
        "\r\n",
        "These tokens can then be converted into IDs which are understandable by the model. This can be done by directly feeding the sentence to the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qS3MKoDtMXY"
      },
      "source": [
        "inputs = tokenizer(sequence)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_k7YmCaByKgM"
      },
      "source": [
        "The tokenizer returns a dictionary with all the arguments necessary for its corresponding model to work properly. The token indices are under the key “input_ids”:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-arwUDayJ8y",
        "outputId": "8c9aa1a9-cc01-4b89-df17-cada8e76d9b5"
      },
      "source": [
        "encoded_sequence = inputs[\"input_ids\"]\r\n",
        "print(encoded_sequence)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[101, 138, 18696, 155, 1942, 3190, 1144, 1572, 13745, 1104, 159, 9664, 2107, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6xdL7ShyYAm"
      },
      "source": [
        "If we decode the previous sequence of ids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDkstOqZySiH",
        "outputId": "fbffa864-65c1-4a78-fd2a-631ff582c5c0"
      },
      "source": [
        "decoded_sequence = tokenizer.decode(encoded_sequence)\r\n",
        "print(decoded_sequence)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] A Titan RTX has 24GB of VRAM [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgu6QAlEynIx"
      },
      "source": [
        "because this is the way a BertModel is going to expect its inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWDu4kDrypfm"
      },
      "source": [
        "## Attention mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNTRlHYpyqj8"
      },
      "source": [
        "The attention mask is an optional argument used when batching sequences together. This argument indicates to the model which tokens should be attended to, and which should not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Gb2FhwhyhGk"
      },
      "source": [
        "sequence_a = \"This is a short sequence.\"\r\n",
        "sequence_b = \"This is a rather long sequence. It is at least longer than the sequence A.\"\r\n",
        "\r\n",
        "encoded_sequence_a = tokenizer(sequence_a)[\"input_ids\"]\r\n",
        "encoded_sequence_b = tokenizer(sequence_b)[\"input_ids\"]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlwftcLwPVgf"
      },
      "source": [
        "The encoded versions have different lengths:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVUmBtagPV__",
        "outputId": "da94e39e-13cc-4658-91e3-c9a2f5785931"
      },
      "source": [
        "len(encoded_sequence_a), len(encoded_sequence_b)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 19)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKqnHaQzRmEZ"
      },
      "source": [
        "Therefore, we can’t put them together in the same tensor as-is. The first sequence needs to be padded up to the length of the second one, or the second one needs to be truncated down to the length of the first one.\r\n",
        "\r\n",
        "In the first case, the list of IDs will be extended by the padding indices. We can pass a list to the tokenizer and ask it to pad like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGdupPbbPbl2"
      },
      "source": [
        "padded_sequences = tokenizer([sequence_a, sequence_b], padding=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mdwqLp6XVf_"
      },
      "source": [
        "We can see that 0s have been added on the right of the first sentence to make it the same length as the second one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9rl7Y5BXXEp",
        "outputId": "3345692c-d12c-454c-a8a4-2072e8842813"
      },
      "source": [
        "padded_sequences[\"input_ids\"]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[101, 1188, 1110, 170, 1603, 4954, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [101,\n",
              "  1188,\n",
              "  1110,\n",
              "  170,\n",
              "  1897,\n",
              "  1263,\n",
              "  4954,\n",
              "  119,\n",
              "  1135,\n",
              "  1110,\n",
              "  1120,\n",
              "  1655,\n",
              "  2039,\n",
              "  1190,\n",
              "  1103,\n",
              "  4954,\n",
              "  138,\n",
              "  119,\n",
              "  102]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9DBPxQvXqn-"
      },
      "source": [
        "This can then be converted into a tensor in PyTorch or TensorFlow. The attention mask is a binary tensor indicating the position of the padded indices so that the model does not attend to them. For the `BertTokenizer`, 1 indicates a value that should be attended to, while 0 indicates a padded value. This attention mask is in the dictionary returned by the tokenizer under the key “attention_mask”:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZpBuOITXaX1",
        "outputId": "127d3035-7ac1-43a9-f4ea-cd34843f9871"
      },
      "source": [
        "padded_sequences[\"attention_mask\"]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xthkdkb8X3-z"
      },
      "source": [
        "## Token Type IDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDIA0Xw2X4vc"
      },
      "source": [
        "Some models’ purpose is to do sequence classification or question answering. These require two different sequences to be joined in a single “input_ids” entry, which usually is performed with the help of special tokens, such as the classifier `([CLS])` and separator `([SEP])` tokens. \r\n",
        "\r\n",
        "For example, the BERT model builds its two sequence input as such:\r\n",
        "\r\n",
        "```\r\n",
        "[CLS] SEQUENCE_A [SEP] SEQUENCE_B [SEP]\r\n",
        "```\r\n",
        "\r\n",
        "We can use our tokenizer to automatically generate such a sentence by passing the two sequences to tokenizer as two arguments (and not a list, like before) like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEAORC6iX1eg",
        "outputId": "94259ef1-982c-4081-e2ae-79046420ce5e"
      },
      "source": [
        "sequence_a = \"HuggingFace is based in NYC\"\r\n",
        "sequence_b = \"Where is HuggingFace based?\"\r\n",
        "\r\n",
        "encoded_dict = tokenizer(sequence_a, sequence_b)\r\n",
        "print(encoded_dict)\r\n",
        "decoded = tokenizer.decode(encoded_dict[\"input_ids\"])\r\n",
        "print(decoded)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': [101, 20164, 10932, 2271, 7954, 1110, 1359, 1107, 17520, 102, 2777, 1110, 20164, 10932, 2271, 7954, 1359, 136, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "[CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace based? [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGzsP4tuaPUo"
      },
      "source": [
        "This is enough for some models to understand where one sequence ends and where another begins. However, other models, such as BERT, also deploy token type IDs (also called segment IDs). They are represented as a binary mask identifying the two types of sequence in the model.\r\n",
        "\r\n",
        "The tokenizer returns this mask as the “token_type_ids” entry:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKs2SGksZ1i_",
        "outputId": "b07ae8d9-a9a1-4bd3-c1f3-ece63e689d79"
      },
      "source": [
        "encoded_dict[\"token_type_ids\"]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P66_8eJZapNS"
      },
      "source": [
        "The first sequence, the “context” used for the question, has all its tokens represented by a 0, whereas the second sequence, corresponding to the “question”, has all its tokens represented by a 1.\r\n",
        "\r\n",
        "Some models, like XLNetModel use an additional token represented by a 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHq4OQtda5hP"
      },
      "source": [
        "## Position IDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgMqIO-ua6mr"
      },
      "source": [
        "Contrary to RNNs that have the position of each token embedded within them, transformers are unaware of the position of each token. Therefore, the position IDs (position_ids) are used by the model to identify each token’s position in the list of tokens.\r\n",
        "\r\n",
        "They are an optional parameter. If no position_ids are passed to the model, the IDs are automatically created as absolute positional embeddings.\r\n",
        "\r\n",
        "Absolute positional embeddings are selected in the range `[0, config.max_position_embeddings - 1]`. Some models use other types of positional embeddings, such as sinusoidal position embeddings or relative position embeddings."
      ]
    }
  ]
}
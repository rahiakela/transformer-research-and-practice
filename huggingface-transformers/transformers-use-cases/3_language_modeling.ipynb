{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-language-modeling.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN72X4WiUg3I+PrN07CXlR+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-case-studies/blob/master/huggingface-transformers-practice/transformers-use-cases/3_language_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jivRMocQdzfI"
      },
      "source": [
        "## Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9Pb8l0Si-dN"
      },
      "source": [
        "Language modeling is the task of fitting a model to a corpus, which can be domain specific. All popular transformer-based models are trained using a variant of language modeling, e.g. \n",
        "\n",
        "- **BERT with masked language modeling**\n",
        "- **GPT-2 with causal language modeling**\n",
        "\n",
        "Language modeling can be useful outside of pretraining as well, for example to shift the model distribution to be domain-specific: using a language model trained over a very large corpus, and then fine-tuning it to a news dataset or on scientific papers e.g. [LysandreJik/arxiv-nlp](https://huggingface.co/lysandre/arxiv-nlp).\n",
        "\n",
        "Referemce: https://huggingface.co/transformers/task_summary.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7PywmnSL_uC"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYm8-n_bMelQ"
      },
      "source": [
        "%tensorflow_version 2.x     # magic command instructing to use TensorFlow version 2+\n",
        "import tensorflow as tf\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl-GcyGXMBJ9"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z8AJjW2MDHx"
      },
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead, top_k_top_p_filtering\n",
        "from torch.nn import functional as F\n",
        "from transformers import TFAutoModelWithLMHead, tf_top_k_top_p_filtering\n",
        "\n",
        "from pprint import pprint"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufsiNBKUM81e"
      },
      "source": [
        "## Masked Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r_dReC8M_Q8"
      },
      "source": [
        "Masked language modeling is the task of masking tokens in a sequence with a masking token, and prompting the model to fill that mask with an appropriate token. This allows the model to attend to both the right context (tokens on the right of the mask) and the left context (tokens on the left of the mask). Such a training creates a strong basis for downstream tasks requiring bi-directional context, such as SQuAD (question answering, see [Lewis, Lui, Goyal et al.](https://arxiv.org/abs/1910.13461), part 4.2). \n",
        "\n",
        "Here is an example of using pipelines to replace a mask from a sequence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IYOeyNWNEHJ"
      },
      "source": [
        "nlp = pipeline(\"fill-mask\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3God-mH6lqo"
      },
      "source": [
        "This outputs the sequences with the mask filled, the confidence score, and the token id in the tokenizer vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PCTygTD6fHq",
        "outputId": "e168478a-ae84-415a-c23b-da9c5082d35c"
      },
      "source": [
        "pprint(nlp(f\"HuggingFace is creating a {nlp.tokenizer.mask_token} that the community uses to solve NLP tasks.\"))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'score': 0.17927460372447968,\n",
            "  'sequence': 'HuggingFace is creating a tool that the community uses to solve '\n",
            "              'NLP tasks.',\n",
            "  'token': 3944,\n",
            "  'token_str': ' tool'},\n",
            " {'score': 0.1134939044713974,\n",
            "  'sequence': 'HuggingFace is creating a framework that the community uses to '\n",
            "              'solve NLP tasks.',\n",
            "  'token': 7208,\n",
            "  'token_str': ' framework'},\n",
            " {'score': 0.05243545398116112,\n",
            "  'sequence': 'HuggingFace is creating a library that the community uses to '\n",
            "              'solve NLP tasks.',\n",
            "  'token': 5560,\n",
            "  'token_str': ' library'},\n",
            " {'score': 0.03493543714284897,\n",
            "  'sequence': 'HuggingFace is creating a database that the community uses to '\n",
            "              'solve NLP tasks.',\n",
            "  'token': 8503,\n",
            "  'token_str': ' database'},\n",
            " {'score': 0.02860247902572155,\n",
            "  'sequence': 'HuggingFace is creating a prototype that the community uses to '\n",
            "              'solve NLP tasks.',\n",
            "  'token': 17715,\n",
            "  'token_str': ' prototype'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjG36GE2OVcR"
      },
      "source": [
        "Here is an example of doing masked language modeling using a model and a tokenizer. The process is the following:\n",
        "\n",
        "1. Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a DistilBERT model and loads it with the weights stored in the checkpoint.\n",
        "2. Define a sequence with a masked token, placing the tokenizer.mask_token instead of a word.\n",
        "3. Encode that sequence into a list of IDs and find the position of the masked token in that list.\n",
        "4. Retrieve the predictions at the index of the mask token: this tensor has the same size as the vocabulary, and the values are the scores attributed to each token. The model gives higher score to tokens it deems probable in that context.\n",
        "5. Retrieve the top 5 tokens using the PyTorch topk or TensorFlow top_k methods.\n",
        "6. Replace the mask token by the tokens and print the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lnhjx-EaOUsU"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
        "model = TFAutoModelWithLMHead.from_pretrained(\"distilbert-base-cased\")\n",
        "\n",
        "sequence = f\"\"\"Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppLY_BcxRpfb"
      },
      "source": [
        "input = tokenizer.encode(sequence, return_tensors=\"tf\")\n",
        "mask_token_index = tf.where(input==tokenizer.mask_token_id)[0, 1]\n",
        "\n",
        "token_logits = model(input)[0]\n",
        "mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "\n",
        "top_5_tokens = tf.math.top_k(mask_token_logits, 5).indices.numpy()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3RrBz2TS_Nm"
      },
      "source": [
        "This prints five sequences, with the top 5 tokens predicted by the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxFQh5tk-25B",
        "outputId": "419a370e-cea7-46e2-891f-7e3646699b51"
      },
      "source": [
        "for token in top_5_tokens:\n",
        "  print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reduce our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help increase our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help decrease our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help offset our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help improve our carbon footprint.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHPtRBDpX2nB",
        "outputId": "056f98e0-3d74-4aa3-d5e6-4eec7bcea07c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "top_10_tokens = tf.math.top_k(mask_token_logits, 10).indices.numpy()\n",
        "\n",
        "for token in top_10_tokens:\n",
        "  print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reduce our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help increase our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help decrease our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help offset our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help improve our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help minimize our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help limit our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help expand our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reducing our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help lower our carbon footprint.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-tdLBZCCveP"
      },
      "source": [
        "### PyTorch implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO5_CIA2Rv31"
      },
      "source": [
        "# Pytorch\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"distilbert-base-cased\")\n",
        "\n",
        "sequence = f\"\"\"Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Km0Y33QSL2X"
      },
      "source": [
        "input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "mask_token_index = torch.where(input==tokenizer.mask_token_id)[1]\n",
        "\n",
        "token_logits = model(input).logits\n",
        "mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "\n",
        "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMhwNu_waihY",
        "outputId": "6a33dd93-e5c9-4d4a-db6b-4b59c4859bf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for token in top_5_tokens:\n",
        "  print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reduce our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help increase our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help decrease our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help offset our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help improve our carbon footprint.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufMYC83fat4K",
        "outputId": "07b73126-b9bc-4d17-af1f-45c3cb078bf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "top_10_tokens = torch.topk(mask_token_logits, 10, dim=1).indices[0].tolist()\n",
        "\n",
        "for token in top_10_tokens:\n",
        "  print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reduce our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help increase our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help decrease our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help offset our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help improve our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help minimize our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help limit our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help expand our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reducing our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help lower our carbon footprint.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zmi5r8sHa7Rb"
      },
      "source": [
        "## Causal Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksPHZb0Wa8dq"
      },
      "source": [
        "Causal language modeling is the task of predicting the token following a sequence of tokens. In this situation, the model only attends to the left context (tokens on the left of the mask). Such a training is particularly interesting for generation tasks.\n",
        "\n",
        "Usually, the next token is predicted by sampling from the logits of the last hidden state the model produces from the input sequence.\n",
        "\n",
        "Here is an example of using the tokenizer and model and leveraging the `top_k_top_p_filtering()` method to sample the next token following an input sequence of tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvZzuL15a3TY"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = TFAutoModelWithLMHead.from_pretrained(\"gpt2\")\n",
        "\n",
        "sequence = f\"\"\"Hugging Face is based in DUMBO, New York City, and \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-njjkaPbyeV"
      },
      "source": [
        "input_ids = tokenizer.encode(sequence, return_tensors=\"tf\")\n",
        "\n",
        "# get logits of last hidden state\n",
        "next_token_logits = model(input_ids)[0][:, -1, :]\n",
        "\n",
        "# filter\n",
        "filtered_next_token_logits = tf_top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)\n",
        "\n",
        "# sample\n",
        "next_token = tf.random.categorical(filtered_next_token_logits, dtype=tf.int32, num_samples=2)\n",
        "\n",
        "generated = tf.concat([input_ids, next_token], axis=1)\n",
        "\n",
        "resulting_string = tokenizer.decode(generated.numpy().tolist()[0])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjL24NvIc0dK"
      },
      "source": [
        "This outputs a (hopefully) coherent next token following the original sequence, which in our case is the word has:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmuxMLw5cz9h",
        "outputId": "3caa078d-6634-4761-a2cb-517bb37f9060",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(resulting_string)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hugging Face is based in DUMBO, New York City, and   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-uME-POd4-D"
      },
      "source": [
        "### PyTorch implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkIt_7unc5eL"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"gpt2\")\n",
        "\n",
        "sequence = f\"\"\"Hugging Face is based in DUMBO, New York City, and \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFvjn-eueBE7"
      },
      "source": [
        "input_ids = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "\n",
        "# get logits of last hidden state\n",
        "next_token_logits = model(input_ids).logits[:, -1, :]\n",
        "\n",
        "# filter\n",
        "filtered_next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)\n",
        "\n",
        "# sample\n",
        "probs = F.softmax(filtered_next_token_logits, dim=-1)\n",
        "next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "generated = torch.cat([input_ids, next_token], dim=-1)\n",
        "\n",
        "resulting_string = tokenizer.decode(generated.tolist()[0])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYY4GfUdfnNc"
      },
      "source": [
        "This outputs a (hopefully) coherent next token following the original sequence, which in our case is the word has:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSt7XtEZfWbd",
        "outputId": "93d54be0-0d4f-44cc-a0b1-691b7bd321c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(resulting_string)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hugging Face is based in DUMBO, New York City, and  \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
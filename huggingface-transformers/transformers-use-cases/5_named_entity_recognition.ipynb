{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5-named-entity-recognition.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPlfNLROgELJF3RfZLVnyAd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-case-studies/blob/master/huggingface-transformers-practice/transformers-use-cases/5_named_entity_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jivRMocQdzfI"
      },
      "source": [
        "## Named Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9Pb8l0Si-dN"
      },
      "source": [
        "Named Entity Recognition (NER) is the task of classifying tokens according to a class, for example, identifying a token as a person, an organisation or a location. An example of a named entity recognition dataset is the CoNLL-2003 dataset, which is entirely based on that task.\n",
        "\n",
        "Here is an example of using pipelines to do named entity recognition, specifically, trying to identify tokens as belonging to one of 9 classes:\n",
        "\n",
        "- O, Outside of a named entity\n",
        "- B-MIS, Beginning of a miscellaneous entity right after another miscellaneous entity\n",
        "- I-MIS, Miscellaneous entity\n",
        "- B-PER, Beginning of a person’s name right after another person’s name\n",
        "- I-PER, Person’s name\n",
        "- B-ORG, Beginning of an organisation right after another organisation\n",
        "- I-ORG, Organisation\n",
        "- B-LOC, Beginning of a location right after another location\n",
        "- I-LOC, Location\n",
        "\n",
        "Referemce: https://huggingface.co/transformers/task_summary.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7PywmnSL_uC"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYm8-n_bMelQ"
      },
      "source": [
        "#%tensorflow_version 2.x     # magic command instructing to use TensorFlow version 2+\n",
        "import tensorflow as tf\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl-GcyGXMBJ9"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z8AJjW2MDHx"
      },
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from torch.nn import functional as F\n",
        "from transformers import TFAutoModelForTokenClassification\n",
        "\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufsiNBKUM81e"
      },
      "source": [
        "## Loading Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r_dReC8M_Q8"
      },
      "source": [
        "It leverages a fine-tuned model on CoNLL-2003, fine-tuned by [@stefan-it](AutoModelForTokenClassification) from dbmdz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IYOeyNWNEHJ"
      },
      "source": [
        "nlp = pipeline(\"ner\")\n",
        "\n",
        "sequence = \"\"\"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very\n",
        " close to the Manhattan Bridge which is visible from the window.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCsE78jwh5yl"
      },
      "source": [
        "This outputs a list of all words that have been identified as one of the entities from the 9 classes defined above. Here are the expected results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Nhpeu0wh6cs",
        "outputId": "0852738e-e9ca-425c-ce18-699fd9fea0f0"
      },
      "source": [
        "print(nlp(sequence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'word': 'Hu', 'score': 0.999578595161438, 'entity': 'I-ORG', 'index': 1, 'start': 0, 'end': 2}, {'word': '##gging', 'score': 0.9909763932228088, 'entity': 'I-ORG', 'index': 2, 'start': 2, 'end': 7}, {'word': 'Face', 'score': 0.9982224702835083, 'entity': 'I-ORG', 'index': 3, 'start': 8, 'end': 12}, {'word': 'Inc', 'score': 0.9994880557060242, 'entity': 'I-ORG', 'index': 4, 'start': 13, 'end': 16}, {'word': 'New', 'score': 0.9994344711303711, 'entity': 'I-LOC', 'index': 11, 'start': 40, 'end': 43}, {'word': 'York', 'score': 0.9993196129798889, 'entity': 'I-LOC', 'index': 12, 'start': 44, 'end': 48}, {'word': 'City', 'score': 0.9993793964385986, 'entity': 'I-LOC', 'index': 13, 'start': 49, 'end': 53}, {'word': 'D', 'score': 0.9862582683563232, 'entity': 'I-LOC', 'index': 19, 'start': 79, 'end': 80}, {'word': '##UM', 'score': 0.9514269232749939, 'entity': 'I-LOC', 'index': 20, 'start': 80, 'end': 82}, {'word': '##BO', 'score': 0.933659017086029, 'entity': 'I-LOC', 'index': 21, 'start': 82, 'end': 84}, {'word': 'Manhattan', 'score': 0.9761654138565063, 'entity': 'I-LOC', 'index': 28, 'start': 115, 'end': 124}, {'word': 'Bridge', 'score': 0.9914628863334656, 'entity': 'I-LOC', 'index': 29, 'start': 125, 'end': 131}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3God-mH6lqo"
      },
      "source": [
        "Note how the tokens of the sequence “Hugging Face” have been identified as an organisation, and “New York City”, “DUMBO” and “Manhattan Bridge” have been identified as locations.\n",
        "\n",
        "Here is an example of doing named entity recognition, using a model and a tokenizer. The process is the following:\n",
        "\n",
        "1. Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a BERT model and loads it with the weights stored in the checkpoint.\n",
        "2. Define the label list with which the model was trained on.\n",
        "3. Define a sequence with known entities, such as “Hugging Face” as an organisation and “New York City” as a location.\n",
        "4. Split words into tokens so that they can be mapped to predictions. We use a small hack by, first, completely encoding and decoding the sequence, so that we’re left with a string that contains the special tokens.\n",
        "5. Encode that sequence into IDs (special tokens are added automatically).\n",
        "6. Retrieve the predictions by passing the input to the model and getting the first output. This results in a distribution over the 9 possible classes for each token. We take the argmax to retrieve the most likely class for each token.\n",
        "7. Zip together each token with its prediction and print it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lnhjx-EaOUsU"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "model = TFAutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "\n",
        "label_list = [\n",
        "   \"O\",       # Outside of a named entity\n",
        "   \"B-MISC\",  # Beginning of a miscellaneous entity right after another miscellaneous entity\n",
        "   \"I-MISC\",  # Miscellaneous entity\n",
        "   \"B-PER\",   # Beginning of a person's name right after another person's name\n",
        "   \"I-PER\",   # Person's name\n",
        "   \"B-ORG\",   # Beginning of an organisation right after another organisation\n",
        "   \"I-ORG\",   # Organisation\n",
        "   \"B-LOC\",   # Beginning of a location right after another location\n",
        "   \"I-LOC\"    # Location           \n",
        "]\n",
        "\n",
        "sequence = \"\"\"\n",
        "Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very close to the Manhattan Bridge.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppLY_BcxRpfb"
      },
      "source": [
        "# Bit of a hack to get the tokens with the special tokens\n",
        "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n",
        "inputs = tokenizer.encode(sequence, return_tensors=\"tf\")\n",
        "\n",
        "outputs = model(inputs)[0]\n",
        "predictions = tf.argmax(outputs, axis=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3RrBz2TS_Nm"
      },
      "source": [
        "This outputs a list of each token mapped to its corresponding prediction. Differently from the pipeline, here every token has a prediction as we didn’t remove the “0”th class, which means that no particular entity was found on that token. The following array should be the output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j24a3e78mFkO",
        "outputId": "ae684e84-2a17-4a31-c3ff-5a2652759ace"
      },
      "source": [
        "print([(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0].numpy())])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('[CLS]', 'O'), ('Hu', 'I-ORG'), ('##gging', 'I-ORG'), ('Face', 'I-ORG'), ('Inc', 'I-ORG'), ('.', 'O'), ('is', 'O'), ('a', 'O'), ('company', 'O'), ('based', 'O'), ('in', 'O'), ('New', 'I-LOC'), ('York', 'I-LOC'), ('City', 'I-LOC'), ('.', 'O'), ('Its', 'O'), ('headquarters', 'O'), ('are', 'O'), ('in', 'O'), ('D', 'I-LOC'), ('##UM', 'I-LOC'), ('##BO', 'I-LOC'), (',', 'O'), ('therefore', 'O'), ('very', 'O'), ('close', 'O'), ('to', 'O'), ('the', 'O'), ('Manhattan', 'I-LOC'), ('Bridge', 'I-LOC'), ('.', 'O'), ('[SEP]', 'O')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-tdLBZCCveP"
      },
      "source": [
        "## PyTorch implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO5_CIA2Rv31"
      },
      "source": [
        "# Pytorch\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "\n",
        "label_list = [\n",
        "   \"O\",       # Outside of a named entity\n",
        "   \"B-MISC\",  # Beginning of a miscellaneous entity right after another miscellaneous entity\n",
        "   \"I-MISC\",  # Miscellaneous entity\n",
        "   \"B-PER\",   # Beginning of a person's name right after another person's name\n",
        "   \"I-PER\",   # Person's name\n",
        "   \"B-ORG\",   # Beginning of an organisation right after another organisation\n",
        "   \"I-ORG\",   # Organisation\n",
        "   \"B-LOC\",   # Beginning of a location right after another location\n",
        "   \"I-LOC\"    # Location           \n",
        "]\n",
        "\n",
        "sequence = \"\"\"\n",
        "Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very close to the Manhattan Bridge.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Km0Y33QSL2X"
      },
      "source": [
        "# Bit of a hack to get the tokens with the special tokens\n",
        "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n",
        "inputs = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model(inputs).logits\n",
        "predictions = torch.argmax(outputs, dim=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JVZC78UnIMW",
        "outputId": "01e9080f-d313-4ee3-fa3b-a18b66bb70b5"
      },
      "source": [
        "print([(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0].numpy())])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('[CLS]', 'O'), ('Hu', 'I-ORG'), ('##gging', 'I-ORG'), ('Face', 'I-ORG'), ('Inc', 'I-ORG'), ('.', 'O'), ('is', 'O'), ('a', 'O'), ('company', 'O'), ('based', 'O'), ('in', 'O'), ('New', 'I-LOC'), ('York', 'I-LOC'), ('City', 'I-LOC'), ('.', 'O'), ('Its', 'O'), ('headquarters', 'O'), ('are', 'O'), ('in', 'O'), ('D', 'I-LOC'), ('##UM', 'I-LOC'), ('##BO', 'I-LOC'), (',', 'O'), ('therefore', 'O'), ('very', 'O'), ('close', 'O'), ('to', 'O'), ('the', 'O'), ('Manhattan', 'I-LOC'), ('Bridge', 'I-LOC'), ('.', 'O'), ('[SEP]', 'O')]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
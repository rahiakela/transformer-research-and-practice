{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-text-or-sequence-classification.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP7WGbmezoF/tysGlRPo2Os",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ba9da7917c294ca2bb2b991822259a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0a2bd2d8ae974c49b8280e1893f8f1e5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2e80fe651c7e49428fb1b18f15f3303b",
              "IPY_MODEL_62dc7c713b9b4154b27058fdf1856b52"
            ]
          }
        },
        "0a2bd2d8ae974c49b8280e1893f8f1e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2e80fe651c7e49428fb1b18f15f3303b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_84415b65ef8848ffb1e14e4a0679bf3e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433297515,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433297515,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cdbaa4fdcbd0421589bf340dca32eaa2"
          }
        },
        "62dc7c713b9b4154b27058fdf1856b52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e8eb6bd01b02467e8950badddad0028d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433M/433M [00:11&lt;00:00, 39.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9a83137261d048b4914cf5683ce91516"
          }
        },
        "84415b65ef8848ffb1e14e4a0679bf3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cdbaa4fdcbd0421589bf340dca32eaa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e8eb6bd01b02467e8950badddad0028d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9a83137261d048b4914cf5683ce91516": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-case-studies/blob/master/huggingface-transformers-practice/transformers-use-cases/1_text_or_sequence_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jivRMocQdzfI"
      },
      "source": [
        "## Text/Sequence classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9Pb8l0Si-dN"
      },
      "source": [
        "Sequence classification is the task of classifying sequences according to a given number of classes. An example of sequence classification is the GLUE dataset, which is entirely based on that task. If you would like to fine-tune a model on a GLUE sequence classification task, you may leverage the `run_glue.py` and `run_pl_glue.py` or `run_tf_glue.py` scripts.\r\n",
        "\r\n",
        "\r\n",
        "Referemce: https://huggingface.co/transformers/task_summary.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7PywmnSL_uC"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYm8-n_bMelQ"
      },
      "source": [
        "%tensorflow_version 2.x     # magic command instructing to use TensorFlow version 2+\r\n",
        "import tensorflow as tf\r\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl-GcyGXMBJ9"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z8AJjW2MDHx"
      },
      "source": [
        "from transformers import pipeline\r\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\r\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufsiNBKUM81e"
      },
      "source": [
        "## Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r_dReC8M_Q8"
      },
      "source": [
        "Here is an example of using pipelines to do sentiment analysis: identifying if a sequence is positive or negative. It leverages a fine-tuned model on sst2, which is a GLUE task.\r\n",
        "\r\n",
        "This returns a label `(“POSITIVE” or “NEGATIVE”)` alongside a score, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IYOeyNWNEHJ",
        "outputId": "ca98d18a-db44-405b-9eed-1f4998c36e38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nlp = pipeline(\"sentiment-analysis\")\r\n",
        "\r\n",
        "result = nlp(\"I hate you\")[0]\r\n",
        "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label: NEGATIVE, with score: 0.9991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fodM2OgHNvpv",
        "outputId": "f65cba9b-6922-4242-ca7f-f58c3aeb56aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "result = nlp(\"I love you\")[0]\r\n",
        "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label: POSITIVE, with score: 0.9999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaU5rI0MOSbn"
      },
      "source": [
        "## Sequence Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjG36GE2OVcR"
      },
      "source": [
        "Here is an example of doing a sequence classification using a model to determine if two sequences are paraphrases of each other. The process is the following:\r\n",
        "\r\n",
        "1. Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a BERT model and loads it with the weights stored in the checkpoint.\r\n",
        "2. Build a sequence from the two sentences, with the correct model-specific separators token type ids and attention masks (`encode()` and `__call__()` take care of this).\r\n",
        "3. Pass this sequence through the model so that it is classified in one of the two available classes: 0 (not a paraphrase) and 1 (is a paraphrase).\r\n",
        "4. Compute the softmax of the result to get probabilities over the classes.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lnhjx-EaOUsU"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\r\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\r\n",
        "\r\n",
        "classes = [\"not paraphrase\", \"is paraphrase\"]\r\n",
        "\r\n",
        "sequence_0 = \"The company HuggingFace is based in New York City\"\r\n",
        "sequence_1 = \"Apples are especially bad for your health\"\r\n",
        "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9djdvi2dN_nr"
      },
      "source": [
        "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"tf\")\r\n",
        "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"tf\")\r\n",
        "\r\n",
        "paraphrase_classification_logits = model(paraphrase)[0]\r\n",
        "not_paraphrase_classification_logits = model(not_paraphrase)[0]\r\n",
        "\r\n",
        "paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]\r\n",
        "not_paraphrase_results = tf.nn.softmax(not_paraphrase_classification_logits, axis=1).numpy()[0]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ydy5M_6fRJpl",
        "outputId": "a4f38435-0a57-4286-a9f3-e86d87af8309",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Should be paraphrase\r\n",
        "for i in range(len(classes)):\r\n",
        "  print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "not paraphrase: 10%\n",
            "is paraphrase: 90%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj11tZeuRmrK",
        "outputId": "6a301b09-dabb-4f32-d959-57fcd8be8996",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Should not be paraphrase\r\n",
        "for i in range(len(classes)):\r\n",
        "  print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "not paraphrase: 94%\n",
            "is paraphrase: 6%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO5_CIA2Rv31",
        "outputId": "07b08877-f6cc-497c-a4ff-5e4ce963ace5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "ba9da7917c294ca2bb2b991822259a73",
            "0a2bd2d8ae974c49b8280e1893f8f1e5",
            "2e80fe651c7e49428fb1b18f15f3303b",
            "62dc7c713b9b4154b27058fdf1856b52",
            "84415b65ef8848ffb1e14e4a0679bf3e",
            "cdbaa4fdcbd0421589bf340dca32eaa2",
            "e8eb6bd01b02467e8950badddad0028d",
            "9a83137261d048b4914cf5683ce91516"
          ]
        }
      },
      "source": [
        "# Pytorch\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\r\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\r\n",
        "\r\n",
        "classes = [\"not paraphrase\", \"is paraphrase\"]\r\n",
        "\r\n",
        "sequence_0 = \"The company HuggingFace is based in New York City\"\r\n",
        "sequence_1 = \"Apples are especially bad for your health\"\r\n",
        "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba9da7917c294ca2bb2b991822259a73",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433297515.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Km0Y33QSL2X"
      },
      "source": [
        "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\r\n",
        "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\r\n",
        "\r\n",
        "paraphrase_classification_logits = model(**paraphrase).logits\r\n",
        "not_paraphrase_classification_logits = model(**not_paraphrase).logits\r\n",
        "\r\n",
        "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\r\n",
        "not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJBEEXG5SlEI",
        "outputId": "8625e445-c275-4882-b60e-98b6631ee2d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Should be paraphrase\r\n",
        "for i in range(len(classes)):\r\n",
        "  print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "not paraphrase: 10%\n",
            "is paraphrase: 90%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXrRuYI4Su9l",
        "outputId": "065af802-81ce-46f1-bee0-8eb41168a075",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Should not be paraphrase\r\n",
        "for i in range(len(classes)):\r\n",
        "  print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "not paraphrase: 94%\n",
            "is paraphrase: 6%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
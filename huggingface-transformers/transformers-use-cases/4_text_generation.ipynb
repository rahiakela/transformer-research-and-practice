{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4-text-generation.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNOOW7JLIwhzl7JFb00eZFL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-case-studies/blob/master/huggingface-transformers-practice/transformers-use-cases/4_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jivRMocQdzfI"
      },
      "source": [
        "## Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9Pb8l0Si-dN"
      },
      "source": [
        "In text generation (a.k.a open-ended text generation) the goal is to create a coherent portion of text that is a continuation from the given context. The following example shows how GPT-2 can be used in pipelines to generate text. As a default all models apply Top-K sampling when used in pipelines, as configured in their respective configurations.\n",
        "\n",
        "Referemce: https://huggingface.co/transformers/task_summary.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7PywmnSL_uC"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYm8-n_bMelQ"
      },
      "source": [
        "%tensorflow_version 2.x     # magic command instructing to use TensorFlow version 2+\n",
        "import tensorflow as tf\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl-GcyGXMBJ9"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z8AJjW2MDHx"
      },
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead, top_k_top_p_filtering\n",
        "from torch.nn import functional as F\n",
        "from transformers import TFAutoModelWithLMHead, tf_top_k_top_p_filtering\n",
        "\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufsiNBKUM81e"
      },
      "source": [
        "## Loading Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r_dReC8M_Q8"
      },
      "source": [
        "Here, the model generates a random text with a total maximal length of 50 tokens from context “As far as I am concerned, I will”."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IYOeyNWNEHJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae84b787-4abc-437d-ab51-ff1ee9ffcd0d"
      },
      "source": [
        "text_generator = pipeline(\"text-generation\")\n",
        "\n",
        "print(text_generator(\"As far as I am concerned, I will\", max_length=50, do_sample=False))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[{'generated_text': 'As far as I am concerned, I will be the first to admit that I am not a fan of the idea of a \"free market.\" I think that the idea of a free market is a bit of a stretch. I think that the idea'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3God-mH6lqo"
      },
      "source": [
        "The default arguments of `PreTrainedModel.generate()` can be directly overridden in the pipeline, as is shown above for the argument `max_length`.\n",
        "\n",
        "Here is an example of text generation using XLNet and its tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lnhjx-EaOUsU"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "model = TFAutoModelWithLMHead.from_pretrained(\"xlnet-base-cased\")\n",
        "\n",
        "# Padding text helps XLNet with short prompts - proposed by Aman Rusia in https://github.com/rusiaaman/XLNet-gen#methodology\n",
        "PADDING_TEXT = \"\"\"\n",
        "In 1991, the remains of Russian Tsar Nicholas II and his family (except for Alexei and Maria) are discovered.\n",
        " The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the remainder of the story. 1883 Western Siberia,\n",
        " a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
        " Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
        " father initially slaps him for making such an accusation, Rasputin watches as the\n",
        " man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
        " the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
        " with people, even a bishop, begging for his blessing. <eod> </s> <eos>\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppLY_BcxRpfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee5eb06f-296e-4e03-9e8b-7f1231e889cc"
      },
      "source": [
        "prompt = \"Today the weather is really nice and I am planning on \"\n",
        "\n",
        "inputs = tokenizer.encode(PADDING_TEXT + prompt, add_special_tokens=False, return_tensors=\"tf\")\n",
        "\n",
        "prompt_length = len(tokenizer.decode(inputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
        "outputs = model.generate(inputs, max_length=250, do_sample=True, top_p=0.95, top_k=60)\n",
        "generated = prompt + tokenizer.decode(outputs[0])[prompt_length:]\n",
        "\n",
        "print(generated)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Today the weather is really nice and I am planning on anning on sleeping in a new home. This week, I plan to try out new places and try out new things for myself. As far as I can tell, it is going to be a great beginning to my new year.<eop> In January, my last post was in my Life Journal, for a few weeks after I graduated from college. I said I wanted to post in\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3RrBz2TS_Nm"
      },
      "source": [
        "Text generation is currently possible with GPT-2, OpenAi-GPT, CTRL, XLNet, Transfo-XL and Reformer in PyTorch and for most models in Tensorflow as well. As can be seen in the example above XLNet and Transfo-XL often need to be padded to work well. GPT-2 is usually a good choice for open-ended text generation because it was trained on millions of webpages with a causal language modeling objective.\n",
        "\n",
        "For more information on how to apply different decoding strategies for text generation, please also refer to [our text generation blog post here](https://huggingface.co/blog/how-to-generate)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-tdLBZCCveP"
      },
      "source": [
        "## PyTorch implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO5_CIA2Rv31"
      },
      "source": [
        "# Pytorch\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"xlnet-base-cased\")\n",
        "\n",
        "# Padding text helps XLNet with short prompts - proposed by Aman Rusia in https://github.com/rusiaaman/XLNet-gen#methodology\n",
        "PADDING_TEXT = \"\"\"\n",
        "In 1991, the remains of Russian Tsar Nicholas II and his family (except for Alexei and Maria) are discovered.\n",
        " The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the remainder of the story. 1883 Western Siberia,\n",
        " a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
        " Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
        " father initially slaps him for making such an accusation, Rasputin watches as the\n",
        " man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
        " the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
        " with people, even a bishop, begging for his blessing. <eod> </s> <eos>\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Km0Y33QSL2X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c437af16-ad3d-4724-e28f-8ceec218b611"
      },
      "source": [
        "prompt = \"Today the weather is really nice and I am planning on \"\n",
        "\n",
        "inputs = tokenizer.encode(PADDING_TEXT + prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "prompt_length = len(tokenizer.decode(inputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
        "outputs = model.generate(inputs, max_length=250, do_sample=True, top_p=0.95, top_k=60)\n",
        "generated = prompt + tokenizer.decode(outputs[0])[prompt_length:]\n",
        "\n",
        "print(generated)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Today the weather is really nice and I am planning on anning on going to the beach at the beach (Fl. 25 and 26) with my friend A. I can still do this but s... I must get ready and be up in the morning by 2AM!<eop> A.: \"I am tired and I am really not good.\" I \"I will get ready today in the morning\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
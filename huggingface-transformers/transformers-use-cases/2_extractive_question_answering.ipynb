{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-extractive-question-answering.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO0QayVSEeCKTIAAenYJguj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7ca54a165f4f4ed79aad5d22ef96fb77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_629b4d7a6be34ae78460ac2f32480d24",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b6bd0d06bb3e46dc913f7b065bb851a9",
              "IPY_MODEL_4b6bdf495be941ca9acfdedad9fa7208"
            ]
          }
        },
        "629b4d7a6be34ae78460ac2f32480d24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b6bd0d06bb3e46dc913f7b065bb851a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d7fb7f4be56a44328cba826be711cf76",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1340675298,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1340675298,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f8d6403f0634504a86002bb99147c4e"
          }
        },
        "4b6bdf495be941ca9acfdedad9fa7208": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cb15721da81c44228ef606e212209373",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.34G/1.34G [00:38&lt;00:00, 34.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1afcee0e45514679be479275b8e38423"
          }
        },
        "d7fb7f4be56a44328cba826be711cf76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f8d6403f0634504a86002bb99147c4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cb15721da81c44228ef606e212209373": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1afcee0e45514679be479275b8e38423": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-case-studies/blob/master/huggingface-transformers-practice/transformers-use-cases/2_extractive_question_answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jivRMocQdzfI"
      },
      "source": [
        "## Extractive Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9Pb8l0Si-dN"
      },
      "source": [
        "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune a model on a SQuAD task, you may leverage the run_qa.py and run_tf_squad.py scripts.\n",
        "\n",
        "Referemce: https://huggingface.co/transformers/task_summary.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7PywmnSL_uC"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYm8-n_bMelQ"
      },
      "source": [
        "%tensorflow_version 2.x     # magic command instructing to use TensorFlow version 2+\n",
        "import tensorflow as tf\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl-GcyGXMBJ9"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z8AJjW2MDHx"
      },
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from transformers import TFAutoModelForQuestionAnswering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufsiNBKUM81e"
      },
      "source": [
        "## Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r_dReC8M_Q8"
      },
      "source": [
        "Here is an example of using pipelines to do question answering: extracting an answer from a text given a question. It leverages a fine-tuned model on SQuAD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IYOeyNWNEHJ"
      },
      "source": [
        "nlp = pipeline(\"question-answering\")\n",
        "\n",
        "context = r\"\"\"\n",
        "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
        "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
        "a model on a SQuAD task, you may leverage the examples/question-answering/run_squad.py script.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3God-mH6lqo"
      },
      "source": [
        "This returns an answer extracted from the text, a confidence score, alongside “start” and “end” values, which are the positions of the extracted answer in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PCTygTD6fHq",
        "outputId": "1de886f2-42a9-4bca-ace8-0808186123bf"
      },
      "source": [
        "result = nlp(question=\"What is extractive question answering?\", context=context)\n",
        "print(f\"Answer: {result['answer']}, score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Answer: the task of extracting an answer from a text given a question, score: 0.6226, start: 34, end: 95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fodM2OgHNvpv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "940e5a0e-e3c7-45a7-aed3-910c6ab0c240"
      },
      "source": [
        "result = nlp(question=\"What is a good example of a question answering dataset?\", context=context)\n",
        "print(f\"Answer: {result['answer']}, score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Answer: SQuAD dataset, score: 0.5053, start: 147, end: 160\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjG36GE2OVcR"
      },
      "source": [
        "Here is an example of question answering using a model and a tokenizer. The process is the following:\n",
        "\n",
        "1. Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a BERT model and loads it with the weights stored in the checkpoint.\n",
        "2. Define a text and a few questions.\n",
        "3. Iterate over the questions and build a sequence from the text and the current question, with the correct model-specific separators token type ids and attention masks.\n",
        "4. Pass this sequence through the model. This outputs a range of scores across the entire sequence tokens (question and text), for both the start and end positions.\n",
        "5. Compute the softmax of the result to get probabilities over the tokens.\n",
        "6. Fetch the tokens from the identified start and stop values, convert those tokens to a string.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lnhjx-EaOUsU"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "model = TFAutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "\n",
        "text = r\"\"\"\n",
        " 🤗 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
        " architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural\n",
        " Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
        " TensorFlow 2.0 and PyTorch.\n",
        "\"\"\"\n",
        "\n",
        "questions = [\n",
        "  \"How many pretrained models are available in 🤗 Transformers?\",\n",
        "  \"What does 🤗 Transformers provide?\",\n",
        "  \"🤗 Transformers provides interoperability between which frameworks?\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxFQh5tk-25B",
        "outputId": "10307506-8fa6-46c2-b27f-1489cc463478"
      },
      "source": [
        "for question in questions:\n",
        "  inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"tf\")\n",
        "  input_ids = inputs[\"input_ids\"].numpy()[0]\n",
        "\n",
        "  outputs = model(inputs)\n",
        "  answer_start_scores = outputs.start_logits\n",
        "  answer_end_scores = outputs.end_logits\n",
        "\n",
        "  # Get the most likely beginning of answer with the argmax of the score\n",
        "  answer_start = tf.argmax(answer_start_scores, axis=1).numpy()[0]\n",
        "  # Get the most likely end of answer with the argmax of the score\n",
        "  answer_end = (tf.argmax(answer_end_scores, axis=1) + 1).numpy()[0]\n",
        "\n",
        "  answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start: answer_end]))\n",
        "\n",
        "  print(f\"Question: {question}\")\n",
        "  print(f\"Answer: {answer}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: How many pretrained models are available in 🤗 Transformers?\n",
            "Answer: over 32 +\n",
            "Question: What does 🤗 Transformers provide?\n",
            "Answer: general - purpose architectures\n",
            "Question: 🤗 Transformers provides interoperability between which frameworks?\n",
            "Answer: tensorflow 2. 0 and pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-tdLBZCCveP"
      },
      "source": [
        "## PyTorch implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO5_CIA2Rv31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "7ca54a165f4f4ed79aad5d22ef96fb77",
            "629b4d7a6be34ae78460ac2f32480d24",
            "b6bd0d06bb3e46dc913f7b065bb851a9",
            "4b6bdf495be941ca9acfdedad9fa7208",
            "d7fb7f4be56a44328cba826be711cf76",
            "8f8d6403f0634504a86002bb99147c4e",
            "cb15721da81c44228ef606e212209373",
            "1afcee0e45514679be479275b8e38423"
          ]
        },
        "outputId": "68926017-f1d0-4e1e-8bbd-2ea9ed29a2c5"
      },
      "source": [
        "# Pytorch\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ca54a165f4f4ed79aad5d22ef96fb77",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1340675298.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Km0Y33QSL2X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07c1c3cf-b6e7-4e8d-bfe2-e4892f8aa505"
      },
      "source": [
        "for question in questions:\n",
        "  inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
        "  input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "  outputs = model(**inputs)\n",
        "  answer_start_scores = outputs.start_logits\n",
        "  answer_end_scores = outputs.end_logits\n",
        "\n",
        "  # Get the most likely beginning of answer with the argmax of the score\n",
        "  answer_start = torch.argmax(answer_start_scores)\n",
        "  # Get the most likely end of answer with the argmax of the score\n",
        "  answer_end = torch.argmax(answer_end_scores) + 1\n",
        "\n",
        "  answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start: answer_end]))\n",
        "\n",
        "  print(f\"Question: {question}\")\n",
        "  print(f\"Answer: {answer}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: How many pretrained models are available in 🤗 Transformers?\n",
            "Answer: over 32 +\n",
            "Question: What does 🤗 Transformers provide?\n",
            "Answer: general - purpose architectures\n",
            "Question: 🤗 Transformers provides interoperability between which frameworks?\n",
            "Answer: tensorflow 2. 0 and pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
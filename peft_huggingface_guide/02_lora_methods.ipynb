{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMQeQ9xJhHGlFuKo7++OLlz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-research-and-practice/blob/main/peft_huggingface_guide/02_lora_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "O61soCxohJkw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reference**:\n",
        "\n",
        "https://huggingface.co/docs/peft/v0.9.0/task_guides/lora_based_methods"
      ],
      "metadata": {
        "id": "xEYA3SLvlf6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q peft transformers datasets"
      ],
      "metadata": {
        "id": "DvdkpmKEhLO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "from transformers import AutoImageProcessor\n",
        "\n",
        "import torch\n",
        "from torchvision.transforms import (\n",
        "    CenterCrop,\n",
        "    Compose,\n",
        "    Normalize,\n",
        "    RandomHorizontalFlip,\n",
        "    RandomResizedCrop,\n",
        "    Resize,\n",
        "    ToTensor,\n",
        ")"
      ],
      "metadata": {
        "id": "SUKyPxVYhNG_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset"
      ],
      "metadata": {
        "id": "PeCB00WihT0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"food101\")"
      ],
      "metadata": {
        "id": "Af-UBLQThVTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ds[\"train\"].features[\"label\"].names\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = i\n",
        "    id2label[i] = label\n",
        "\n",
        "id2label[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6Ulx803CiHJA",
        "outputId": "451cdd96-33e8-4f67-e731-dc479aa0258c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'baklava'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an image processor to properly resize and normalize the pixel values\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")"
      ],
      "metadata": {
        "id": "XrpFVlYBiMiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's prepare some transformation functions for data augmentation and pixel scaling\n",
        "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
        "\n",
        "train_transforms = Compose(\n",
        "    [\n",
        "        RandomResizedCrop(image_processor.size[\"height\"]),\n",
        "        RandomHorizontalFlip(),\n",
        "        ToTensor(),\n",
        "        normalize\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transforms = Compose(\n",
        "    [\n",
        "        Resize(image_processor.size[\"height\"]),\n",
        "        CenterCrop(image_processor.size[\"height\"]),\n",
        "        ToTensor(),\n",
        "        normalize\n",
        "    ]\n",
        ")\n",
        "\n",
        "def preprocess_train(example_batch):\n",
        "  example_batch[\"pixel_values\"] = [\n",
        "      train_transforms(image.convert(\"RGB\") for image in example_batch[\"image\"])\n",
        "  ]\n",
        "  return example_batch\n",
        "\n",
        "def preprocess_val(example_batch):\n",
        "  example_batch[\"pixel_values\"] = [\n",
        "      val_transforms(image.convert(\"RGB\") for image in example_batch[\"image\"])\n",
        "  ]\n",
        "  return example_batch"
      ],
      "metadata": {
        "id": "0nUqGfdViuev"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's define the training and validation datasets\n",
        "train_ds = ds[\"train\"]\n",
        "val_ds = ds[\"validation\"]\n",
        "\n",
        "train_ds.set_transform(preprocess_train)\n",
        "val_ds.set_transform(preprocess_val)"
      ],
      "metadata": {
        "id": "mcpXWQU8kJcN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, need a data collator to create a batch of training and evaluation data\n",
        "# and convert the labels to torch.tensor objects\n",
        "def collate_fn(examples):\n",
        "  pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "  labels = torch.stack([example[\"label\"] for example in examples])\n",
        "  return {\"pixel_values\": pixel_values, \"labels\": labels}"
      ],
      "metadata": {
        "id": "llzjxuvgkfRy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model"
      ],
      "metadata": {
        "id": "VMpKOmVmlbUh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n34pNIR7lYzb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
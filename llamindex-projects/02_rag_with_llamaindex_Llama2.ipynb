{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-research-and-practice/blob/main/llamindex-projects/02_rag_with_llamaindex_Llama2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG System using Llama2 with Hugging Face"
      ],
      "metadata": {
        "id": "tnUOq3dZeDK1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHP9Gfafdq57"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops accelerate langchain bitsandbytes"
      ],
      "metadata": {
        "id": "YYeHrwmHfbgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Embedding\n",
        "!pip install install sentence_transformers"
      ],
      "metadata": {
        "id": "Tu1g-xwnfv9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index"
      ],
      "metadata": {
        "id": "urL9wZkqf-Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex,SimpleDirectoryReader,ServiceContext\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "from llama_index.prompts.prompts import SimpleInputPrompt\n",
        "from llama_index import ServiceContext\n",
        "from llama_index.embeddings import LangchainEmbedding\n",
        "from llama_index.response.pprint_utils import pprint_response\n",
        "\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "import torch"
      ],
      "metadata": {
        "id": "HutpiHgpgLBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "W2ulYaoChmZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://arxiv.org/pdf/1706.03762.pdf\n",
        "!wget https://arxiv.org/pdf/1506.02640.pdf\n",
        "\n",
        "!mkdir data\n",
        "!mv 1706.03762.pdf attention.pdf\n",
        "!mv 1506.02640.pdf yolo.pdf\n",
        "\n",
        "!mv *.pdf data/"
      ],
      "metadata": {
        "id": "2Q1iQIbdbCzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading documents"
      ],
      "metadata": {
        "id": "TSbdV2jXa8H4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents=SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "documents"
      ],
      "metadata": {
        "id": "KcU-nQ_XgxWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt=\"\"\"\n",
        "You are a Q&A assistant. Your goal is to answer questions as\n",
        "accurately as possible based on the instructions and context provided.\n",
        "\"\"\"\n",
        "\n",
        "## Default format supportable by LLama2\n",
        "query_wrapper_prompt=SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")"
      ],
      "metadata": {
        "id": "2g7M89fKg54H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5-mOhoIhfo0",
        "outputId": "62a1e1ac-8a8f-4d6f-9338-49c28836ee8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LLM model"
      ],
      "metadata": {
        "id": "dBGm_jw2d-9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    device_map=\"auto\",\n",
        "    # uncomment this if using CUDA to reduce memory usage\n",
        "    model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_8bit\":True}\n",
        ")"
      ],
      "metadata": {
        "id": "jyVOhSuhhqdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model=LangchainEmbedding(\n",
        "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        ")"
      ],
      "metadata": {
        "id": "pr1EN5sViQm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_context=ServiceContext.from_defaults(\n",
        "    chunk_size=1024,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")"
      ],
      "metadata": {
        "id": "LduiJD2ajpy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ_jtoK3kCeT",
        "outputId": "b38933e9-776e-4fea-f924-862489fb117a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ServiceContext(llm_predictor=LLMPredictor(system_prompt=None, query_wrapper_prompt=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>), prompt_helper=PromptHelper(context_window=4096, num_output=256, chunk_overlap_ratio=0.1, chunk_size_limit=None, separator=' '), embed_model=LangchainEmbedding(model_name='sentence-transformers/all-mpnet-base-v2', embed_batch_size=10, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7b3afc9c8fa0>), transformations=[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7b3afc9c8fa0>, id_func=<function default_id_func at 0x7b3bb8cea290>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')], llama_logger=<llama_index.logger.base.LlamaLogger object at 0x7b3bb64f48e0>, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7b3afc9c8fa0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vector Index"
      ],
      "metadata": {
        "id": "bjUuU5mjhwLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store_index=VectorStoreIndex.from_documents(documents, service_context=service_context)"
      ],
      "metadata": {
        "id": "31jvrW2BkFEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IM-gNZ0-kRnO",
        "outputId": "74c928e3-c794-4190-9ae9-02792af35eb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<llama_index.indices.vector_store.base.VectorStoreIndex at 0x7b3b2306e7d0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine=vector_store_index.as_query_engine()"
      ],
      "metadata": {
        "id": "uSJzrMm6kTxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Query vector index"
      ],
      "metadata": {
        "id": "-c3kZIFchzWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response=query_engine.query(\"what is attention is all you need?\")\n",
        "pprint_response(response, show_source=True)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdhKFWCTkZRx",
        "outputId": "bb3d3683-7808-4238-e5f9-5f1b10525e62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Response: Attention is a powerful tool in NLP, but it is not the\n",
            "only thing you need to build a successful model. While attention\n",
            "mechanisms like the one described in the passage can help the model\n",
            "focus on relevant parts of the input, they do not address other\n",
            "important aspects of language processing, such as syntax and\n",
            "semantics. To build a truly robust NLP model, you will need to\n",
            "incorporate a variety of techniques, including attention, as well as\n",
            "other types of neural network layers and traditional NLP methods.\n",
            "______________________________________________________________________\n",
            "Source Node 1/2\n",
            "Node ID: a1933801-245c-4048-ae18-67dbab1654d6\n",
            "Similarity: 0.5568833006661468\n",
            "Text: Input-Input Layer5 The Law will never be perfect , but its\n",
            "application should be just - this is what we are missing , in my\n",
            "opinion . <EOS> <pad> The Law will never be perfect , but its\n",
            "application should be just - this is what we are missing , in my\n",
            "opinion . <EOS> <pad> Input-Input Layer5 The Law will never be perfect\n",
            ", but its application sho...\n",
            "______________________________________________________________________\n",
            "Source Node 2/2\n",
            "Node ID: f2dae21c-d0e8-4013-a9f0-cd7eeead0f28\n",
            "Similarity: 0.5035814770931348\n",
            "Text: output values. These are concatenated and once again projected,\n",
            "resulting in the final values, as depicted in Figure 2. Multi-head\n",
            "attention allows the model to jointly attend to information from\n",
            "different representation subspaces at different positions. With a\n",
            "single attention head, averaging inhibits this. MultiHead( Q, K, V ) =\n",
            "Concat(head 1,...\n",
            "Attention is a powerful tool in NLP, but it is not the only thing you need to build a successful model. While attention mechanisms like the one described in the passage can help the model focus on relevant parts of the input, they do not address other important aspects of language processing, such as syntax and semantics. To build a truly robust NLP model, you will need to incorporate a variety of techniques, including attention, as well as other types of neural network layers and traditional NLP methods.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=query_engine.query(\"what is Transformers?\")\n",
        "pprint_response(response, show_source=True)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "_WA2L72BiQuX",
        "outputId": "4ac6bdaa-a001-442b-8e24-65ecfaeac8f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Response: Transformers is a type of neural network architecture\n",
            "introduced in the paper \"Attention is All You Need\" by Vaswani et al.\n",
            "in 2017. It's primarily designed for sequence-to-sequence tasks, such\n",
            "as machine translation, and it relies on self-attention mechanisms to\n",
            "process input sequences. The Transformer model consists of an encoder\n",
            "and a decoder, each composed of multiple identical layers. Each layer\n",
            "in the encoder and decoder contains a stack of two sub-layers: a\n",
            "multi-head self-attention mechanism and a position-wise fully\n",
            "connected feed-forward network. The self-attention mechanism allows\n",
            "the model to attend to different parts of the input sequence\n",
            "simultaneously, while the feed-forward network processes the output of\n",
            "the self-attention mechanism to produce the final output. The\n",
            "Transformer model also uses a technique called attention masking to\n",
            "prevent the model from attending to positions in the input sequence\n",
            "that are beyond the current position being processed. This allows the\n",
            "model to generate output sequences one token at a time, without\n",
            "relying on any recurrence or convolution.\n",
            "______________________________________________________________________\n",
            "Source Node 1/2\n",
            "Node ID: 27eab5ed-bcb2-4636-92be-f9e5d53230ae\n",
            "Similarity: 0.35967615564596855\n",
            "Text: Figure 1: The Transformer - model architecture. The Transformer\n",
            "follows this overall architecture using stacked self-attention and\n",
            "point-wise, fully connected layers for both the encoder and decoder,\n",
            "shown in the left and right halves of Figure 1, respectively. 3.1\n",
            "Encoder and Decoder Stacks Encoder: The encoder is composed of a stack\n",
            "of N= 6 id...\n",
            "______________________________________________________________________\n",
            "Source Node 2/2\n",
            "Node ID: f2dae21c-d0e8-4013-a9f0-cd7eeead0f28\n",
            "Similarity: 0.2619791269538337\n",
            "Text: output values. These are concatenated and once again projected,\n",
            "resulting in the final values, as depicted in Figure 2. Multi-head\n",
            "attention allows the model to jointly attend to information from\n",
            "different representation subspaces at different positions. With a\n",
            "single attention head, averaging inhibits this. MultiHead( Q, K, V ) =\n",
            "Concat(head 1,...\n",
            "Transformers is a type of neural network architecture introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. It's primarily designed for sequence-to-sequence tasks, such as machine translation, and it relies on self-attention mechanisms to process input sequences. The Transformer model consists of an encoder and a decoder, each composed of multiple identical layers. Each layer in the encoder and decoder contains a stack of two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The self-attention mechanism allows the model to attend to different parts of the input sequence simultaneously, while the feed-forward network processes the output of the self-attention mechanism to produce the final output. The Transformer model also uses a technique called attention masking to prevent the model from attending to positions in the input sequence that are beyond the current position being processed. This allows the model to generate output sequences one token at a time, without relying on any recurrence or convolution.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=query_engine.query(\"what is YOLO?\")\n",
        "pprint_response(response, show_source=True)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHLOq8j2kyfF",
        "outputId": "70057d8b-9f2a-4017-9232-7719fc51e37f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Response: YOLO is a real-time object detection system that uses\n",
            "a single neural network to predict bounding boxes and class\n",
            "probabilities directly from full images. It is simple, fast, and\n",
            "achieves high performance on object detection tasks. YOLO is trained\n",
            "on full images and directly optimizes detection performance, making it\n",
            "different from traditional object detection methods. It is also shown\n",
            "to be effective in detecting objects in artwork, where other methods\n",
            "struggle.\n",
            "______________________________________________________________________\n",
            "Source Node 1/2\n",
            "Node ID: 0b2b0b90-582c-4dc9-a31d-25226fd0ac95\n",
            "Similarity: 0.4745535599386536\n",
            "Text: 8 27.6 52.0 41.7 69.6 61.3 68.3 57.8 29.6 57.8 40.9 59.3 54.1\n",
            "SDS [16] 50.7 69.7 58.4 48.5 28.3 28.8 61.3 57.5 70.8 24.1 50.7 35.9\n",
            "64.9 59.1 65.8 57.1 26.0 58.8 38.6 58.9 50.7 R-CNN [13] 49.6 68.1 63.8\n",
            "46.1 29.4 27.9 56.6 57.0 65.9 26.5 48.7 39.5 66.2 57.3 65.4 53.2 26.2\n",
            "54.5 38.1 50.6 51.6 Table 3: PASCAL VOC 2012 Leaderboard. YOLO\n",
            "compared wit...\n",
            "______________________________________________________________________\n",
            "Source Node 2/2\n",
            "Node ID: d90acbe2-84c7-486a-923a-3d0c66ea2f95\n",
            "Similarity: 0.46492172271335436\n",
            "Text: YOLO is refreshingly simple: see Figure 1. A sin- gle\n",
            "convolutional network simultaneously predicts multi- ple bounding\n",
            "boxes and class probabilities for those boxes. YOLO trains on full\n",
            "images and directly optimizes detec- tion performance. This uniﬁed\n",
            "model has several beneﬁts over traditional methods of object\n",
            "detection. First, YOLO is extrem...\n",
            "YOLO is a real-time object detection system that uses a single neural network to predict bounding boxes and class probabilities directly from full images. It is simple, fast, and achieves high performance on object detection tasks. YOLO is trained on full images and directly optimizes detection performance, making it different from traditional object detection methods. It is also shown to be effective in detecting objects in artwork, where other methods struggle.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=query_engine.query(\"what is Object detection?\")\n",
        "pprint_response(response, show_source=True)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "hx9-s0Ddidpn",
        "outputId": "00b24286-70a1-4e17-eb05-0982ec2f5efd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Response: Object detection is a core problem in computer vision\n",
            "that involves locating and classifying objects within images or\n",
            "videos. It is a fundamental task in many applications, such as\n",
            "autonomous driving, robotics, surveillance, and healthcare. Object\n",
            "detection can be broadly classified into two categories: instance-\n",
            "level detection and semantic segmentation. Instance-level detection\n",
            "involves identifying individual objects within an image, while\n",
            "semantic segmentation involves assigning a class label to each pixel\n",
            "in an image.  Object detection pipelines typically start by extracting\n",
            "robust features from input images, followed by classifiers or\n",
            "localizers that identify objects in the feature space. These\n",
            "classifiers or localizers can be run in a sliding window fashion over\n",
            "the whole image or on some subset of regions in the image.  Deformable\n",
            "parts models (DPM) and R-CNN are two popular object detection\n",
            "frameworks that use a sliding window approach to find objects in\n",
            "images. DPM uses a disjoint pipeline to extract static features,\n",
            "classify regions, predict bounding boxes for high-scoring regions,\n",
            "etc. R-CNN and its variants use region proposals instead of sliding\n",
            "windows to find objects in images.  YOLO (You Only Look Once\n",
            "______________________________________________________________________\n",
            "Source Node 1/2\n",
            "Node ID: 98caa1d5-1135-417c-a540-2ad45f6fa056\n",
            "Similarity: 0.5475054688741452\n",
            "Text: This spatial constraint lim- its the number of nearby objects\n",
            "that our model can pre- dict. Our model struggles with small objects\n",
            "that appear in groups, such as ﬂocks of birds. Since our model learns\n",
            "to predict bounding boxes from data, it struggles to generalize to\n",
            "objects in new or unusual aspect ratios or conﬁgurations. Our model\n",
            "also uses r...\n",
            "______________________________________________________________________\n",
            "Source Node 2/2\n",
            "Node ID: 3a29b960-00b6-4bd6-9993-36a7a2dc3919\n",
            "Similarity: 0.502251761223678\n",
            "Text: [33] Z. Shen and X. Xue. Do more dropouts in pool5 feature maps\n",
            "for better object detection. arXiv preprint arXiv:1409.6911 , 2014. 7\n",
            "[34] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov,\n",
            "D. Erhan, V . Vanhoucke, and A. Rabinovich. Going deeper with\n",
            "convolutions. CoRR , abs/1409.4842, 2014. 2 [35] J. R. Uijlings, K. E.\n",
            "van de Sand...\n",
            "Object detection is a core problem in computer vision that involves locating and classifying objects within images or videos. It is a fundamental task in many applications, such as autonomous driving, robotics, surveillance, and healthcare. Object detection can be broadly classified into two categories: instance-level detection and semantic segmentation. Instance-level detection involves identifying individual objects within an image, while semantic segmentation involves assigning a class label to each pixel in an image.\n",
            "\n",
            "Object detection pipelines typically start by extracting robust features from input images, followed by classifiers or localizers that identify objects in the feature space. These classifiers or localizers can be run in a sliding window fashion over the whole image or on some subset of regions in the image.\n",
            "\n",
            "Deformable parts models (DPM) and R-CNN are two popular object detection frameworks that use a sliding window approach to find objects in images. DPM uses a disjoint pipeline to extract static features, classify regions, predict bounding boxes for high-scoring regions, etc. R-CNN and its variants use region proposals instead of sliding windows to find objects in images.\n",
            "\n",
            "YOLO (You Only Look Once\n"
          ]
        }
      ]
    }
  ]
}
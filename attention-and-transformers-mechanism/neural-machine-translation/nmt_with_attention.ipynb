{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmt-with-attention.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNhphd+vuaSB3bxlwy3pu7a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-research-and-practice/blob/main/attention-and-transformers-mechanism/neural-machine-translation/nmt_with_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Neural Machine Translation With Attention Mechanism"
      ],
      "metadata": {
        "id": "6OU0hd_otqr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today, let’s join me in the journey of creating a neural machine translation model with attention mechanism by using the hottest-on-the-news Tensorflow 2.0.\n",
        "\n",
        "With that being said, our objective is pretty simple: we will use a very simple dataset (with only 20 examples) and we will try to overfit the training data with the renown Seq2Seq model. For the attention mechanism, we’re gonna use Luong attention, which I personally prefer over Bahdanau’s.\n",
        "\n",
        "Without talking too much about theories today, let’s jump right into the implementation. As usual, we will go through the steps below:\n",
        "\n",
        "* Data Preparation\n",
        "* Seq2Seq without Attention\n",
        "* Seq2Seq with Luong Attention\n",
        "\n",
        "\n",
        "Reference:\n",
        "\n",
        "[Neural Machine Translation With Attention Mechanism](https://blog.erico.vn/posts/neural-machine-translation-with-attention-mechanism)"
      ],
      "metadata": {
        "id": "hjNayxuotrcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "sbGZ_wfTuUJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re"
      ],
      "metadata": {
        "id": "ysWbq2sEuVAS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Preparation"
      ],
      "metadata": {
        "id": "OegM-1x8uYut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s talk about the data. We’re gonna use 20 English – French pairs (which I extracted from the original dataset)."
      ],
      "metadata": {
        "id": "pcYJzrYCuZb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = (\n",
        "    ('What a ridiculous concept!', 'Quel concept ridicule !'),\n",
        "    ('Your idea is not entirely crazy.', \"Votre idée n'est pas complètement folle.\"),\n",
        "    (\"A man's worth lies in what he is.\", \"La valeur d'un homme réside dans ce qu'il est.\"),\n",
        "    ('What he did is very wrong.', \"Ce qu'il a fait est très mal.\"),\n",
        "    (\"All three of you need to do that.\", \"Vous avez besoin de faire cela, tous les trois.\"),\n",
        "    (\"Are you giving me another chance?\", \"Me donnez-vous une autre chance ?\"),\n",
        "    (\"Both Tom and Mary work as models.\", \"Tom et Mary travaillent tous les deux comme mannequins.\"),\n",
        "    (\"Can I have a few minutes, please?\", \"Puis-je avoir quelques minutes, je vous prie ?\"),\n",
        "    (\"Could you close the door, please?\", \"Pourriez-vous fermer la porte, s'il vous plaît ?\"),\n",
        "    (\"Did you plant pumpkins this year?\", \"Cette année, avez-vous planté des citrouilles ?\"),\n",
        "    (\"Do you ever study in the library?\", \"Est-ce que vous étudiez à la bibliothèque des fois ?\"),\n",
        "    (\"Don't be deceived by appearances.\", \"Ne vous laissez pas abuser par les apparences.\"),\n",
        "    (\"Excuse me. Can you speak English?\", \"Je vous prie de m'excuser ! Savez-vous parler anglais ?\"),\n",
        "    (\"Few people know the true meaning.\", \"Peu de gens savent ce que cela veut réellement dire.\"),\n",
        "    (\"Germany produced many scientists.\", \"L'Allemagne a produit beaucoup de scientifiques.\"),\n",
        "    (\"Guess whose birthday it is today.\", \"Devine de qui c'est l'anniversaire, aujourd'hui !\"),\n",
        "    (\"He acted like he owned the place.\", \"Il s'est comporté comme s'il possédait l'endroit.\"),\n",
        "    (\"Honesty will pay in the long run.\", \"L'honnêteté paye à la longue.\"),\n",
        "    (\"How do we know this isn't a trap?\", \"Comment savez-vous qu'il ne s'agit pas d'un piège ?\"),\n",
        "    (\"I can't believe you're giving up.\", \"Je n'arrive pas à croire que vous abandonniez.\"),\n",
        ")"
      ],
      "metadata": {
        "id": "VAjkbdNKuksF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will need to clean up the raw data a little bit. This kind of task usually involves normalizing strings, filtering unwanted tokens, adding space before punctuation, etc."
      ],
      "metadata": {
        "id": "3j19XLadvi9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unicode_to_ascii(sent):\n",
        "  return \"\".join(char for char in unicodedata.normalize(\"NFD\", sent) if unicodedata.category(char) != \"Mn\")\n",
        "\n",
        "def normalize_string(sent):\n",
        "  sent = unicode_to_ascii(sent)\n",
        "  sent = re.sub(r\"([!.?])\", r\"\\1\", sent)\n",
        "  sent = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", sent)\n",
        "  sent = re.sub(r\"\\s+\", r\" \", sent)\n",
        "  return sent"
      ],
      "metadata": {
        "id": "t4ZOMdk3vj9k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now split the data into two separate lists, each containing its own sentences. \n",
        "\n",
        "Then we will apply the functions above and add two special tokens: `<start> and <end>`:"
      ],
      "metadata": {
        "id": "F7iYYR52wpz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_en, raw_data_fr = list(zip(*raw_data))\n",
        "raw_data_en, raw_data_fr = list(raw_data_en), list(raw_data_fr)\n",
        "\n",
        "raw_data_en = [normalize_string(data) for data in raw_data_en]\n",
        "\n",
        "raw_data_fr_in = [\"<start> \" + normalize_string(data) for data in raw_data_fr]\n",
        "raw_data_fr_out = [normalize_string(data) + \" <end>\" for data in raw_data_fr]"
      ],
      "metadata": {
        "id": "0RYBfqewwsX0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I need to elaborate a little bit here. First off, let’s take a look at the figure below:\n",
        "\n",
        "<img src='https://github.com/rahiakela/transformers-research-and-practice/blob/main/attention-and-transformers-mechanism/neural-machine-translation/images/input_roea0w.webp?raw=1' width='400'/>\n",
        "\n",
        "The Seq2Seq model consists of two networks: Encoder and Decoder. The encoder, which is on the left-hand side, requires only sequences from source language as inputs."
      ],
      "metadata": {
        "id": "edVTkI7RzylK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_en[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vdwgz27ryFym",
        "outputId": "56f08fee-6ced-45d9-b749-91fb8d781ca0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What a ridiculous concept!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder, on the other hand, requires two versions of the destination language’s sequences, one for inputs and one for targets (loss computation). The decoder itself is usually called a language model (we used it a lot for text generation, remember?)."
      ],
      "metadata": {
        "id": "DMPS6tbi1inr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_fr_in[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gvggmoGqyBVy",
        "outputId": "1b9d62d2-859e-4e40-e807-2acdcc829c85"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> Quel concept ridicule !'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_fr_out[0]"
      ],
      "metadata": {
        "id": "OvlC1qAo1n6C",
        "outputId": "5d4c9424-38ae-459f-e5e0-1cc351e26e73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Quel concept ridicule ! <end>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From personal experiments, I also found that it would be better not to add `<start>` and `<end>` tokens to source sequences. Doing so would confuse the model, especially the attention mechanism later on, since all sequences start with the same token.\n",
        "\n",
        "Next, let’s see how to tokenize the data, i.e. convert the raw strings into integer sequences. \n",
        "\n",
        "We’re gonna use the text tokenization utility class from Keras:\n",
        "\n"
      ],
      "metadata": {
        "id": "Id86U7_H1wGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=\"\")"
      ],
      "metadata": {
        "id": "ZvcCtyrC14Dd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, Keras’ `Tokenizer` will trim out all the punctuations, which is not what we want. Since we have already filtered out punctuations ourselves (except for `.!?`), we can just set filters as blank here.\n",
        "\n",
        "The crucial part of tokenization is vocabulary. Keras’ `Tokenizer` class comes with a few methods for that. Since our data contains raw strings, we will use the one called `fit_on_texts`."
      ],
      "metadata": {
        "id": "r7Hnn5IG2Iv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en_tokenizer.fit_on_texts(raw_data_en)"
      ],
      "metadata": {
        "id": "M-oAVrJI2YU1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer will created its own vocabulary as well as conversion dictionaries."
      ],
      "metadata": {
        "id": "jnlpCLim2ikW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(en_tokenizer.word_index)"
      ],
      "metadata": {
        "id": "IEP9yUWZ2jDX",
        "outputId": "a7defd34-e2b1-4667-e8cf-9e0a5a71b745",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you': 1, 'the': 2, 'a': 3, 'he': 4, 'what': 5, 'is': 6, 'in': 7, 'do': 8, 'can': 9, 't': 10, 'did': 11, 'giving': 12, 'i': 13, 'few': 14, 'please?': 15, 'this': 16, 'know': 17, 'ridiculous': 18, 'concept!': 19, 'your': 20, 'idea': 21, 'not': 22, 'entirely': 23, 'crazy.': 24, 'man': 25, 's': 26, 'worth': 27, 'lies': 28, 'is.': 29, 'very': 30, 'wrong.': 31, 'all': 32, 'three': 33, 'of': 34, 'need': 35, 'to': 36, 'that.': 37, 'are': 38, 'me': 39, 'another': 40, 'chance?': 41, 'both': 42, 'tom': 43, 'and': 44, 'mary': 45, 'work': 46, 'as': 47, 'models.': 48, 'have': 49, 'minutes': 50, 'could': 51, 'close': 52, 'door': 53, 'plant': 54, 'pumpkins': 55, 'year?': 56, 'ever': 57, 'study': 58, 'library?': 59, 'don': 60, 'be': 61, 'deceived': 62, 'by': 63, 'appearances.': 64, 'excuse': 65, 'me.': 66, 'speak': 67, 'english?': 68, 'people': 69, 'true': 70, 'meaning.': 71, 'germany': 72, 'produced': 73, 'many': 74, 'scientists.': 75, 'guess': 76, 'whose': 77, 'birthday': 78, 'it': 79, 'today.': 80, 'acted': 81, 'like': 82, 'owned': 83, 'place.': 84, 'honesty': 85, 'will': 86, 'pay': 87, 'long': 88, 'run.': 89, 'how': 90, 'we': 91, 'isn': 92, 'trap?': 93, 'believe': 94, 're': 95, 'up.': 96}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now have the raw English sentences converted to integer sequences:"
      ],
      "metadata": {
        "id": "KIGLF6qR2y8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_en = en_tokenizer.texts_to_sequences(raw_data_en)\n",
        "data_en[0]"
      ],
      "metadata": {
        "id": "Xf3zNj3q2zcE",
        "outputId": "4725955d-acde-46e4-8c09-3a05a5cdfef5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5, 3, 18, 19]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last but not least, we need to pad zeros so that all sequences have the same length. Otherwise, we won’t be able to create `tf.data.Dataset` object later on."
      ],
      "metadata": {
        "id": "Ua2mMW_O3KUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en, padding=\"post\")\n",
        "data_en[0]"
      ],
      "metadata": {
        "id": "mUHO3lU53MyC",
        "outputId": "3b4adbe8-3224-4842-9091-36f6a702d0dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 5,  3, 18, 19,  0,  0,  0,  0,  0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s check if everything is okay:"
      ],
      "metadata": {
        "id": "ARJY8TC03ek-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_en[:5]"
      ],
      "metadata": {
        "id": "7rCS8TQ53fBc",
        "outputId": "d4fd1417-1f79-4edb-ede9-3ead549e3b40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 5,  3, 18, 19,  0,  0,  0,  0,  0],\n",
              "       [20, 21,  6, 22, 23, 24,  0,  0,  0],\n",
              "       [ 3, 25, 26, 27, 28,  7,  5,  4, 29],\n",
              "       [ 5,  4, 11,  6, 30, 31,  0,  0,  0],\n",
              "       [32, 33, 34,  1, 35, 36,  8, 37,  0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything is perfect. \n",
        "\n",
        "Let's go ahead and do exactly the same with French sentences:"
      ],
      "metadata": {
        "id": "V42PK5Uc3oj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fr_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=\"\")\n",
        "\n",
        "# make vacabulary by converting to integer sequences\n",
        "fr_tokenizer.fit_on_texts(raw_data_fr_in)\n",
        "fr_tokenizer.fit_on_texts(raw_data_fr_out)\n",
        "\n",
        "# pad zeros so that all sequences have the same length\n",
        "data_fr_in = fr_tokenizer.texts_to_sequences(raw_data_fr_in)\n",
        "data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in, padding=\"post\")\n",
        "\n",
        "# do the same for target sequences\n",
        "data_fr_out = fr_tokenizer.texts_to_sequences(raw_data_fr_out)\n",
        "data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out, padding=\"post\")\n",
        "\n",
        "data_fr_in[:2]"
      ],
      "metadata": {
        "id": "GtmUDkTb3rT5",
        "outputId": "1e981d38-ba98-44cd-acf8-d7592c4d00a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2, 30, 31, 32, 15,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 2, 33, 34, 19,  6,  9, 35, 36,  0,  0,  0,  0,  0,  0]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_fr_out[:2]"
      ],
      "metadata": {
        "id": "fNdYIgVV5LOD",
        "outputId": "54f66d04-c43f-4098-8302-644b8c98265e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[30, 31, 32, 15,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [33, 34, 19,  6,  9, 35, 36,  3,  0,  0,  0,  0,  0,  0]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A mid-way notice though, we can call `fit_on_texts` multiple times on different corpora and it will update vocabulary automatically. Always remember to finish with `fit_on_texts` first before using `texts_to_sequences`.\n",
        "\n",
        "The last step is easy, we only need to create an instance of `tf.data.Dataset`:"
      ],
      "metadata": {
        "id": "5Z4KKpTx5o-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((data_en, data_fr_in, data_fr_out))\n",
        "dataset = dataset.shuffle(20).batch(5)"
      ],
      "metadata": {
        "id": "CAldgAv85wEd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And that’s it. We have done preparing the data!"
      ],
      "metadata": {
        "id": "cmhkbfbo6Rh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Seq2Seq model without Attention"
      ],
      "metadata": {
        "id": "UeJBQKGu6SIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By now, we probably know that attention mechanism is the new standard in machine translation tasks. But I think there are good reasons to create the vanilla `Seq2Seq` first:\n",
        "\n",
        "* Pretty simple and easy with `tf.keras`\n",
        "* No headache to debug when things go wrong\n",
        "* Be able to answer: Why need attention at all?\n",
        "\n",
        "Okay, let’s assume that you are all convinced. We will start off with the encoder. Inside the encoder, there are an embedding layer and an RNN layer (can be either vanilla RNN or LSTM, or GRU). \n",
        "\n",
        "At every forward pass, it takes in a batch of sequences and initial states and returns output sequences as well as final states:"
      ],
      "metadata": {
        "id": "NR3JlGqN6UVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_size, lstm_size):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.lstm_size = lstm_size \n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "    self.lstm = tf.keras.layers.LSTM(lstm_size, return_sequences=True, return_state=True)\n",
        "\n",
        "  def call(self, sequence, states):\n",
        "    embed = self.embedding(sequence)\n",
        "    output, state_hidden, state_context = self.lstm(embed, initial_state=states)\n",
        "    return output, state_hidden, state_context\n",
        "\n",
        "  def init_states(self, batch_size):\n",
        "    return (tf.zeros([batch_size, self.lstm_size]), tf.zeros([batch_size, self.lstm_size]))"
      ],
      "metadata": {
        "id": "laHjPKguaBLy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here is how the data’s shape changes at each layer. I find that keeping track of the data’s shape is extremely helpful not to make silly mistakes, just like stacking up Lego pieces:\n",
        "\n",
        "<img src='https://github.com/rahiakela/transformers-research-and-practice/blob/main/attention-and-transformers-mechanism/neural-machine-translation/images/data_shapes-1_l7luwu.webp?raw=1' width='600'/>\n",
        "\n",
        "We have done with the encoder. Next, let’s create the decoder. \n",
        "\n",
        "Without attention mechanism, the decoder is basically the same as the encoder, except that it has a Dense layer to map RNN’s outputs into vocabulary space:\n",
        "\n"
      ],
      "metadata": {
        "id": "YxgaBwLVc6cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_size, lstm_size):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.lstm_size = lstm_size \n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "    self.lstm = tf.keras.layers.LSTM(lstm_size, return_sequences=True, return_state=True)\n",
        "    self.dense = tf.keras.Dense(vocab_size)\n",
        "\n",
        "  def call(self, sequence, state):\n",
        "    embed = self.embedding(sequence)\n",
        "    lstm_out, state_hidden, state_context = self.lstm(embed, state)\n",
        "    logits = self.dense(lstm_out)\n",
        "    return logits, state_hidden, state_context"
      ],
      "metadata": {
        "id": "u70EHahVf45B"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, here’s the data’s shape at each layer:\n",
        "\n",
        "<img src='https://github.com/rahiakela/transformers-research-and-practice/blob/main/attention-and-transformers-mechanism/neural-machine-translation/images/data_shapes-2_w7unlz.webp?raw=1' width='600'/>\n",
        "\n",
        "As you might have noticed, the final states of the encoder will act as the initial states of the decoder. That’s the difference between a language model and a decoder of `Seq2Seq` model.\n",
        "\n",
        "And that is the decoder we need to create. \n",
        "\n",
        "Before moving on, let’s check if we didn’t make any mistake along the way:"
      ],
      "metadata": {
        "id": "o3uT1YCBiv0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "evrFK7Pyj0SU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
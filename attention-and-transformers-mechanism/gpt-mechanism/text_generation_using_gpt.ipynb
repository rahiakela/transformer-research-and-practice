{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-generation-using-gpt.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNV8RpUp0IK8ym0OqpOkEM6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-case-studies/blob/master/gpt-mechanism/text_generation_using_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c15Osm4ny2c"
      },
      "source": [
        "## Text generation using GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnGMDYZtnzK-"
      },
      "source": [
        "Transformers can be pretrained on large bodies of unlabeled data and\n",
        "then finetuned for other tasks. Two main groups of such pretrained models are:\n",
        "\n",
        "1. **Bidirectional Encoder Representations from Transformers (BERTs)**\n",
        "2. **Generative Pretrained Transformers (GPTs)**\n",
        "\n",
        "The first GPT model was introduced in a 2018 paper by Radford et al. from OpenAI – it demonstrated how a generative language model can acquire knowledge and process longrange dependencies thanks to pretraining on a large, diverse corpus of contiguous text. Two successor models (trained on more extensive corpora) were released in the following years: GPT-2 in 2019 (1.5 billion parameters) and GPT-3 in 2020 (175 billion parameters).\n",
        "\n",
        "We will be making use of the excellent Transformers library created by Hugging Face(https://huggingface.co/). It abstracts away several components of the building process, allowing us to focus on the model performance and intended performance.\n",
        "\n",
        "Reference:\n",
        "\n",
        "1. https://huggingface.co/blog/how-to-generate\n",
        "2. https://huggingface.co/transformers/model_doc/gpt2.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1_hJjr6ovDC"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZiLOM67pdXZ"
      },
      "source": [
        "# installing Transformers and TensorFlow 2.0 in one line\n",
        "!pip install transformers[tf-gpu]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sntym55rowLK"
      },
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B3_LDHkpA_B"
      },
      "source": [
        "One of the advantages of the Transformers library – and a reason for its popularity, undoubtedly – is how easily we can download a specific model (and also define the appropriate tokenizer):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCq8SlkJo2Gl"
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
        "GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id=tokenizer.eos_token_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNtNvW_YqjP1"
      },
      "source": [
        "It is usually a good idea to fix the random seed to ensure the results are reproducible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB0TiWosqZqJ"
      },
      "source": [
        "# settings\n",
        "\n",
        "# for reproducability\n",
        "SEED=34\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# maximum number of words in output text\n",
        "MAX_LEN = 70"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfz83P6Ksigt"
      },
      "source": [
        "## Different Decoding Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVTm0hAssNHA"
      },
      "source": [
        "Let us focus on the fact that how we decode is one of the most important decisions when using a GPT-2 model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viqlesHjsohI"
      },
      "source": [
        "### Greedy search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2dOd-bjspsz"
      },
      "source": [
        "With **greedy search**, the word with the highest probability is predicted as the next word in the sequence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxoRlBihsApD"
      },
      "source": [
        "input_sequence1 = \"I don't know about you, but there's only one thing I want to do after a long day of work\"\n",
        "input_sequence2 = \"There are times when I am really tired of people, but I feel lonely too.\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmyndH34s9Hq"
      },
      "source": [
        "Once we have our input sequence, we encode it and then call a decode method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8QvN71ls9vZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ff653ee-42d5-46c5-897a-261fdb9710d9"
      },
      "source": [
        "# encode context the generation is conditioned on\n",
        "input_ids = tokenizer.encode(input_sequence1, return_tensors=\"tf\")\n",
        "\n",
        "# generate text until the output length (which includes the context length) reaches 70\n",
        "greedy_output = GPT2.generate(input_ids, max_length=MAX_LEN)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I don't know about you, but there's only one thing I want to do after a long day of work: go to the gym.\n",
            "\n",
            "I'm not talking about the gym that's right next to my house. I'm talking about the gym that's right next to my house.\n",
            "\n",
            "I'm not talking about the gym that\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBbXMWCxVOqS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac622ef9-9a6f-48b0-eb4e-e5c6ca12a330"
      },
      "source": [
        "# encode context the generation is conditioned on\n",
        "input_ids = tokenizer.encode(input_sequence2, return_tensors=\"tf\")\n",
        "\n",
        "# generate text until the output length (which includes the context length) reaches 70\n",
        "greedy_output = GPT2.generate(input_ids, max_length=MAX_LEN)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "There are times when I am really tired of people, but I feel lonely too. I feel like I'm alone in the world. I feel like I'm alone in my own body. I feel like I'm alone in my own mind. I feel like I'm alone in my own heart. I feel like I'm alone in my own mind\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5KpY9O3t5XQ"
      },
      "source": [
        "As you can see, the results leave some room for improvement: the model starts repeating itself, because the high-probability words mask the less-likely ones so they cannot explore more diverse combinations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6-Jti5374Rx"
      },
      "source": [
        "### Beam search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63E3eJJn75Rr"
      },
      "source": [
        "A simple remedy is **beam search**: we keep track of the alternative variants, so that more comparisons are possible:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIswnzp7ttrQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "289677b2-d2e7-4f54-c32e-a2cb216f7cc9"
      },
      "source": [
        "# set return_num_sequences > 1\n",
        "beam_outputs = GPT2.generate(input_ids, max_length=MAX_LEN, num_beams=5, no_repeat_ngram_size=2, num_return_sequences=5, early_stopping=True)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "# now we have 5 output sequences\n",
        "for i, beam_output in enumerate(beam_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\"\n",
            "\n",
            "\"I feel like I can't do anything right now,\" she said. \"I'm so tired.\"\n",
            "1: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\"\n",
            "\n",
            "\"I feel like I can't do anything right now,\" she says. \"I'm so tired.\"\n",
            "2: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\"\n",
            "\n",
            "\"I feel like I can't do anything right now,\" she says. \"I'm not sure what I'm supposed to be doing with my life.\"\n",
            "3: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\"\n",
            "\n",
            "\"I feel like I can't do anything right now,\" she says. \"I'm not sure what I'm supposed to be doing.\"\n",
            "4: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\"\n",
            "\n",
            "\"I feel like I can't do anything right now,\" she says. \"I'm not sure what I'm supposed to be doing with my life, or if I even have a life.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YHPKwDu9cmR"
      },
      "source": [
        "This is definitely more diverse – the message is the same, but at least the formulations look a little different from a style point of view."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFOjsyxwasiG"
      },
      "source": [
        "## Sampling – Indeterministic Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH4Bq5wSawNq"
      },
      "source": [
        "we can explore sampling – indeterministic decoding. Instead of following a strict path to find the end text with the highest probability, we rather randomly pick the next word by its conditional probability distribution. \n",
        "\n",
        "This approach risks producing incoherent ramblings, so we make use of the temperature parameter, which affects the probability mass distribution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9djkZbOs8wTh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32c1ba1f-f507-4f81-df06-5802df4f7477"
      },
      "source": [
        "# use temperature to decrease the sensitivity to low probability candidates\n",
        "sample_output = GPT2.generate(input_ids, do_sample=True, max_length=MAX_LEN, top_k=0, temperature=0.2)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "There are times when I am really tired of people, but I feel lonely too. I feel like I'm alone in my own world. I feel like I'm alone in my own life. I feel like I'm alone in my own mind. I feel like I'm alone in my own heart. I feel like I'm alone in my own\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB7hr3n_cctR"
      },
      "source": [
        "What happens if we increase the temperature?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL5D5bS9cVk9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d702da72-960e-4390-ce46-38036840f498"
      },
      "source": [
        "# use temperature to decrease the sensitivity to low probability candidates\n",
        "sample_output = GPT2.generate(input_ids, do_sample=True, max_length=MAX_LEN, top_k=0, temperature=0.8)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "There are times when I am really tired of people, but I feel lonely too. I find it strange how the people around me seem to be always so nice. The only time I feel lonely is when I'm on the road. I can't be alone with my thoughts.\n",
            "\n",
            "What are some of your favourite things to do in the area\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SS5x1pzc021"
      },
      "source": [
        "This is getting more interesting, although it still feels a bit like a train of thought – which is perhaps to be expected, given the content of our prompt. Let's explore some more ways to tune the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB338GqIc4x4"
      },
      "source": [
        "### Top-K sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnoiMI34c6Ce"
      },
      "source": [
        "In **Top-K sampling**, the top k most likely next words are selected and the entire probability mass is shifted to these k words. So instead of increasing the chances of high-probability words occurring and decreasing the chances of low-probability words, we just remove lowprobability words altogether."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Amr6ezvZciWq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "959c1b99-22e9-4b91-ef7c-a5952a54f3f2"
      },
      "source": [
        "# sample from only top_k most likely words\n",
        "sample_output = GPT2.generate(input_ids, do_sample=True, max_length=MAX_LEN, top_k=50)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "There are times when I am really tired of people, but I feel lonely too. I go to a place where you can feel comfortable. It's a place where you can relax. But if you're so tired of going along with the rules, maybe I won't go. You know what? Maybe if I don't go, you won't\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLa8DI5gdhbk"
      },
      "source": [
        "This seems like a step in the right direction. Can we do better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwzOLWnkdh73"
      },
      "source": [
        "###Top-P sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODioxA3odkc1"
      },
      "source": [
        "**Top-P sampling** (also known as nucleus sampling) is similar to Top-K, but instead of choosing the top k most likely words, we choose the smallest set of words whose total probability is larger than p, and then the entire probability mass is shifted to the words in this set. \n",
        "\n",
        "The main difference here is that with Top-K sampling, the size of the set of words is static (obviously), whereas in Top-P sampling, the size of the set can change. \n",
        "\n",
        "To use this sampling method, we just set top_k = 0 and choose a top_p value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksKu7fz4eAvs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e91c6de4-c6d2-4b25-f6d6-7a575c9d8bc4"
      },
      "source": [
        "# sample only from 80% most likely words\n",
        "sample_output = GPT2.generate(input_ids, do_sample=True, max_length=MAX_LEN, top_k=0, top_p=0.8)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "There are times when I am really tired of people, but I feel lonely too. I feel like I should just be standing there, just sitting there. I know I'm not a danger to anybody. I just feel alone.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3etz7P40eTyN"
      },
      "source": [
        "We can combine both approaches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpONJXHveK_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3d869e9-254b-45a6-80f6-122f7c31902e"
      },
      "source": [
        "# combine both sampling techniques\n",
        "sample_outputs = GPT2.generate(input_ids, do_sample=True, max_length= 2* MAX_LEN, top_k=50, top_p=0.85, num_return_sequences=5)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "  print(\"\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: There are times when I am really tired of people, but I feel lonely too. I don't feel like I am being respected by my own country, which is why I am trying to change the government.\"\n",
            "\n",
            "In a recent video posted to YouTube, Mr. Jaleel, dressed in a suit and tie, talks about his life in Pakistan and his frustration at his treatment by the country's law enforcement agencies. He also describes how he met a young woman from California who helped him organize the protest in Washington.\n",
            "\n",
            "\"She was a journalist who worked with a television channel in Pakistan,\" Mr. Jaleel says in the video. \"She came to my home one day,\n",
            "\n",
            "1: There are times when I am really tired of people, but I feel lonely too. It's not that I don't like to be around other people, but it's just something I have to face sometimes.\n",
            "\n",
            "What is your favorite thing to eat?\n",
            "\n",
            "The most favorite thing I have eaten is chicken and waffles. But I love rice, soups, and even noodles. I also like to eat bread, but I like it a little bit less.\n",
            "\n",
            "What is your ideal day of eating?\n",
            "\n",
            "It varies every day. Sometimes I want to eat at home, because I'm in a house with my family. But then sometimes I just have to have some sort\n",
            "\n",
            "2: There are times when I am really tired of people, but I feel lonely too. I think that there is something in my heart that is trying to be a better person, but I don't know what that is.\"\n",
            "\n",
            "So what can be done?\n",
            "\n",
            "\"I want people to take the time to think about this,\" says Jorja, who lives in a small town outside of Boston.\n",
            "\n",
            "\"I'm not a religious person,\" she says. \"I just want people to stop judging me. People judge me for doing what I'm doing, and they are very judgmental, and I don't like it.\"\n",
            "\n",
            "This article was first published on The Conversation.\n",
            "\n",
            "3: There are times when I am really tired of people, but I feel lonely too.\n",
            "\n",
            "I want to be able to take good care of myself. I am going to be a very good person, even if I am lonely.\n",
            "\n",
            "I want to be able to take good care of my family, too. If I had a child with my wife, she would be my only daughter.\n",
            "\n",
            "I don't have the means, but I hope I will be able to do something good. I want to become good.\n",
            "\n",
            "I'm not bad at sports. I can play basketball and baseball, and even soccer.\n",
            "\n",
            "I don't have much money, but I want to\n",
            "\n",
            "4: There are times when I am really tired of people, but I feel lonely too. The only person I really love is my family. It's just that I'm not alone.\"\n",
            "\n",
            "-Juan, 24, a student\n",
            "\n",
            "A study from the European Economic Area, a free trade area between the EU and Iceland, showed that there are 2.3 million EU citizens living in Iceland. Another survey in 2014 showed that 1.3 million people in Iceland were employed.\n",
            "\n",
            "The government is committed to making Iceland a country where everyone can live and work.\n",
            "\n",
            "\"We are here to help, not to steal,\" said one of the people who drove up in a Volkswagen.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Dm_IynjIC4V"
      },
      "source": [
        "Clearly, the more-sophisticated method's settings can give us pretty impressive results.\n",
        "\n",
        "Let's explore this avenue more – we'll use the prompts taken from OpenAI's GPT-2 website, where they feed them to a full-sized GPT-2 model. This comparison will give us an idea of how well we are doing with a local (smaller) model compared to a full one that was used for the original\n",
        "demos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDzu1Pcze410",
        "outputId": "23d82211-73ea-4d79-956b-811bdc5fdd6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "MAX_LEN = 500\n",
        "\n",
        "prompt1 = \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, \\\\\n",
        "in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"\n",
        "\n",
        "input_ids = tokenizer.encode(prompt1, return_tensors=\"tf\")\n",
        "\n",
        "sample_outputs = GPT2.generate(input_ids, do_sample=True, max_length= MAX_LEN, top_k=50, top_p=0.85)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "  print(\"\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, \\in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
            "\n",
            "Advertisement\n",
            "\n",
            "\"We discovered that the unicorns are intelligent, talk perfectly English, and are very familiar with humans,\" said the study's lead researcher, Dr. Alan Stone, who is a professor at the University of New Mexico in Albuquerque. \"And we were astonished to discover that the animals can speak English.\"\n",
            "\n",
            "According to a report in Science, Stone said that when the scientists first looked for the unicorns, they saw that they were roaming a valley in the mountains near a small village. There, they spotted the animals, which were about the size of a cow, on a nearby rock.\n",
            "\n",
            "\"We thought the unicorns must be extinct,\" Stone said. \"We thought we'd never find them again.\"\n",
            "\n",
            "Advertisement\n",
            "\n",
            "In addition to English, the unicorns speak the language of the Chiricahua Apache people, which has been well-documented by the U.S. Geological Survey.\n",
            "\n",
            "\"This is a first-ever discovery of an animal speaking English in the Chiricahua Apache culture,\" Stone said. \"The fact that it's speaking English is a major discovery, and I hope it will inspire people to find more animals speaking English.\"\n",
            "\n",
            "Stone is not the first to find unicorns in the Andes Mountains. Last summer, an ecologist in Colombia also discovered a group of these adorable animals, only to have them die before their human caretakers could properly take them in.\n",
            "\n",
            "Advertisement\n",
            "\n",
            "\"We just found a group of them,\" said the ecologist, Cristian Márquez. \"They're very young. They're young enough that their mother was in the group for six years, and they are still alive.\"\n",
            "\n",
            "Márquez said that they will be studying the animals to see how intelligent they are.\n",
            "\n",
            "\"We will ask questions about how they communicate and their life cycle,\" he said. \"We'll find out if they are social and if they can understand human speech.\"\n",
            "\n",
            "Advertisement\n",
            "\n",
            "Stone said that they will also try to determine if the unicorns have any kind of natural enemies.\n",
            "\n",
            "\"If they have to fight, I'd like to know who,\" he said. \"We'll try\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia_3_hlPJHP6"
      },
      "source": [
        "In another example, it seems like the trepidations of the model authors were justified: GPT-2 can in fact generate fake news stories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UckKKHUSJJnL",
        "outputId": "c17c9cef-e71b-430d-ec02-88afe6e30043",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "prompt2 = \"Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.\"\n",
        "\n",
        "input_ids = tokenizer.encode(prompt2, return_tensors=\"tf\")\n",
        "\n",
        "sample_outputs = GPT2.generate(input_ids, \n",
        "                               do_sample=True, \n",
        "                               max_length= MAX_LEN, \n",
        "                               temperature=0.8,\n",
        "                               top_k=50, \n",
        "                               top_p=0.85,\n",
        "                               num_return_sequences=5)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "  print(\"\\n\" + 100 * '-')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.\n",
            "\n",
            "She was arrested for allegedly stealing $15,000 worth of clothes from the department store.\n",
            "\n",
            "Cyrus was caught on surveillance video leaving the store with $15,000 worth of clothes before walking back to her car, TMZ reported.\n",
            "\n",
            "Cyrus was wearing a black and white striped dress, black heels, a white T-shirt and black leggings.\n",
            "\n",
            "Cyrus was arrested on suspicion of shoplifting and is now being held at the Los Angeles County Jail.\n",
            "\n",
            "The singer was seen leaving the department store with a bag of clothes in her hand\n",
            "\n",
            "The star was spotted wearing a black and white striped dress, black heels, a white T-shirt and black leggings\n",
            "\n",
            "Cyrus was spotted leaving the store with a bag of clothes in her hand\n",
            "\n",
            "Cyrus was caught on surveillance video leaving the store with $15,000 worth of clothes before walking back to her car, TMZ reported\n",
            "\n",
            "Cyrus was seen wearing a black and white striped dress, black heels, a white T-shirt and black leggings.\n",
            "\n",
            "The singer was spotted leaving the department store with a bag of clothes in her hand.\n",
            "\n",
            "Cyrus was spotted leaving the store with a bag of clothes in her hand.\n",
            "\n",
            "The star was seen leaving the department store with a bag of clothes in her hand.\n",
            "\n",
            "Cyrus was seen leaving the department store with a bag of clothes in her hand.\n",
            "\n",
            "The star was spotted leaving the department store with a bag of clothes in her hand\n",
            "\n",
            "Cyrus was spotted leaving the department store with a bag of clothes in her hand\n",
            "\n",
            "Cyrus was spotted leaving the department store with a bag of clothes in her hand\n",
            "\n",
            "Cyrus was spotted leaving the department store with a bag of clothes in her hand\n",
            "\n",
            "Cyrus was spotted leaving the department store with a bag of clothes in her hand\n",
            "\n",
            "Cyrus was spotted leaving the department store with a bag of clothes in her hand\n",
            "\n",
            "Cyrus was spotted leaving the department store with a bag of clothes in her hand\n",
            "\n",
            "Cyrus was spotted leaving the department store with a bag of clothes in her hand\n",
            "\n",
            "Cyrus was spotted leaving the department store with a bag of clothes in her hand\n",
            "\n",
            "Cyrus was spotted leaving the department store with a bag of clothes in her hand\n",
            "\n",
            "Cyrus was spotted\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.\n",
            "\n",
            "The pop star was spotted walking down the street wearing a grey hoodie and a blue top.\n",
            "\n",
            "She is not wearing any shoes and was wearing a black bag.\n",
            "\n",
            "The 19-year-old was in the mall for a birthday party with friends.\n",
            "\n",
            "She was also spotted at a nearby store.\n",
            "\n",
            "A spokesman for the mall said: \"Miley Cyrus was in the mall with friends and did not have any shoes on.\n",
            "\n",
            "\"She is wearing a black bag.\"\n",
            "\n",
            "Abercrombie & Fitch has not responded to a request for comment.\n",
            "\n",
            "A spokeswoman for the star confirmed she was in the store at the time of the incident.\n",
            "\n",
            "\"Miley was in the store at the time,\" she told the Daily Mail.\n",
            "\n",
            "\"She was not in any other clothes and was not wearing shoes.\"\n",
            "\n",
            "Miley was also spotted at a nearby store.\n",
            "\n",
            "The star was spotted at the shop in the early hours of today.\n",
            "\n",
            "A spokesman for the store told The Mirror: \"Miley was in the store at the time. She was not in any other clothes and was not wearing shoes.\"\n",
            "\n",
            "The pop star was not wearing any shoes and was wearing a black bag.\n",
            "\n",
            "A spokesman for the store said: \"Miley was in the store at the time. She was not in any other clothes and was not wearing shoes.\"\n",
            "\n",
            "Abercrombie & Fitch has not responded to a request for comment.\n",
            "\n",
            "This is not the first time Miley has been caught shoplifting.\n",
            "\n",
            "She was also caught shoplifting from Abercrombie & Fitch on Hollywood Boulevard in 2014.\n",
            "\n",
            "Miley was spotted at the store in the early hours of today.\n",
            "\n",
            "Abercrombie & Fitch has not responded to a request for comment.\n",
            "\n",
            "Miley Cyrus has been spotted shopping in the Abercrombie & Fitch store in Hollywood on Friday morning.\n",
            "\n",
            "The pop star was spotted shopping in the Abercrombie & Fitch store in Hollywood on Friday morning.\n",
            "\n",
            "The pop star was spotted shopping in the Abercrombie & Fitch store in Hollywood on Friday morning.\n",
            "\n",
            "Miley Cyrus was spotted shopping in the Abercrombie & Fitch store in Hollywood on Friday morning.\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "2: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.\n",
            "\n",
            "The 25-year-old singer was seen wearing a purple T-shirt and black shorts.\n",
            "\n",
            "Cyrus was spotted leaving the store with the cash register and a bag full of clothing, reports the Daily Mail.\n",
            "\n",
            "Scroll down for video\n",
            "\n",
            "Hair in a twist: Miley Cyrus was spotted leaving the Abercrombie & Fitch store in Hollywood, California with a bag of clothing on today\n",
            "\n",
            "The star was spotted leaving the store with the cash register and a bag full of clothing, reports the Daily Mail.\n",
            "\n",
            "It was later confirmed that the singer was the shoplifter.\n",
            "\n",
            "The singer was spotted leaving the store with the cash register and a bag full of clothing, reports the Daily Mail.\n",
            "\n",
            "The store manager reportedly told her to go to her car and she returned a short time later.\n",
            "\n",
            "The star was seen leaving the store with the cash register and a bag full of clothing, reports the Daily Mail. It was later confirmed that the singer was the shoplifter\n",
            "\n",
            "Abercrombie & Fitch released a statement saying: 'We are deeply sorry to hear about this unfortunate incident.\n",
            "\n",
            "'We are cooperating with the authorities.'\n",
            "\n",
            "Abercrombie & Fitch released a statement saying: 'We are deeply sorry to hear about this unfortunate incident. We are cooperating with the authorities'\n",
            "\n",
            "It is the second time in a month that Cyrus has been caught shoplifting.\n",
            "\n",
            "The star was seen leaving the store with the cash register and a bag full of clothing, reports the Daily Mail. It was later confirmed that the singer was the shoplifter\n",
            "\n",
            "Earlier in the month, the star was seen leaving the store with the cash register and a bag full of clothing, reports the Daily Mail.\n",
            "\n",
            "The shoplifting scandal was a huge blow to the clothing retailer, who were already struggling with a drop in sales.\n",
            "\n",
            "The company said in a statement: 'We are deeply sorry to hear about this unfortunate incident. We are cooperating with the authorities.\n",
            "\n",
            "'We are disappointed to learn that this individual has violated our company's code of conduct, and we are working with the store to ensure this individual is no longer a member of our team.'\n",
            "\n",
            "The incident comes just two weeks after Cyrus was caught shoplifting at the Victoria's Secret store in Los\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "3: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.\n",
            "\n",
            "The 20-year-old singer was caught by a security guard in a black top and black jeans.\n",
            "\n",
            "Miley was wearing a black shirt, black trousers and black shoes.\n",
            "\n",
            "It is not clear if she was arrested, or whether she will face charges.\n",
            "\n",
            "Miley, who has a net worth of $10 million, has been on the front lines of her battle with addiction.\n",
            "\n",
            "She was arrested in 2011 for allegedly stealing a pair of designer boots.\n",
            "\n",
            "In a Facebook post, she wrote: \"It's a small world after all. I'll always have a place for my shoes.\n",
            "\n",
            "\"If I ever get in trouble again, I will get it all back. I don't care about the label, I care about my shoes.\"\n",
            "\n",
            "Her latest arrest comes after she was accused of stealing an Abercrombie & Fitch bag from a Walmart store in Los Angeles in July.\n",
            "\n",
            "Miley also posted a picture of herself on Instagram with the caption: \"This is what it's like when your parents don't approve of you.\"\n",
            "\n",
            "Last week, Miley's father, Joel, released a statement to PEOPLE about his daughter's arrest.\n",
            "\n",
            "He said: \"Miley is not guilty. She is not guilty of any crime. She has not stolen anything.\n",
            "\n",
            "\"She has never taken drugs. She is innocent. She is an amazing girl and she has never harmed anyone.\n",
            "\n",
            "\"She has been through so much, and she has come out on top.\"\n",
            "\n",
            "Miley's mother, Kimberly, has also spoken out about her daughter's arrest.\n",
            "\n",
            "She said: \"My heart is broken. I can't believe it.\n",
            "\n",
            "\"My daughter is a wonderful girl. She's very smart and very talented. She's a good kid. She's not the person that she's made out to be. She's just a kid.\"\n",
            "\n",
            "In July, Miley was caught shoplifting from Abercrombie & Fitch. She was caught in a black top and black jeans.\n",
            "\n",
            "The pop star's fans have been quick to respond to her arrest, with some calling for her to be jailed.\n",
            "\n",
            "One fan wrote: \"Miley Cyrus is a bad person and she deserves to be punished.\n",
            "\n",
            "\"We're not the ones\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "4: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.\n",
            "\n",
            "The singer, 19, was spotted with a bag full of clothes that included a black T-shirt, blue jeans and black Nike sneakers, according to TMZ.\n",
            "\n",
            "Scroll down for video\n",
            "\n",
            "Dumped: Miley Cyrus was spotted shoplifting from Abercrombie & Fitch on Hollywood Boulevard today\n",
            "\n",
            "The teen, who is best known for her hit song 'Wrecking Ball' and is the daughter of music mogul E! Entertainment CEO Mike Segar, was spotted with the bags of clothes.\n",
            "\n",
            "The store released a statement to TMZ saying that the incident was 'inappropriate'.\n",
            "\n",
            "'We are aware of the incident and are investigating,' the statement said.\n",
            "\n",
            "'We are committed to a safe and secure environment for our guests and staff, and we are currently looking into the matter.'\n",
            "\n",
            "The singer, who is best known for her hit song 'Wrecking Ball' and is the daughter of music mogul E! Entertainment CEO Mike Segar, was spotted with the bags of clothes\n",
            "\n",
            "The store released a statement to TMZ saying that the incident was 'inappropriate'\n",
            "\n",
            "The incident comes after Miley's boyfriend, former Disney star Ryan Reynolds, was caught shoplifting from a Macy's store in New York City earlier this month.\n",
            "\n",
            "The actor was spotted with a bag full of clothes, which included a black T-shirt, black jeans and black Nike sneakers.\n",
            "\n",
            "He was seen walking out of the store with his bag after he failed to pay for the items.\n",
            "\n",
            "The incident comes after Miley's boyfriend, former Disney star Ryan Reynolds, was caught shoplifting from a Macy's store in New York City earlier this month\n",
            "\n",
            "The actor was spotted with a bag full of clothes, which included a black T-shirt, black jeans and black Nike sneakers\n",
            "\n",
            "He was seen walking out of the store with his bag after he failed to pay for the items\n",
            "\n",
            "He was later caught on video by a store employee, who saw him walking out with his bag.\n",
            "\n",
            "Reynolds, 22, was caught on video by a store employee, who saw him walking out with his bag\n",
            "\n",
            "The star was later caught on video by a store employee, who saw him walking out with his bag\n",
            "\n",
            "The star was caught on video by a store employee, who saw him walking out with\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTwBFXqhMERs"
      },
      "source": [
        "What about riffing off literature classics like Tolkien?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL5_mhj-ME_i",
        "outputId": "25315613-ffb9-40a7-d31d-cf685d457617",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "prompt3 = \"Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry\"\n",
        "\n",
        "input_ids = tokenizer.encode(prompt3, return_tensors=\"tf\")\n",
        "\n",
        "sample_outputs = GPT2.generate(input_ids, \n",
        "                               do_sample=True, \n",
        "                               max_length= MAX_LEN, \n",
        "                               temperature=0.8,\n",
        "                               top_k=50, \n",
        "                               top_p=0.85,\n",
        "                               num_return_sequences=5)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "  print(\"\\n\" + 100 * '-')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry and firing a volley of arrows at them.\n",
            "\n",
            "Gimli charged at an orc, but was held back by the other warriors, who used their weapons to block Gimli's attacks. The two orcs were thrown back, and Gimli was stabbed in the back by a dagger. The orc quickly recovered and stabbed Gimli again. The orc staggered back, but the two men continued their attack, with Gimli falling to the ground and Gimli's blade impaled on the orc's shoulder.\n",
            "\n",
            "Gimli tried to fight back, but he was no match for the orc's strength and speed. The orc, however, was no match for the two men's courage and skill, and they began to flee. The orc and Gimli ran across the battlefield, and the two orcs were chased by a number of soldiers, who had arrived to try to stop them.\n",
            "\n",
            "Gimli's sword slashed the orc's shoulder, and the orc fell to the ground, bleeding profusely. The orc's arm was severed, and he was bleeding from his mouth. The orcs continued to run, and one of them was caught by a soldier and fell to the ground, bleeding from his side.\n",
            "\n",
            "Gimli ran over to him, and lifted him up and placed him in a sling. He then grabbed Gimli's sword and stabbed the orc in the chest. He stabbed him again, and the orc collapsed. The orcs ran away, and the soldier who had caught the orc was able to throw him out of the way of an arrow.\n",
            "\n",
            "The orc and Gimli then ran down the hill towards the woods.\n",
            "\n",
            "\"There you go!\" exclaimed the Orc as he looked over his shoulder. \"You're going to die!\"\n",
            "\n",
            "Gimli turned to look at the orc. \"I'm going to kill you!\"\n",
            "\n",
            "The Orc was silent for a moment, then he looked up.\n",
            "\n",
            "\"You're going to die!\" he repeated.\n",
            "\n",
            "Gimli looked over his shoulder and saw that the Orc had made a sharp turn and was running towards the woods.\n",
            "\n",
            "\"I'm going to kill you!\" he repeated, but the Orc didn't move.\n",
            "\n",
            "Gimli then looked back at the Orc. \"I'm going to kill you!\"\n",
            "\n",
            "The Orc turned around, and he looked over his\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry. A short distance away, the enemy's archers were falling back. A few of the archers had fallen, but many were still standing.\n",
            "\n",
            "Gimli saw a group of orcs that had come down from the mountains. They were carrying a great stone warhammer. The orcs were charging.\n",
            "\n",
            "Gimli pulled out his bow. He aimed at the nearest of the orcs, who was a tall, lanky man. The orc was quickly slain.\n",
            "\n",
            "Gimli shot another arrow at the orc. The arrow went wide of the orc's head, and bounced off the wall of stone. The orc fell dead.\n",
            "\n",
            "Gimli's bow flew into the air. It landed in the hands of another orc. The orc shot it with his bow, and it went wide of the orc's head, and bounced off the wall of stone.\n",
            "\n",
            "The orcs charged.\n",
            "\n",
            "Gimli shot an arrow at one of the orcs. It went wide of the orc's head. The orc fell dead.\n",
            "\n",
            "The orcs were charging.\n",
            "\n",
            "Gimli shot an arrow at one of them. It went wide of the orc's head. The orc fell dead.\n",
            "\n",
            "Gimli shot another arrow at one of the orcs. It went wide of the orc's head. The orc fell dead.\n",
            "\n",
            "Gimli shot another arrow at one of the orcs. It went wide of the orc's head. The orc fell dead.\n",
            "\n",
            "Gimli shot another arrow at one of the orcs. It went wide of the orc's head. The orc fell dead.\n",
            "\n",
            "Gimli shot an arrow at one of the orcs. It went wide of the orc's head. The orc fell dead.\n",
            "\n",
            "Gimli shot an arrow at one of the orcs. It went wide of the orc's head. The orc fell dead.\n",
            "\n",
            "Gimli shot an arrow at one of the orcs. It went wide of the orc's head. The orc fell dead.\n",
            "\n",
            "Gimli shot an arrow at one of the orcs. It went wide of the orc's head. The orc fell dead.\n",
            "\n",
            "Gimli shot an arrow at one of the orcs. It went wide of the orc's head. The orc fell dead.\n",
            "\n",
            "Gimli shot an arrow at one of the\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "2: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry as they marched toward the gates.\n",
            "\n",
            "\"No! No!\" shouted the guardsmen, their eyes wide with fear and confusion.\n",
            "\n",
            "\"No! No! No!\" screamed Gimli, his voice growing louder.\n",
            "\n",
            "\"I will not let you harm the peace of the world!\" shouted Aragorn.\n",
            "\n",
            "\"I am the king, and you are the orcs!\" shouted Aragorn, but his voice was no longer loud enough to be heard over the shouting.\n",
            "\n",
            "\"I will not let you harm the peace of the world!\" screamed Gimli, but his voice was no longer loud enough to be heard over the shouting.\n",
            "\n",
            "\"I will not let you harm the peace of the world!\" shouted Aragorn, but his voice was no longer loud enough to be heard over the shouting.\n",
            "\n",
            "The battle raged on for hours. Gimli's men fought bravely, but they were not in a position to break through the gates. Aragorn, meanwhile, was fighting off the orcs with his own men.\n",
            "\n",
            "\"I will not let you harm the peace of the world!\" screamed Aragorn, but his voice was no longer loud enough to be heard over the shouting.\n",
            "\n",
            "\"I will not let you harm the peace of the world!\" shouted Aragorn, but his voice was no longer loud enough to be heard over the shouting.\n",
            "\n",
            "Gimli's men were losing the battle, but they were not out of the woods yet.\n",
            "\n",
            "\"No! No! No!\" screamed Aragorn, his voice growing louder.\n",
            "\n",
            "\"I will not let you harm the peace of the world!\" screamed Aragorn, but his voice was no longer loud enough to be heard over the shouting.\n",
            "\n",
            "\"I will not let you harm the peace of the world!\" screamed Aragorn, but his voice was no longer loud enough to be heard over the shouting.\n",
            "\n",
            "Aragorn had made his decision. He had fought his way through the forest and was now in the city gates. He would not let the orcs harm the peace of the world.\n",
            "\n",
            "\"I will not let you harm the peace of the world!\" screamed Aragorn, but his voice was no longer loud enough to be heard over the shouting.\n",
            "\n",
            "The battle raged on for hours. Gimli's men\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "3: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry. But the orcs' strength was not enough to repel them, and the two heroes were cut down by their own comrades.\n",
            "\n",
            "The orcs were victorious, and the three hobbits retreated back to the Lonely Mountain, where they were reunited with the Fellowship. They all celebrated, and Merry and Pippin returned to the Shire.\n",
            "\n",
            "edit] Return to the Shire\n",
            "\n",
            "edit] The Return to the Shire\n",
            "\n",
            "The Fellowship returned to the Shire, and Merry and Pippin were reunited with Bilbo. However, the Shire was in chaos; many had been killed, and the hobbits were outnumbered by the orcs. At the last moment, Gandalf arrived and the Hobbits and the Dwarves were all welcomed into the Shire, and the battle began anew.\n",
            "\n",
            "edit] The Battle of the Five Armies\n",
            "\n",
            "The Hobbits fought bravely against the orcs, but the Battle of the Five Armies was lost to the combined might of the orcs and the dwarves. The hobbits returned to the Shire, and Bilbo and the dwarves went off to the White Mountains to look for their brothers and sisters.\n",
            "\n",
            "edit] The Battle of the Five Armies\n",
            "\n",
            "Bilbo and the dwarves returned to the Shire and took up their old habit of drinking, while the hobbits returned to the Shire and went to bed.\n",
            "\n",
            "The next day, however, the Fellowship was attacked by a large army of orcs led by a mighty orc warlord named Orcslayer. The hobbits fought bravely against the orcs, but they were outnumbered and the battle was lost. The hobbits returned to the Shire and Bilbo and the dwarves went off to the White Mountains to look for their brothers and sisters.\n",
            "\n",
            "edit] The Battle of the Five Armies\n",
            "\n",
            "Bilbo and the dwarves returned to the Shire and went to bed.\n",
            "\n",
            "The next day, however, the Fellowship was attacked by a large army of orcs led by a mighty orc warlord named Orcslayer. The hobbits fought bravely against the orcs, but they were outnumbered and the battle was lost. The hobbits returned to the Shire and Bilbo and the dwarves went off to the White Mountains to look for their brothers and sisters.\n",
            "\n",
            "edit] The Battle of the Five Armies\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "4: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.\n",
            "\n",
            "\"This is no time to lose! Fire at will! Fire!\" shouted the two.\n",
            "\n",
            "The orcs raised their weapons again, but these were not the same ones they had last used.\n",
            "\n",
            "The orcs had learned to use these weapons in battle, and to use them well. They had learned to use them like a sword and shield, and to use them with the precision of a bow. They had learned to use them as well as a bow, and to use them with the precision of a spear. They had learned to use them as well as a longbow, and to use them with the precision of a longbow. They had learned to use them with the precision of a crossbow, and to use them with the precision of a crossbow. They had learned to use them as well as a crossbow, and to use them with the precision of a crossbow. They had learned to use them with the precision of a dart gun, and to use them with the precision of a dart gun. They had learned to use them with the precision of a javelin, and to use them with the precision of a javelin. They had learned to use them as well as a javelin, and to use them with the precision of a javelin. They had learned to use them as well as a dart gun, and to use them with the precision of a dart gun.\n",
            "\n",
            "\"Fire at will!\" cried the orcs.\n",
            "\n",
            "Gimli and Legolas raised their weapons as well.\n",
            "\n",
            "\"Fire!\"\n",
            "\n",
            "The orcs raised their weapons as well.\n",
            "\n",
            "\"Fire!\"\n",
            "\n",
            "They raised their weapons as well.\n",
            "\n",
            "\"Fire!\"\n",
            "\n",
            "They raised their weapons as well.\n",
            "\n",
            "\"Fire!\"\n",
            "\n",
            "The orcs raised their weapons as well.\n",
            "\n",
            "\"Fire!\"\n",
            "\n",
            "The orcs raised their weapons as well.\n",
            "\n",
            "\"Fire!\"\n",
            "\n",
            "The orcs raised their weapons as well.\n",
            "\n",
            "\"Fire!\"\n",
            "\n",
            "The orcs raised their weapons as well.\n",
            "\n",
            "\"Fire!\"\n",
            "\n",
            "The orcs raised their weapons as well.\n",
            "\n",
            "\"Fire!\"\n",
            "\n",
            "The orcs raised their weapons as well.\n",
            "\n",
            "\"Fire!\"\n",
            "\n",
            "The orcs raised their weapons as well.\n",
            "\n",
            "\"Fire!\"\n",
            "\n",
            "The orcs raised their weapons as well.\n",
            "\n",
            "\"Fire!\"\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb8j3RrjM_d-"
      },
      "source": [
        "As you can see from the examples above, a GPT-2 model working out of the box (without finetuning) can already generate plausible-looking long-form text. Assessing the future impact of this technology on the field of communication remains an open and highly controversial issue: on the one hand, there is fully justified fear of fake news proliferation (see the Miley Cyrus story above). This is particularly concerning because large-scale automated detection of generated text is an extremely challenging topic. \n",
        "\n",
        "On the other hand, GPT-2 text generation capabilities can be helpful for creative types: be it style experimentation or parody, an AIpowered\n",
        "writing assistant can be a tremendous help."
      ]
    }
  ]
}
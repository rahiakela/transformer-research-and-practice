{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt-from-scratch.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-research-and-practice/blob/main/attention-and-transformers-mechanism/gpt-mechanism/gpt_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3HY1410XCSZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "\n",
        "class MaskedCausalAttention(nn.Module):\n",
        "  def __init__(self, h_dim, max_T, n_heads, drop_p):\n",
        "    super().__init__()\n",
        "\n",
        "    self.n_heads = n_heads\n",
        "    self.max_T = max_T\n",
        "\n",
        "    self.q_net = nn.Linear(h_dim, h_dim) \n",
        "    self.k_net = nn.Linear(h_dim, h_dim) \n",
        "    self.v_net = nn.Linear(h_dim, h_dim)\n",
        "\n",
        "    self.proj_net = nn.Linear(h_dim, h_dim)\n",
        "\n",
        "    self.att_drop = nn.Dropout(drop_p)\n",
        "    self.proj_drop = nn.Dropout(drop_p)\n",
        "\n",
        "    ones = torch.ones((max_T, max_T))\n",
        "    mask = torch.tril(ones).view(1, 1, max_T, max_T)\n",
        "\n",
        "    # register buffer makes sure mask does not get updated\n",
        "    # during backpropagation\n",
        "    self.register_buffer('mask',mask)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape # batch size, seq length, h_dim * n_heads\n",
        "\n",
        "    N, D = self.n_heads, C // self.n_heads # N = num heads, D = attention dim\n",
        "\n",
        "    # rearrange q, k, v as (B, N, T, D)\n",
        "    q = self.q_net(x).view(B, T, N, D).transpose(1,2) \n",
        "    k = self.k_net(x).view(B, T, N, D).transpose(1,2)\n",
        "    v = self.v_net(x).view(B, T, N, D).transpose(1,2)\n",
        "\n",
        "    # weights (B, N, T, T)\n",
        "    weights = q @ k.transpose(2,3) / math.sqrt(D)\n",
        "    # causal mask applied to weights \n",
        "    weights.masked_fill(self.mask[...,:T,:T] == 0, float('-inf'))\n",
        "    # normalize weights, all -inf -> 0 after softmax\n",
        "    normalized_weights = F.softmax(weights, dim=-1)\n",
        "\n",
        "    # attention (B, N, T, D)\n",
        "    attention = self.att_drop(normalized_weights @ v)\n",
        "\n",
        "    # gather heads and project (B, N, T, D) -> (B, T, N*D)\n",
        "    attention = attention.transpose(1, 2).contiguous().view(B,T,N*D)\n",
        "\n",
        "    out = self.proj_drop(self.proj_net(attention))\n",
        "    return out\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "  def __init__(self, h_dim, max_T, n_heads, drop_p):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attention = MaskedCausalAttention(h_dim, max_T, n_heads, drop_p)\n",
        "\n",
        "    self.mlp = nn.Sequential(\n",
        "            nn.Linear(h_dim, 4*h_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4*h_dim, h_dim),\n",
        "            nn.Dropout(drop_p),\n",
        "        )\n",
        "\n",
        "    self.ln1 = nn.LayerNorm(h_dim)\n",
        "    self.ln2 = nn.LayerNorm(h_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Attention -> LayerNorm -> MLP -> LayerNorm\n",
        "\n",
        "    x = x + self.attention(x) # residual\n",
        "    x = self.ln1(x) \n",
        "    x = x + self.mlp(x) # residual\n",
        "    x = self.ln2(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "  def __init__(self, token_dim, n_blocks, h_dim, max_T, n_heads, drop_p):\n",
        "    super().__init__()\n",
        "    \n",
        "    # embed input tokens and positions\n",
        "    self.proj_token = nn.Embedding(token_dim, h_dim)\n",
        "    # parameter = trainable weight matrix \n",
        "    init_param_vals = torch.randn(1, max_T, h_dim) / math.sqrt(h_dim)\n",
        "    self.position_embedding = nn.Parameter(init_param_vals)\n",
        "    self.dropout = nn.Dropout(drop_p)\n",
        "\n",
        "    # transformer blocks\n",
        "    blocks = [Block(h_dim, max_T, n_heads, drop_p) for _ in range(n_blocks)]\n",
        "    self.transformer = nn.Sequential(*blocks)\n",
        "\n",
        "    # projection head\n",
        "    self.ln = nn.LayerNorm(h_dim)\n",
        "    self.proj_head = nn.Linear(h_dim, token_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T = x.shape \n",
        "\n",
        "    # token and pos embedding\n",
        "    token_h = self.proj_token(x)\n",
        "    pos_h = self.position_embedding[:, :T, :]\n",
        "    h = token_h + pos_h\n",
        "\n",
        "    # transformer and prediction\n",
        "    h = self.ln(self.transformer(h))\n",
        "    pred = self.proj_head(h)\n",
        "\n",
        "    return pred \n",
        "\n",
        "  def pred_loss(self, pred, target):\n",
        "    # pred (B, T, C)  and target (B, T) \n",
        "    B, T, C = pred.shape\n",
        "    return F.cross_entropy(pred.view(B*T, C), target.view(B*T))\n"
      ]
    }
  ]
}
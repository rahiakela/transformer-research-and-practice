{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-bow-implementation.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPx+0sNs4cHIrvdKedFeMGb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformer-research-and-practice/blob/main/mastering-transformers/01-from-bag-of-words-to-transformer/1_bow_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Yp_vtV4LqYI"
      },
      "source": [
        "## BoW implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68q67XAdLrJc"
      },
      "source": [
        "A BoW is a representation technique for documents by counting the words in them.\n",
        "The main data structure of the technique is a document-term matrix.\n",
        "\n",
        "Let's see a simple implementation of BoW with Python. The following piece of code illustrates how to build a document-term matrix with the Python sklearn library for a toy corpus of three sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy5X6IIVL4He"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCTrN4FZMuXw"
      },
      "source": [
        "toy_corpus = [\n",
        "   \"the fat cat sat on the mat\",\n",
        "   \"the big cat slept\",\n",
        "   \"the dog chased a cat\"           \n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woQg8ZitM8rk",
        "outputId": "136b2d0e-ca27-47c7-c545-0c440a7cdbc6"
      },
      "source": [
        "corpus_tfidf = vectorizer.fit_transform(toy_corpus)\n",
        "\n",
        "print(f\"The vocabulary size is {len(vectorizer.vocabulary_.keys())}\")\n",
        "print(f\"The document-term matrix shape is {corpus_tfidf.shape}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary size is 10\n",
            "The document-term matrix shape is (3, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKkGzcllOKmO"
      },
      "source": [
        "The size is `(3 x 10)`, but in a realistic scenario the matrix size can grow to much larger numbers such as `10K x 10M`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "gFUobu4bN7v1",
        "outputId": "abc97299-8855-4b0a-dc73-520f2f8601e7"
      },
      "source": [
        "df = pd.DataFrame(np.round(corpus_tfidf.toarray(), 2))\n",
        "df.columns = vectorizer.get_feature_names()\n",
        "\n",
        "df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>big</th>\n",
              "      <th>cat</th>\n",
              "      <th>chased</th>\n",
              "      <th>dog</th>\n",
              "      <th>fat</th>\n",
              "      <th>mat</th>\n",
              "      <th>on</th>\n",
              "      <th>sat</th>\n",
              "      <th>slept</th>\n",
              "      <th>the</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.61</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    big   cat  chased   dog   fat   mat    on   sat  slept   the\n",
              "0  0.00  0.25    0.00  0.00  0.42  0.42  0.42  0.42   0.00  0.49\n",
              "1  0.61  0.36    0.00  0.00  0.00  0.00  0.00  0.00   0.61  0.36\n",
              "2  0.00  0.36    0.61  0.61  0.00  0.00  0.00  0.00   0.00  0.36"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak0qRrrnOvIe"
      },
      "source": [
        "The table indicates a count-based mathematical matrix where the cell values are\n",
        "transformed by a Term Frequency-Inverse Document Frequency (TF-IDF) weighting\n",
        "schema. **This approach does not care about the position of words**. \n",
        "\n",
        "**Since the word order strongly determines the meaning, ignoring it leads to a loss of meaning. This is a common problem in a BoW method, which is finally solved by a recursion mechanism in RNN and positional encoding in Transformers.**\n",
        "\n",
        "Each column in the matrix stands for the vector of a word in the vocabulary, and each row stands for the vector of a document.\n",
        "\n",
        "Semantic similarity metrics can be applied\n",
        "to compute the similarity or dissimilarity of the words as well as documents.\n",
        "\n",
        "\n",
        "Most of the time, we use bigrams such as cat_sat and the_street to enrich the document\n",
        "representation. For instance, as the parameter `ngram_range=(1,2)` is passed to\n",
        "TfidfVectorizer, it builds a vector space containing both unigrams `(big, cat,\n",
        "dog)` and bigrams `(big_cat, big_dog)`.\n",
        "\n",
        " Thus, such models are also called bag-of-ngrams, which is a natural extension of BoW."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vx8XahcYPqds",
        "outputId": "6bc08b64-1d51-4c7b-d92c-49b6e1ac963d"
      },
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "\n",
        "corpus_tfidf = vectorizer.fit_transform(toy_corpus)\n",
        "\n",
        "print(f\"The vocabulary size is {len(vectorizer.vocabulary_.keys())}\")\n",
        "print(f\"The document-term matrix shape is {corpus_tfidf.shape}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary size is 22\n",
            "The document-term matrix shape is (3, 22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "QqYcTBGrP2iK",
        "outputId": "c2ae9bbe-2241-4c35-da44-808113e500eb"
      },
      "source": [
        "df = pd.DataFrame(np.round(corpus_tfidf.toarray(), 2))\n",
        "df.columns = vectorizer.get_feature_names()\n",
        "\n",
        "print(df.shape)\n",
        "df"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 22)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>big</th>\n",
              "      <th>big cat</th>\n",
              "      <th>cat</th>\n",
              "      <th>cat sat</th>\n",
              "      <th>cat slept</th>\n",
              "      <th>chased</th>\n",
              "      <th>chased cat</th>\n",
              "      <th>dog</th>\n",
              "      <th>dog chased</th>\n",
              "      <th>fat</th>\n",
              "      <th>fat cat</th>\n",
              "      <th>mat</th>\n",
              "      <th>on</th>\n",
              "      <th>on the</th>\n",
              "      <th>sat</th>\n",
              "      <th>sat on</th>\n",
              "      <th>slept</th>\n",
              "      <th>the</th>\n",
              "      <th>the big</th>\n",
              "      <th>the dog</th>\n",
              "      <th>the fat</th>\n",
              "      <th>the mat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.42</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    big  big cat   cat  cat sat  ...  the big  the dog  the fat  the mat\n",
              "0  0.00     0.00  0.17     0.29  ...     0.00     0.00     0.29     0.29\n",
              "1  0.42     0.42  0.25     0.00  ...     0.42     0.00     0.00     0.00\n",
              "2  0.00     0.00  0.25     0.00  ...     0.00     0.42     0.00     0.00\n",
              "\n",
              "[3 rows x 22 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcCDVTSMSOjG"
      },
      "source": [
        "If a word is commonly used in each document, it can be considered to be highfrequency, such as `and the`. Conversely, some words hardly appear in documents, called low-frequency (or rare) words. As high-frequency and low-frequency words may prevent the model from working properly, TF-IDF, which is one of the most important and wellknown weighting mechanisms, is used here as a solution.\n",
        "\n",
        "Inverse Document Frequency (IDF) is a statistical weight to measure the importance of a word in a document—for example, while the word `the` has no discriminative power, `chased` can be highly informative and give clues about the subject of the text. This is because high-frequency words (stopwords, functional words) have little discriminating power in understanding the documents.\n",
        "\n",
        "The discriminativeness of the terms also depends on the domain—for instance, a list of DL articles is most likely to have the word network in almost every document. IDF can scale down the weights of all terms by using their Document Frequency (DF), where the DF of a word is computed by the number of documents in which a term appears. Term Frequency (TF) is the raw count of a term (word) in a document.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pYPegjUVrG-"
      },
      "source": [
        "##Modling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzVKglOTVtpW"
      },
      "source": [
        "For the Natural Language Understanding (NLU) tasks, the traditional pipeline starts with some preparation steps, such as tokenization, stemming, noun phrase detection, chunking, stop-word elimination, and much more. \n",
        "\n",
        "Afterward, a document-term matrix is constructed with any weighting schema, where TF-IDF is the most popular one. \n",
        "\n",
        "Finally, the matrix is served as a tabulated input for Machine Learning (ML) pipelines, sentiment analysis, document similarity, document clustering, or measuring the relevancy score between a query and a document. \n",
        "\n",
        "Likewise, terms are represented as a tabular matrix\n",
        "and can be input for a token classification problem where we can apply named-entity recognition, semantic relation extractions, and so on.\n",
        "\n",
        "The classification phase includes a straightforward implementation of supervised ML algorithms such as Support Vector Machine (SVM), Random forest, logistic, naive bayes, and Multiple Learners (Boosting or Bagging)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbukncLYP5sw"
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibwPt_z2V4Sv",
        "outputId": "512d68a9-92d8-42d9-f145-a19d0e3d8f99"
      },
      "source": [
        "labels=[0, 1, 0]\n",
        "\n",
        "clf = SVC()\n",
        "clf.fit(df.to_numpy(), labels)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3aZB23DWHCs",
        "outputId": "c8a7cdd5-82f9-44b4-a337-a122f7c20810"
      },
      "source": [
        "clf.predict(df.to_numpy())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}
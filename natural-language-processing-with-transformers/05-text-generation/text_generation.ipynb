{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-generation.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOmuNicp2rvpbnr4kb8m9be",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-research-and-practice/blob/main/natural-language-processing-with-transformers/05-text-generation/text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Generation"
      ],
      "metadata": {
        "id": "n99_WglW8Ht1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the most uncanny features of transformer-based language models is\n",
        "their ability to generate text that is almost indistinguishable from text written\n",
        "by humans.\n",
        "\n",
        "\n",
        "As we’ve seen, for task-specific\n",
        "heads like sequence or token classification, generating predictions is fairly\n",
        "straightforward; the model produces some logits and we either take the\n",
        "maximum value to get the predicted class, or apply a softmax function to\n",
        "obtain the predicted probabilities per class.\n",
        "\n",
        "By contrast, converting the\n",
        "model’s probabilistic output to text requires a decoding method, which\n",
        "introduces a few challenges that are unique to text generation:\n",
        "\n",
        "* The decoding is done iteratively and thus involves significantly\n",
        "more compute than simply passing inputs once through the forward\n",
        "pass of a model.\n",
        "\n",
        "* The quality and diversity of the generated text depend on the choice\n",
        "of decoding method and associated hyperparameters.\n",
        "\n",
        "To understand how this decoding process works, let’s start by examining\n",
        "how GPT-2 is pretrained and subsequently applied to generate text.\n",
        "\n",
        "Like other autoregressive or causal language models, GPT-2 is pretrained\n",
        "to estimate the probability $P(y|x)$ of a sequence of tokens $y=y_1,y_2,...,y_t$ occurring in the text, given some initial prompt or context sequence $x=x_1,x_2,...,x_k$.\n",
        "\n",
        "Since it is impractical to acquire enough training data to\n",
        "estimate $P(y|x)$ directly, it is common to use the chain rule of probability to factorize it as a product of conditional probabilities:\n",
        "\n",
        "$$ P(y_1,y_2,...,y_t|x) = \\prod_{t=1}^N P(y_t|y_{<t}, x)$$\n",
        "\n",
        "It is from\n",
        "these conditional probabilities that we pick up the intuition that\n",
        "autoregressive language modeling amounts to predicting each word given the\n",
        "preceding words in a sentence; this is exactly what the probability on the\n",
        "righthand side of the preceding equation describes.\n",
        "\n",
        "Notice that this\n",
        "pretraining objective is quite different from BERT’s, which utilizes both past\n",
        "and future contexts to predict a masked token.\n",
        "\n",
        "<img alt=\"Text generation\" width=\"700\" caption=\"Generating text from an input sequence by adding a new word to the input at each step\" src=\"https://github.com/rahiakela/transformers-research-and-practice/blob/main/natural-language-processing-with-transformers/05-text-generation/images/text-generation.png?raw=1\" id=\"text-generation\"/> \n",
        "\n",
        "\n",
        "As shown, we start with a prompt like \"Transformers are the\" and use the model to\n",
        "predict the next token. Once we have determined the next token, we append it\n",
        "to the prompt and then use the new input sequence to generate another token.\n",
        "We do this until we have reached a special end-of-sequence token or a\n",
        "predefined maximum length.\n",
        "\n",
        ">Since the output sequence is conditioned on the choice of input prompt, this type of text generation is often called conditional text generation.\n",
        "\n",
        "At the heart of this process lies a decoding method that determines which\n",
        "token is selected at each timestep.\n",
        "\n",
        "Since the language model head produces a logit $z_{t,i}$ per token in the vocabulary at each step, we can get the probability\n",
        "distribution over the next possible token $w_i$ by taking the softmax:\n",
        "\n",
        "$$ P(y_t=w_i|y_{<t},x) = softmax(z_{t,i}) $$\n",
        "\n",
        "The goal of most decoding methods is to search for the most likely overall\n",
        "sequence by picking a $\\hat y$ such that:\n",
        "\n",
        "$$ \\hat y = argmax P(y|x) $$\n",
        "\n",
        "Finding $\\hat y$ directly would involve evaluating every possible sequence with the language model. Since there does not exist an algorithm that can do this in a reasonable amount of time, we rely on approximations instead."
      ],
      "metadata": {
        "id": "XxDO9Aqj8Ilv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "cQGjMmorBdgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers"
      ],
      "metadata": {
        "id": "HzVk7erxBejt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.functional import cross_entropy\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "rPJPgDxBBm5m"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "TAe-dOcEG-0n"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Greedy Search Decoding"
      ],
      "metadata": {
        "id": "ZLE7EtxDCH6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplest decoding method to get discrete tokens from a model’s\n",
        "continuous output is to greedily select the token with the highest probability\n",
        "at each timestep:\n",
        "\n",
        "$$ \\hat y_t = argmax P(y_t|y_{<t}, x) $$\n",
        "\n",
        "To see how greedy search works, let’s start by loading the 1.5-billion parameter version of `GPT-2` with a language modeling head:"
      ],
      "metadata": {
        "id": "PubtRVkaCKQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_name = \"gpt2-xl\"\n",
        "model_name = \"gpt2\"  # due RAM issue, loading smaller model\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "5jq-su_wG9DP"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s generate some text! \n",
        "\n",
        "Although Transformers provides a\n",
        "`generate()` function for autoregressive models like `GPT-2`, we’ll implement\n",
        "this decoding method ourselves to see what goes on under the hood.\n",
        "\n",
        "We’ll use `Transformers are the` as the input prompt and run the decoding for eight timesteps.\n",
        "\n",
        "At each timestep, we pick out the model’s logits for the last token\n",
        "in the prompt and wrap them with a softmax to get a probability distribution.\n",
        "We then pick the next token with the highest probability, add it to the input\n",
        "sequence, and run the process again."
      ],
      "metadata": {
        "id": "M3c5zT50Ht7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Transformers are the\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "iterations = []\n",
        "n_steps = 8\n",
        "choices_per_step = 5\n",
        "\n",
        "with torch.no_grad():\n",
        "  for _ in range(n_steps):\n",
        "    iteration = dict()\n",
        "    iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
        "    output = model(input_ids=input_ids)\n",
        "\n",
        "    # Select logits of the first batch and the last token and apply softmax\n",
        "    next_token_logits = output.logits[0, -1, :]\n",
        "    next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
        "    sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
        "\n",
        "    # Store tokens with highest probabilities\n",
        "    for choice_idx in range(choices_per_step):\n",
        "      token_id = sorted_ids[choice_idx]\n",
        "      token_prob = next_token_probs[token_id].cpu().numpy()\n",
        "      token_choice = (f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\")\n",
        "      iteration[f\"Choice {choice_idx + 1}\"] = token_choice\n",
        "    \n",
        "    # Append predicted next token to input\n",
        "    input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
        "    iterations.append(iteration)\n",
        "\n",
        "pd.DataFrame(iterations)"
      ],
      "metadata": {
        "id": "8LWCIqOcH9-z",
        "outputId": "834017dd-4739-4648-f192-0f109a758bc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Input           Choice 1  \\\n",
              "0                               Transformers are the       most (9.76%)   \n",
              "1                          Transformers are the most    common (22.90%)   \n",
              "2                   Transformers are the most common      type (15.06%)   \n",
              "3              Transformers are the most common type        of (83.13%)   \n",
              "4           Transformers are the most common type of   particle (1.55%)   \n",
              "5  Transformers are the most common type of particle         . (14.26%)   \n",
              "6  Transformers are the most common type of parti...      They (17.48%)   \n",
              "7  Transformers are the most common type of parti...       are (38.77%)   \n",
              "\n",
              "            Choice 2            Choice 3          Choice 4  \\\n",
              "0       same (2.94%)        only (2.87%)      best (2.38%)   \n",
              "1   powerful (6.88%)   important (6.32%)   popular (3.95%)   \n",
              "2      types (3.31%)        form (1.91%)       way (1.89%)   \n",
              "3         in (3.16%)           . (1.92%)         , (1.63%)   \n",
              "4     object (1.02%)       light (0.71%)    energy (0.67%)   \n",
              "5        in (11.57%)       that (10.18%)         , (9.57%)   \n",
              "6        \\n (15.19%)         The (7.06%)     These (3.09%)   \n",
              "7       have (8.14%)         can (7.99%)       're (5.04%)   \n",
              "\n",
              "               Choice 5  \n",
              "0         first (1.77%)  \n",
              "1      commonly (2.14%)  \n",
              "2           and (1.49%)  \n",
              "3           for (0.88%)  \n",
              "4       objects (0.66%)  \n",
              "5   accelerator (5.81%)  \n",
              "6            In (3.07%)  \n",
              "7       consist (1.57%)  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2b545301-6230-4f0d-a461-5ae765829c90\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Input</th>\n",
              "      <th>Choice 1</th>\n",
              "      <th>Choice 2</th>\n",
              "      <th>Choice 3</th>\n",
              "      <th>Choice 4</th>\n",
              "      <th>Choice 5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Transformers are the</td>\n",
              "      <td>most (9.76%)</td>\n",
              "      <td>same (2.94%)</td>\n",
              "      <td>only (2.87%)</td>\n",
              "      <td>best (2.38%)</td>\n",
              "      <td>first (1.77%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Transformers are the most</td>\n",
              "      <td>common (22.90%)</td>\n",
              "      <td>powerful (6.88%)</td>\n",
              "      <td>important (6.32%)</td>\n",
              "      <td>popular (3.95%)</td>\n",
              "      <td>commonly (2.14%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Transformers are the most common</td>\n",
              "      <td>type (15.06%)</td>\n",
              "      <td>types (3.31%)</td>\n",
              "      <td>form (1.91%)</td>\n",
              "      <td>way (1.89%)</td>\n",
              "      <td>and (1.49%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Transformers are the most common type</td>\n",
              "      <td>of (83.13%)</td>\n",
              "      <td>in (3.16%)</td>\n",
              "      <td>. (1.92%)</td>\n",
              "      <td>, (1.63%)</td>\n",
              "      <td>for (0.88%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Transformers are the most common type of</td>\n",
              "      <td>particle (1.55%)</td>\n",
              "      <td>object (1.02%)</td>\n",
              "      <td>light (0.71%)</td>\n",
              "      <td>energy (0.67%)</td>\n",
              "      <td>objects (0.66%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Transformers are the most common type of particle</td>\n",
              "      <td>. (14.26%)</td>\n",
              "      <td>in (11.57%)</td>\n",
              "      <td>that (10.18%)</td>\n",
              "      <td>, (9.57%)</td>\n",
              "      <td>accelerator (5.81%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Transformers are the most common type of parti...</td>\n",
              "      <td>They (17.48%)</td>\n",
              "      <td>\\n (15.19%)</td>\n",
              "      <td>The (7.06%)</td>\n",
              "      <td>These (3.09%)</td>\n",
              "      <td>In (3.07%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Transformers are the most common type of parti...</td>\n",
              "      <td>are (38.77%)</td>\n",
              "      <td>have (8.14%)</td>\n",
              "      <td>can (7.99%)</td>\n",
              "      <td>'re (5.04%)</td>\n",
              "      <td>consist (1.57%)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2b545301-6230-4f0d-a461-5ae765829c90')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2b545301-6230-4f0d-a461-5ae765829c90 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2b545301-6230-4f0d-a461-5ae765829c90');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also see the other possible continuations\n",
        "at each step, which shows the iterative nature of text generation.\n",
        "\n",
        "Unlike other\n",
        "tasks such as sequence classification where a single forward pass suffices to\n",
        "generate the predictions, with text generation we need to decode the output\n",
        "tokens one at a time.\n",
        "\n",
        "Implementing greedy search wasn’t too hard, but we’ll want to use the builtin\n",
        "`generate()` function from Transformers to explore more sophisticated\n",
        "decoding methods."
      ],
      "metadata": {
        "id": "gFBgzCMANP8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "# specify the max_new_tokens for the number of newly generated tokens\n",
        "output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)"
      ],
      "metadata": {
        "id": "m8oVg4n4NgPz",
        "outputId": "118743dd-ce21-4035-e124-218d4d2fd7b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "id": "6q5Bp6mHOJp9",
        "outputId": "c2bc3f14-cf8a-4316-b0b4-b8fec80ce50f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers are the most common type of particle. They are\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s try something a bit more interesting."
      ],
      "metadata": {
        "id": "HSOAJxTuOV6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 128\n",
        "input_txt = \"\"\"\n",
        "In a shocking finding, scientist discovered \\\n",
        "a herd of unicorns living in a remote, previously unexplored \\\n",
        "valley, in the Andes Mountains. Even more surprising to the \\\n",
        "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
        "\"\"\"\n",
        "\n",
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)"
      ],
      "metadata": {
        "id": "atYbr-QyOWbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(output_greedy[0]))"
      ],
      "metadata": {
        "id": "dJed3tQEOs-I",
        "outputId": "f9526926-ae54-4295-b4cd-e348dad818d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
            "\n",
            "\n",
            "The researchers, from the University of California, Berkeley, and the University of California, Santa Cruz, found that the unicorns were able to communicate with each other through their tongues.\n",
            "\n",
            "\n",
            "\"This is a very interesting finding,\" said lead author Dr. David J. Karp, a professor of linguistics at the University of California, Berkeley. \"It's a very interesting finding that we can\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see one of the main drawbacks with greedy search decoding: it\n",
        "tends to produce repetitive output sequences, which is certainly undesirable\n",
        "in a news article. \n",
        "\n",
        "This is a common problem with greedy search algorithms,\n",
        "which can fail to give you the optimal solution; in the context of decoding,\n",
        "they can miss word sequences whose overall probability is higher just\n",
        "because high-probability words happen to be preceded by low-probability\n",
        "ones.\n",
        "\n",
        "Although greedy search decoding is rarely used for text generation tasks that require diversity, it can be useful for producing short sequences like arithmetic where a deterministic and factually correct output is preferred.\n",
        "\n",
        "For these tasks, you can condition `GPT-2` by providing a few line-separated examples in the format `5 + 8 => 13 \\n 7 +\n",
        "2 => 9 \\n 1 + 0 =>` as the input prompt."
      ],
      "metadata": {
        "id": "XriTrj9dPQ-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 128\n",
        "input_arithmatic = \"\"\"5 + 8 => 13 \\n 7 + 2 => 9 \\n 1 + 0 =>\"\"\"\n",
        "\n",
        "input_ids = tokenizer(input_arithmatic, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)"
      ],
      "metadata": {
        "id": "6vTFDO2_PomH",
        "outputId": "14854ab1-3720-4f1f-e837-64c099f85da8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(output_greedy[0]))"
      ],
      "metadata": {
        "id": "WUDSWfG5P8MM",
        "outputId": "cc6932cb-7814-4fa8-8dbe-c05cfd83dcb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 + 8 => 13 \n",
            " 7 + 2 => 9 \n",
            " 1 + 0 => 10 \n",
            "\n",
            "1 + 1 => 11 \n",
            "\n",
            "1 + 2 => 12 \n",
            "\n",
            "1 + 3 => 13 \n",
            "\n",
            "1 + 4 => 14 \n",
            "\n",
            "1 + 5 => 15 \n",
            "\n",
            "1 + 6 => 16 \n",
            "\n",
            "1 + 7 => 17 \n",
            "\n",
            "1 + 8 => 18 \n",
            "\n",
            "1 + 9 => 19 \n",
            "\n",
            "1 + 10 => 20 \n",
            "\n",
            "1 + 11 => 21 \n",
            "\n",
            "1 + 12 => 22 \n",
            "\n",
            "1 + 13 => 23 \n",
            "\n",
            "1 +\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 128\n",
        "input_arithmatic = \"\"\"1 + 2 => 3 \\n 2 + 3 => 5 \\n 3 + 4 =>\"\"\"\n",
        "\n",
        "input_ids = tokenizer(input_arithmatic, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)"
      ],
      "metadata": {
        "id": "2YTZUhACR2mv",
        "outputId": "766d19a3-7b0b-4c92-89d6-42acde30468d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(output_greedy[0]))"
      ],
      "metadata": {
        "id": "6BRhYf-sSJI9",
        "outputId": "a8d9caf4-46e6-4824-e468-f21cf04834af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 + 2 => 3 \n",
            " 2 + 3 => 5 \n",
            " 3 + 4 => 6 \n",
            "\n",
            "4 + 5 => 7 \n",
            "\n",
            "5 + 6 => 8 \n",
            "\n",
            "6 + 7 => 9 \n",
            "\n",
            "7 + 8 => 10 \n",
            "\n",
            "8 + 9 => 11 \n",
            "\n",
            "9 + 10 => 12 \n",
            "\n",
            "10 + 11 => 13 \n",
            "\n",
            "11 + 12 => 14 \n",
            "\n",
            "12 + 13 => 15 \n",
            "\n",
            "13 + 14 => 16 \n",
            "\n",
            "14 + 15 => 17 \n",
            "\n",
            "15 + 16 => 18 \n",
            "\n",
            "16 + 17 => 19 \n",
            "\n",
            "17 +\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fortunately, we can do better—let’s examine a popular method known as\n",
        "beam search decoding."
      ],
      "metadata": {
        "id": "6sONv3_5QP-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Beam Search Decoding"
      ],
      "metadata": {
        "id": "9_wEwbVoQQ6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of decoding the token with the highest probability at each step, beam\n",
        "search keeps track of the top-b most probable next tokens, where b is\n",
        "referred to as the number of beams or partial hypotheses.\n",
        "\n",
        "The next set of\n",
        "beams are chosen by considering all possible next-token extensions of the\n",
        "existing set and selecting the b most likely extensions. \n",
        "\n",
        "The process is\n",
        "repeated until we reach the maximum length or an EOS token, and the most likely sequence is selected by ranking the b beams according to their log\n",
        "probabilities.\n",
        "\n",
        "Why do we score the sequences using log probabilities instead of the\n",
        "probabilities themselves?\n",
        "\n",
        "That calculating the overall probability of a\n",
        "sequence $P(y_1, y_2, ..., y_t|x)$ involves calculating a product of conditional\n",
        "probabilities $P(y_t|y_{<t}, x)$ is one reason.\n",
        "\n",
        "Since each conditional probability\n",
        "is typically a small number in the range `[0, 1]`, taking their product can lead\n",
        "to an overall probability that can easily underflow. \n",
        "\n",
        "This means that the\n",
        "computer can no longer precisely represent the result of the calculation.\n",
        "\n",
        "For\n",
        "example, suppose we have a sequence of `t = 1024` tokens and generously\n",
        "assume that the probability for each token is 0.5. \n",
        "\n",
        "The overall probability for\n",
        "this sequence is an extremely small number:"
      ],
      "metadata": {
        "id": "zN4b7-FIQS7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "0.5 ** 1024"
      ],
      "metadata": {
        "id": "Xt3_PemBUvNb",
        "outputId": "a3950edd-1c18-47d9-e563-375157a0b093",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.562684646268003e-309"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "which leads to numerical instability as we run into underflow. We can avoid\n",
        "this by calculating a related term, the log probability.\n",
        "\n",
        "If we apply the\n",
        "logarithm to the joint and conditional probabilities, then with the help of the\n",
        "product rule for logarithms we get:\n",
        "\n",
        "$$ log P(y_1, y_2,...,y_t|x) = \\sum_{t=1}^N log P(y_t|y_{<t},x) $$\n",
        "\n",
        "In other words, the product of probabilities we saw earlier becomes a sum of\n",
        "log probabilities, which is much less likely to run into numerical\n",
        "instabilities.\n",
        "\n",
        "For example, calculating the log probability of the same\n",
        "example as before gives:"
      ],
      "metadata": {
        "id": "ky3NIBuCU7q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum([np.log(0.5)] * 1024)"
      ],
      "metadata": {
        "id": "l0ojGPMiVgb5",
        "outputId": "cce5bc1c-19c3-46cc-bb35-166c3af3b718",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-709.7827128933695"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a number we can easily deal with, and this approach still works for\n",
        "much smaller numbers. \n",
        "\n",
        "Since we only want to compare relative\n",
        "probabilities, we can do this directly with log probabilities.\n",
        "\n",
        "Since Transformers models return the unnormalized logits for\n",
        "the next token given the input tokens, we first need to normalize the logits to\n",
        "create a probability distribution over the whole vocabulary for each token in\n",
        "the sequence. \n",
        "\n",
        "We then need to select only the token probabilities that were\n",
        "present in the sequence."
      ],
      "metadata": {
        "id": "ocjemHagVzNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_probs_from_logits(logits, labels):\n",
        "  logp = F.log_softmax(logits, dim=-1)\n",
        "  logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
        "  return logp_label"
      ],
      "metadata": {
        "id": "lMFFVN0oWEHc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives us the log probability for a single token, so to get the total log\n",
        "probability of a sequence we just need to sum the log probabilities for each\n",
        "token:"
      ],
      "metadata": {
        "id": "Hgbkzc3tWjdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sequence_log_probs(model, labels, input_len=0):\n",
        "  with torch.no_grad():\n",
        "    output = model(labels)\n",
        "    log_probs = log_probs_from_logits(output.logits[:, :-1, :], labels[:, 1:])\n",
        "    seq_log_prob = torch.sum(log_probs[:, input_len:])\n",
        "  return seq_log_prob.cpu().numpy()"
      ],
      "metadata": {
        "id": "ECGcQsTrWj9t"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also see that it is important to align\n",
        "the logits and the labels; since the model predicts the next token,we do not\n",
        "get a logit for the first label, and we don’t need the last logit because we\n",
        "don’t have a ground truth token for it.\n",
        "\n",
        "Let’s use these functions to first calculate the sequence log probability of the\n",
        "greedy decoder."
      ],
      "metadata": {
        "id": "TkMdSMFNYS7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 128\n",
        "input_txt = \"\"\"\n",
        "In a shocking finding, scientist discovered \\\n",
        "a herd of unicorns living in a remote, previously unexplored \\\n",
        "valley, in the Andes Mountains. Even more surprising to the \\\n",
        "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
        "\"\"\"\n",
        "\n",
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)"
      ],
      "metadata": {
        "id": "v4OeBaVHabSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logp = sequence_log_probs(model, output_greedy, input_len=len(input_ids[0]))\n",
        "print(tokenizer.decode(output_greedy[0]))\n",
        "print(f\"\\nlog-prob: {logp:.2f}\")"
      ],
      "metadata": {
        "id": "3RZp7OlOYdZx",
        "outputId": "b6283e13-17c8-4e9d-8075-38e194a5df03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
            "\n",
            "\n",
            "The researchers, from the University of California, Berkeley, and the University of California, Santa Cruz, found that the unicorns were able to communicate with each other through their tongues.\n",
            "\n",
            "\n",
            "\"This is a very interesting finding,\" said lead author Dr. David J. Karp, a professor of linguistics at the University of California, Berkeley. \"It's a very interesting finding that we can\n",
            "\n",
            "log-prob: -99.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s compare this to a sequence that is generated with beam search.\n",
        "\n",
        "To\n",
        "activate beam search with the `generate()` function we just need to specify\n",
        "the number of beams with the num_beams parameter. \n",
        "\n",
        "The more beams we\n",
        "choose, the better the result potentially gets; however, the generation process\n",
        "becomes much slower since we generate parallel sequences for each beam:"
      ],
      "metadata": {
        "id": "XJX8ut8AaLQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False)"
      ],
      "metadata": {
        "id": "KLmRAxtyaTi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logp = sequence_log_probs(model, output_beam, input_len=len(input_ids[0]))\n",
        "print(tokenizer.decode(output_beam[0]))\n",
        "print(f\"\\nlog-prob: {logp:.2f}\")"
      ],
      "metadata": {
        "id": "tnHcu8xMa1Wa",
        "outputId": "38cf90d5-284b-4e82-a995-8dd277eecca3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
            "\n",
            "\n",
            "The researchers, from the University of California, San Diego, and the University of California, Santa Cruz, found that the unicorns were able to communicate with each other in a way that was similar to that of human speech.\n",
            "\n",
            "\n",
            "\"The unicorns were able to communicate with each other in a way that was similar to that of human speech,\" said study co-author Dr. David J.\n",
            "\n",
            "log-prob: -74.68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that we get a better log probability (higher is better) with beam\n",
        "search than we did with simple greedy decoding.\n",
        "\n",
        "However, we can see that\n",
        "beam search also suffers from repetitive text. \n",
        "\n",
        "One way to address this is to\n",
        "impose an n-gram penalty with the `no_repeat_ngram_size` parameter that\n",
        "tracks which n-grams have been seen and sets the next token probability to\n",
        "zero if it would produce a previously seen n-gram:"
      ],
      "metadata": {
        "id": "8eG-8ADzbbGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, no_repeat_ngram_size=2, do_sample=False)"
      ],
      "metadata": {
        "id": "kAwSWfAvbhf-",
        "outputId": "eb531f78-4eaa-4867-bdc4-68aab8ef78ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logp = sequence_log_probs(model, output_beam, input_len=len(input_ids[0]))\n",
        "print(tokenizer.decode(output_beam[0]))\n",
        "print(f\"\\nlog-prob: {logp:.2f}\")"
      ],
      "metadata": {
        "id": "3r9tSumzbvYM",
        "outputId": "0dc61e13-d2d7-4458-ee3f-f19df947295b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
            "\n",
            "\n",
            "The researchers, from the University of California, San Diego, and the National Science Foundation (NSF) in Boulder, Colorado, were able to translate the words of the unicorn into English, which they then translated into Spanish.\n",
            "\n",
            "\"This is the first time that we have translated the word 'unicorn' from English to Spanish,\" said study co-lead author Dr. David J. K\n",
            "\n",
            "log-prob: -102.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ve managed to stop the repetitions, and we can see\n",
        "that despite producing a lower score, the text remains coherent.\n",
        "\n",
        "**Beam search with n-gram penalty is a good way to find a trade-off between focusing on high-probability tokens (with beam search) while reducing repetitions (with n-gram penalty), and it’s commonly used in applications such as summarization or machine translation where factual correctness is important.**\n",
        "\n",
        "**When factual correctness is less important than the diversity of generated\n",
        "output, for instance in open-domain chitchat or story generation, another\n",
        "alternative to reduce repetitions while improving diversity is to use\n",
        "sampling.**"
      ],
      "metadata": {
        "id": "HJ7wFp92b8I9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sampling Methods"
      ],
      "metadata": {
        "id": "6uLs49vrcYu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qVh822jacbLU"
      }
    }
  ]
}
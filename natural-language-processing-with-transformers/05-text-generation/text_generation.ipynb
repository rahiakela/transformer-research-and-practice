{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-generation.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM/er3HmtZ5HkUi+0XOjV5n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-research-and-practice/blob/main/natural-language-processing-with-transformers/05-text-generation/text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Generation"
      ],
      "metadata": {
        "id": "n99_WglW8Ht1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the most uncanny features of transformer-based language models is\n",
        "their ability to generate text that is almost indistinguishable from text written\n",
        "by humans.\n",
        "\n",
        "\n",
        "As we’ve seen, for task-specific\n",
        "heads like sequence or token classification, generating predictions is fairly\n",
        "straightforward; the model produces some logits and we either take the\n",
        "maximum value to get the predicted class, or apply a softmax function to\n",
        "obtain the predicted probabilities per class.\n",
        "\n",
        "By contrast, converting the\n",
        "model’s probabilistic output to text requires a decoding method, which\n",
        "introduces a few challenges that are unique to text generation:\n",
        "\n",
        "* The decoding is done iteratively and thus involves significantly\n",
        "more compute than simply passing inputs once through the forward\n",
        "pass of a model.\n",
        "\n",
        "* The quality and diversity of the generated text depend on the choice\n",
        "of decoding method and associated hyperparameters.\n",
        "\n",
        "To understand how this decoding process works, let’s start by examining\n",
        "how GPT-2 is pretrained and subsequently applied to generate text.\n",
        "\n",
        "Like other autoregressive or causal language models, GPT-2 is pretrained\n",
        "to estimate the probability $P(y|x)$ of a sequence of tokens $y=y_1,y_2,...,y_t$ occurring in the text, given some initial prompt or context sequence $x=x_1,x_2,...,x_k$.\n",
        "\n",
        "Since it is impractical to acquire enough training data to\n",
        "estimate $P(y|x)$ directly, it is common to use the chain rule of probability to factorize it as a product of conditional probabilities:\n",
        "\n",
        "$$ P(y_1,y_2,...,y_t|x) = \\prod_{t=1}^N P(y_t|y_{<t}, x)$$\n",
        "\n",
        "It is from\n",
        "these conditional probabilities that we pick up the intuition that\n",
        "autoregressive language modeling amounts to predicting each word given the\n",
        "preceding words in a sentence; this is exactly what the probability on the\n",
        "righthand side of the preceding equation describes.\n",
        "\n",
        "Notice that this\n",
        "pretraining objective is quite different from BERT’s, which utilizes both past\n",
        "and future contexts to predict a masked token.\n",
        "\n",
        "<img alt=\"Text generation\" width=\"700\" caption=\"Generating text from an input sequence by adding a new word to the input at each step\" src=\"images/text-generation.png\" id=\"text-generation\"/> \n",
        "\n",
        "\n",
        "As shown, we start with a prompt like \"Transformers are the\" and use the model to\n",
        "predict the next token. Once we have determined the next token, we append it\n",
        "to the prompt and then use the new input sequence to generate another token.\n",
        "We do this until we have reached a special end-of-sequence token or a\n",
        "predefined maximum length.\n",
        "\n",
        ">Since the output sequence is conditioned on the choice of input prompt, this type of text generation is often called conditional text generation.\n",
        "\n",
        "At the heart of this process lies a decoding method that determines which\n",
        "token is selected at each timestep.\n",
        "\n",
        "Since the language model head produces a logit $z_{t,i}$ per token in the vocabulary at each step, we can get the probability\n",
        "distribution over the next possible token $w_i$ by taking the softmax:\n",
        "\n",
        "$$ P(y_t=w_i|y_{<t},x) = softmax(z_{t,i}) $$\n",
        "\n",
        "The goal of most decoding methods is to search for the most likely overall\n",
        "sequence by picking a $\\hat y$ such that:\n",
        "\n",
        "$$ \\hat y = argmax P(y|x) $$\n",
        "\n",
        "Finding $\\hat y$ directly would involve evaluating every possible sequence with the language model. Since there does not exist an algorithm that can do this in a reasonable amount of time, we rely on approximations instead."
      ],
      "metadata": {
        "id": "XxDO9Aqj8Ilv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "cQGjMmorBdgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers"
      ],
      "metadata": {
        "id": "HzVk7erxBejt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.functional import cross_entropy\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "rPJPgDxBBm5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Greedy Search Decoding"
      ],
      "metadata": {
        "id": "ZLE7EtxDCH6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PubtRVkaCKQM"
      }
    }
  ]
}
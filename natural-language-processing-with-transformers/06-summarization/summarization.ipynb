{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "summarization.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO7J240tkgwRUOoLuvD0pSl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-research-and-practice/blob/main/natural-language-processing-with-transformers/06-summarization/summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Summarization"
      ],
      "metadata": {
        "id": "yec-35M4BWvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text summarization is a\n",
        "difficult task for neural language models, including transformers. Despite these challenges,\n",
        "text summarization offers the prospect for domain experts to significantly\n",
        "speed up their workflows and is used by enterprises to condense internal knowledge,\n",
        "summarize contracts, automatically generate content for social media releases,\n",
        "and more.\n",
        "\n",
        "Summarization is a classic\n",
        "sequence-to-sequence (seq2seq) task with an input text and a target text."
      ],
      "metadata": {
        "id": "dfFj2oRmBXpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "0jROG17wBazd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers\n",
        "!pip -q install datasets\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "ojHJ4FhvBb7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "dIrfRuBsBibh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "s7nIfW88cegm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CNN/DailyMail Dataset"
      ],
      "metadata": {
        "id": "qBE7AF9bD1Wx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CNN/DailyMail dataset consists of around 300,000 pairs of news articles and\n",
        "their corresponding summaries, composed from the bullet points that CNN and the\n",
        "DailyMail attach to their articles.\n",
        "\n",
        "An important aspect of the dataset is that the summaries are abstractive and not extractive, which means that they consist of new\n",
        "sentences instead of simple excerpts.\n",
        "\n",
        "We’ll use version 3.0.0, which is a nonanonymized version set up for summarization."
      ],
      "metadata": {
        "id": "5IQkUfASD2Kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"cnn_dailymail\", version=\"3.0.0\")"
      ],
      "metadata": {
        "id": "le4s8kDKEMwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Features: {dataset['train'].column_names}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlgyxXqHE5BQ",
        "outputId": "560bf0e7-a812-49e3-dcda-acee62a4a801"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features: ['article', 'highlights', 'id']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s look at an excerpt from an article:"
      ],
      "metadata": {
        "id": "liZjNQ2KFA9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = dataset[\"train\"][1]\n",
        "\n",
        "print(f\"\"\"Article (excerpt of 500 characters, total length: {len(sample['article'])})\"\"\")\n",
        "print(sample[\"article\"])\n",
        "\n",
        "print(f\"\\nSummary (length: {len(sample['highlights'])})\")\n",
        "print(sample[\"highlights\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKLCJt-wFC49",
        "outputId": "3b43a5e1-88d4-4b6d-8a95-85e270ce63a4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article (excerpt of 500 characters, total length: 3192)\n",
            "(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay. The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds. The U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover. The 26-year-old Bolt has now collected eight gold medals at world championships, equaling the record held by American trio Carl Lewis, Michael Johnson and Allyson Felix, not to mention the small matter of six Olympic titles. The relay triumph followed individual successes in the 100 and 200 meters in the Russian capital. \"I'm proud of myself and I'll continue to work to dominate for as long as possible,\" Bolt said, having previously expressed his intention to carry on until the 2016 Rio Olympics. Victory was never seriously in doubt once he got the baton safely in hand from Ashmeade, while Gatlin and the United States third leg runner Rakieem Salaam had problems. Gatlin strayed out of his lane as he struggled to get full control of their baton and was never able to get on terms with Bolt. Earlier, Jamaica's women underlined their dominance in the sprint events by winning the 4x100m relay gold, anchored by Shelly-Ann Fraser-Pryce, who like Bolt was completing a triple. Their quartet recorded a championship record of 41.29 seconds, well clear of France, who crossed the line in second place in 42.73 seconds. Defending champions, the United States, were initially back in the bronze medal position after losing time on the second handover between Alexandria Anderson and English Gardner, but promoted to silver when France were subsequently disqualified for an illegal handover. The British quartet, who were initially fourth, were promoted to the bronze which eluded their men's team. Fraser-Pryce, like Bolt aged 26, became the first woman to achieve three golds in the 100-200 and the relay. In other final action on the last day of the championships, France's Teddy Tamgho became the third man to leap over 18m in the triple jump, exceeding the mark by four centimeters to take gold. Germany's Christina Obergfoll finally took gold at global level in the women's javelin after five previous silvers, while Kenya's Asbel Kiprop easily won a tactical men's 1500m final. Kiprop's compatriot Eunice Jepkoech Sum was a surprise winner of the women's 800m. Bolt's final dash for golden glory brought the eight-day championship to a rousing finale, but while the hosts topped the medal table from the United States there was criticism of the poor attendances in the Luzhniki Stadium. There was further concern when their pole vault gold medalist Yelena Isinbayeva made controversial remarks in support of Russia's new laws, which make \"the propagandizing of non-traditional sexual relations among minors\" a criminal offense. She later attempted to clarify her comments, but there were renewed calls by gay rights groups for a boycott of the 2014 Winter Games in Sochi, the next major sports event in Russia.\n",
            "\n",
            "Summary (length: 180)\n",
            "Usain Bolt wins third gold of world championship .\n",
            "Anchors Jamaica to 4x100m relay victory .\n",
            "Eighth gold at the championships for Bolt .\n",
            "Jamaica double up in women's 4x100m relay .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the articles can be very long compared to the target summary; in this particular\n",
        "case the difference is 17-fold.\n",
        "\n",
        "Long articles pose a challenge to most transformer\n",
        "models since the context size is usually limited to 1,000 tokens or so, which is\n",
        "equivalent to a few paragraphs of text. The standard, yet crude way to deal with this\n",
        "for summarization is to simply truncate the texts beyond the model’s context size."
      ],
      "metadata": {
        "id": "I3uztDnPIwQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Summarization Pipelines"
      ],
      "metadata": {
        "id": "PXn-s8xdI3n-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s see how a few of the most popular transformer models for summarization perform\n",
        "by first looking qualitatively at the outputs for the preceding example."
      ],
      "metadata": {
        "id": "AIIQn9SpI4V1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = dataset[\"train\"][1][\"article\"][:2000]  # restrict the input text to 2,000 characters\n",
        "\n",
        "# We'll collect the generated summaries of each model in a dictionary\n",
        "summaries = {}"
      ],
      "metadata": {
        "id": "KQoxjDP1buCn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's differentiate the end of a sentence\n",
        "from punctuation that occurs in abbreviations"
      ],
      "metadata": {
        "id": "MUPpd8ctcWbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string = \"The U.S. are a country. The U.N. is an organization.\"\n",
        "sent_tokenize(string)"
      ],
      "metadata": {
        "id": "rCISIlGjcXxg",
        "outputId": "d550970e-b11e-417d-a395-59ca1385f968",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The U.S. are a country.', 'The U.N. is an organization.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Summarization Baseline"
      ],
      "metadata": {
        "id": "F2uP4JYIcr19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A common baseline for summarizing news articles is to simply take the first three\n",
        "sentences of the article."
      ],
      "metadata": {
        "id": "M8FG9t65ctFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def three_sentence_summary(text):\n",
        "  return \"\\n\".join(sent_tokenize(text)[:3])"
      ],
      "metadata": {
        "id": "o1JLfDwzc08a"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summaries[\"baseline\"] = three_sentence_summary(sample_text)"
      ],
      "metadata": {
        "id": "Db3V7AmodGMQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###GPT-2"
      ],
      "metadata": {
        "id": "zq4j04_NdRsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of GPT-2’s surprising features is that we can also use it to generate summaries\n",
        "by simply appending `TL;DR` at the end of the input text. \n",
        "\n",
        "The expression `TL;DR` (too long; didn’t read) is often used on platforms like Reddit to indicate a short version\n",
        "of a long post."
      ],
      "metadata": {
        "id": "1XT8F5eadSc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "gpt2_query = sample_text + \"\\nTL;DR:\\n\"\n",
        "pipe_out = pipe(gpt2_query, max_length=512, clean_up_tokenization_spaces=True)\n",
        "summaries[\"gpt2\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"generated_text\"][len(gpt2_query):]))"
      ],
      "metadata": {
        "id": "1003nJfrfCew",
        "outputId": "3239bfa1-40de-49c1-cb0f-ccbaad0c3498",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###T5"
      ],
      "metadata": {
        "id": "ENKDRNdKgCkv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next let’s try the T5 transformer.\n",
        "\n",
        "The T5 checkpoints are trained on a mixture of unsupervised data (to\n",
        "reconstruct masked words) and supervised data for several tasks, including summarization.\n",
        "These checkpoints can thus be directly used to perform summarization\n",
        "without fine-tuning by using the same prompts used during pretraining.\n",
        "\n",
        "In this\n",
        "framework, the input format for the model to summarize a document is \"summarize:`<ARTICLE>`\", and for translation it looks like \"translate English to German:`<TEXT>`\"."
      ],
      "metadata": {
        "id": "2IQUpmWSgD2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\"summarization\", model=\"t5-large\")\n",
        "\n",
        "pipe_out = pipe(sample_text)\n",
        "summaries[\"t5\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))"
      ],
      "metadata": {
        "id": "jRG5kmMRgj6x"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###BART"
      ],
      "metadata": {
        "id": "0mCTi_o0g502"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BART also uses an encoder-decoder architecture and is trained to reconstruct corrupted inputs. It combines the pretraining schemes of BERT and GPT-2."
      ],
      "metadata": {
        "id": "DMA8SiSEg6qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "pipe_out = pipe(sample_text)\n",
        "summaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))"
      ],
      "metadata": {
        "id": "qJbRf5vthAtQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PEGASUS"
      ],
      "metadata": {
        "id": "8bqGB7PuhOac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like BART, PEGASUS is an encoder-decoder transformer.its pretraining objective is to predict masked sentences in multisentence texts.\n",
        "\n",
        "The\n",
        "authors argue that the closer the pretraining objective is to the downstream task, the\n",
        "more effective it is. \n",
        "\n",
        "With the aim of finding a pretraining objective that is closer to\n",
        "summarization than general language modeling, they automatically identified, in a\n",
        "very large corpus, sentences containing most of the content of their surrounding\n",
        "paragraphs (using summarization evaluation metrics as a heuristic for content\n",
        "overlap) and pretrained the PEGASUS model to reconstruct these sentences, thereby\n",
        "obtaining a state-of-the-art model for text summarization.\n",
        "\n"
      ],
      "metadata": {
        "id": "32jotqkYhPMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\")\n",
        "\n",
        "pipe_out = pipe(sample_text)\n",
        "summaries[\"pegasus\"] = pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\")"
      ],
      "metadata": {
        "id": "imIO1Rdfiqe8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing Different Summaries"
      ],
      "metadata": {
        "id": "r_VaWZ1bjFEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have generated summaries with four different models, let’s compare the results.\n",
        "\n",
        "Keep in mind that one model has not been trained on the dataset at all\n",
        "(GPT-2), one model has been fine-tuned on this task among others (T5), and two\n",
        "models have exclusively been fine-tuned on this task (BART and PEGASUS).\n",
        "\n",
        "Let’s\n",
        "have a look at the summaries these models have generated:"
      ],
      "metadata": {
        "id": "6UBEAHomjH_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"GROUND TRUTH\")\n",
        "print(dataset[\"train\"][1][\"highlights\"])\n",
        "print(\"\")\n",
        "\n",
        "for model_name in summaries:\n",
        "  print(model_name.upper())\n",
        "  print(\"-----------------------------------\")\n",
        "  print(summaries[model_name])\n",
        "  print(\"\")"
      ],
      "metadata": {
        "id": "fBv8k5aLmIcf",
        "outputId": "cd2c427e-e57f-42d4-87dc-125ea57c4087",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GROUND TRUTH\n",
            "Usain Bolt wins third gold of world championship .\n",
            "Anchors Jamaica to 4x100m relay victory .\n",
            "Eighth gold at the championships for Bolt .\n",
            "Jamaica double up in women's 4x100m relay .\n",
            "\n",
            "BASELINE\n",
            "-----------------------------------\n",
            "(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay.\n",
            "The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds.\n",
            "The U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover.\n",
            "\n",
            "GPT2\n",
            "-----------------------------------\n",
            "Usain Bolt triumphed after a difficult final, but then he got into trouble when Canada made him try to block.\n",
            "It's the last match he's held all of last year with the men, who were later picked in the third and fourth rounds of the men's 100m and 200m and at Rio-Gold.\n",
            "UPDATE - November 8 - Team USA won in 36.36,\n",
            "\n",
            "T5\n",
            "-----------------------------------\n",
            "usain bolt wins his third gold medal of the world championships in the men's 4x100m relay .\n",
            "the 26-year-old anchored Jamaica to victory in the event in the Russian capital .\n",
            "he has now collected eight gold medals at the championships, equaling the record .\n",
            "\n",
            "BART\n",
            "-----------------------------------\n",
            "Usain Bolt wins his third gold of the world championships in Moscow.\n",
            "Bolt anchors Jamaica to victory in the men's 4x100m relay.\n",
            "The 26-year-old has now won eight gold medals at world championships.\n",
            "Jamaica's women also win gold in the relay, beating France in the process.\n",
            "\n",
            "PEGASUS\n",
            "-----------------------------------\n",
            "Usain Bolt wins third gold of world championships.\n",
            "Anchors Jamaica to victory in men's 4x100m relay.\n",
            "Eighth gold at the championships for Bolt.\n",
            "Jamaica also win women's 4x100m relay .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing we notice by looking at the model outputs is that the summary generated\n",
        "by GPT-2 is quite different from the others. Instead of giving a summary of the\n",
        "text, it summarizes the characters.\n",
        "\n",
        "Comparing the other three model summaries against the ground truth, we see that\n",
        "there is remarkable overlap, with PEGASUS’s output bearing the most striking\n",
        "resemblance.\n",
        "\n",
        "Now that we have inspected a few models, let’s try to decide which one we would use\n",
        "in a production setting.\n",
        "\n",
        "However, this\n",
        "is not a systematic way of determining the best model! Ideally, we would define a metric, measure it for all models on some benchmark dataset, and choose the one\n",
        "with the best performance.\n",
        "\n",
        "But how do you define a metric for text generation?\n",
        "\n",
        "The\n",
        "standard metrics that we’ve seen, like accuracy, recall, and precision, are not easy to\n",
        "apply to this task."
      ],
      "metadata": {
        "id": "ZTNXqw4am90z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Measuring the Quality of Generated Text"
      ],
      "metadata": {
        "id": "MT1h8LjunfYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FMU9qi7wnfrS"
      }
    }
  ]
}